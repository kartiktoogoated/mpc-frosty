diff --git a/SOLANA_MPC_PLATFORM_PATCH.diff b/SOLANA_MPC_PLATFORM_PATCH.diff
deleted file mode 100644
index eee5a6f..0000000
--- a/SOLANA_MPC_PLATFORM_PATCH.diff
+++ /dev/null
@@ -1,4527 +0,0 @@
-diff --git a/.DS_Store b/.DS_Store
-new file mode 100644
-index 0000000..00d0240
-Binary files /dev/null and b/.DS_Store differ
-diff --git a/.cargo/config.toml b/.cargo/config.toml
-new file mode 100644
-index 0000000..4ddee7f
---- /dev/null
-+++ b/.cargo/config.toml
-@@ -0,0 +1,2 @@
-+[env]
-+LIBCLANG_PATH = "/Library/Developer/CommandLineTools/usr/lib"
-diff --git a/Cargo.toml b/Cargo.toml
-index d060d71..c08c26f 100644
---- a/Cargo.toml
-+++ b/Cargo.toml
-@@ -1,3 +1,6 @@
- [workspace]
--version = "3.0"
-+resolver = "2"
- members = ["backend", "indexer", "mpc", "store"]
-+
-+[workspace.dependencies]
-+zeroize = "1.3"
-diff --git a/Dockerfile b/Dockerfile
-new file mode 100644
-index 0000000..ebf12ae
---- /dev/null
-+++ b/Dockerfile
-@@ -0,0 +1,15 @@
-+FROM rust:slim-bullseye as builder
-+WORKDIR /app
-+
-+# Copy workspace + local deps
-+COPY . .
-+
-+# Ensure curv is copied too (if it’s outside workspace root)
-+COPY ../curv ./curv
-+
-+RUN cargo build --release -p mpc
-+
-+FROM debian:bullseye-slim
-+WORKDIR /app
-+COPY --from=builder /app/target/release/mpc /usr/local/bin/mpc
-+CMD ["mpc"]
-diff --git a/backend/.sqlx/query-3339ba1e9d35a5bf2146c9deb9536da828cba800c48c8c047ee4f2f04676f8cb.json b/backend/.sqlx/query-3339ba1e9d35a5bf2146c9deb9536da828cba800c48c8c047ee4f2f04676f8cb.json
-new file mode 100644
-index 0000000..917baf4
---- /dev/null
-+++ b/backend/.sqlx/query-3339ba1e9d35a5bf2146c9deb9536da828cba800c48c8c047ee4f2f04676f8cb.json
-@@ -0,0 +1,17 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "INSERT INTO \"User\" (id, email, password, createdat, updatedat, publickey)\n         VALUES ($1, $2, $3, NOW(), NOW(), $4)",
-+  "describe": {
-+    "columns": [],
-+    "parameters": {
-+      "Left": [
-+        "Uuid",
-+        "Text",
-+        "Text",
-+        "Text"
-+      ]
-+    },
-+    "nullable": []
-+  },
-+  "hash": "3339ba1e9d35a5bf2146c9deb9536da828cba800c48c8c047ee4f2f04676f8cb"
-+}
-diff --git a/backend/.sqlx/query-5866ee2dcaab746f9b647ad5813bd76905fe56a481c838fad0b5da34291df445.json b/backend/.sqlx/query-5866ee2dcaab746f9b647ad5813bd76905fe56a481c838fad0b5da34291df445.json
-new file mode 100644
-index 0000000..3ff165c
---- /dev/null
-+++ b/backend/.sqlx/query-5866ee2dcaab746f9b647ad5813bd76905fe56a481c838fad0b5da34291df445.json
-@@ -0,0 +1,22 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT publickey FROM \"User\" WHERE email = $1",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "publickey",
-+        "type_info": "Text"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false
-+    ]
-+  },
-+  "hash": "5866ee2dcaab746f9b647ad5813bd76905fe56a481c838fad0b5da34291df445"
-+}
-diff --git a/backend/.sqlx/query-804a051cdb1ad644f55724bc3b7f71a8a7a4eabe059ed269737bf93c7385a1f9.json b/backend/.sqlx/query-804a051cdb1ad644f55724bc3b7f71a8a7a4eabe059ed269737bf93c7385a1f9.json
-new file mode 100644
-index 0000000..d4bab0e
---- /dev/null
-+++ b/backend/.sqlx/query-804a051cdb1ad644f55724bc3b7f71a8a7a4eabe059ed269737bf93c7385a1f9.json
-@@ -0,0 +1,40 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "\n        SELECT\n            b.amount as balance,\n            a.mintAddress as token_mint,\n            a.symbol,\n            a.decimals\n        FROM \"Balance\" b\n        JOIN \"User\" u ON b.userId = u.id\n        JOIN \"Asset\" a ON b.assetId = a.id\n        WHERE u.publickey = $1\n        ORDER BY b.amount DESC\n        ",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "balance",
-+        "type_info": "Int8"
-+      },
-+      {
-+        "ordinal": 1,
-+        "name": "token_mint",
-+        "type_info": "Text"
-+      },
-+      {
-+        "ordinal": 2,
-+        "name": "symbol",
-+        "type_info": "Text"
-+      },
-+      {
-+        "ordinal": 3,
-+        "name": "decimals",
-+        "type_info": "Int4"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false,
-+      false,
-+      false,
-+      false
-+    ]
-+  },
-+  "hash": "804a051cdb1ad644f55724bc3b7f71a8a7a4eabe059ed269737bf93c7385a1f9"
-+}
-diff --git a/backend/.sqlx/query-98f1959ede2873366c919a1c2022f0655430f27db14eddc845c4cf82369a11fc.json b/backend/.sqlx/query-98f1959ede2873366c919a1c2022f0655430f27db14eddc845c4cf82369a11fc.json
-new file mode 100644
-index 0000000..861fe5e
---- /dev/null
-+++ b/backend/.sqlx/query-98f1959ede2873366c919a1c2022f0655430f27db14eddc845c4cf82369a11fc.json
-@@ -0,0 +1,34 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "\n        SELECT id, email, password\n        FROM \"User\"\n        WHERE email = $1\n        ",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "id",
-+        "type_info": "Uuid"
-+      },
-+      {
-+        "ordinal": 1,
-+        "name": "email",
-+        "type_info": "Text"
-+      },
-+      {
-+        "ordinal": 2,
-+        "name": "password",
-+        "type_info": "Text"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false,
-+      false,
-+      false
-+    ]
-+  },
-+  "hash": "98f1959ede2873366c919a1c2022f0655430f27db14eddc845c4cf82369a11fc"
-+}
-diff --git a/backend/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json b/backend/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json
-new file mode 100644
-index 0000000..70d7ab3
---- /dev/null
-+++ b/backend/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json
-@@ -0,0 +1,22 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT id FROM \"User\" WHERE email = $1",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "id",
-+        "type_info": "Uuid"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false
-+    ]
-+  },
-+  "hash": "9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8"
-+}
-diff --git a/backend/.sqlx/query-a42f40ea536a41684b43e8931e9ee20369cea99080969adefbc27e81f45c9d4b.json b/backend/.sqlx/query-a42f40ea536a41684b43e8931e9ee20369cea99080969adefbc27e81f45c9d4b.json
-new file mode 100644
-index 0000000..c108fe1
---- /dev/null
-+++ b/backend/.sqlx/query-a42f40ea536a41684b43e8931e9ee20369cea99080969adefbc27e81f45c9d4b.json
-@@ -0,0 +1,17 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "INSERT INTO \"User\" (id, email, password, createdAt, updatedAt, publicKey)\n         VALUES ($1, $2, $3, NOW(), NOW(), $4)",
-+  "describe": {
-+    "columns": [],
-+    "parameters": {
-+      "Left": [
-+        "Uuid",
-+        "Text",
-+        "Text",
-+        "Text"
-+      ]
-+    },
-+    "nullable": []
-+  },
-+  "hash": "a42f40ea536a41684b43e8931e9ee20369cea99080969adefbc27e81f45c9d4b"
-+}
-diff --git a/backend/.sqlx/query-e5e8894ea5b9b910734365afbeebf79fe469f9063d940ab658ac84d82522912c.json b/backend/.sqlx/query-e5e8894ea5b9b910734365afbeebf79fe469f9063d940ab658ac84d82522912c.json
-new file mode 100644
-index 0000000..f937def
---- /dev/null
-+++ b/backend/.sqlx/query-e5e8894ea5b9b910734365afbeebf79fe469f9063d940ab658ac84d82522912c.json
-@@ -0,0 +1,28 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT email, password FROM \"User\" WHERE email = $1",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "email",
-+        "type_info": "Text"
-+      },
-+      {
-+        "ordinal": 1,
-+        "name": "password",
-+        "type_info": "Text"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false,
-+      false
-+    ]
-+  },
-+  "hash": "e5e8894ea5b9b910734365afbeebf79fe469f9063d940ab658ac84d82522912c"
-+}
-diff --git a/backend/.sqlx/query-f2394fb5d5eaa7f3be99a67d67083c956ff0218dba6e76ba3c0c5e12aa847b3a.json b/backend/.sqlx/query-f2394fb5d5eaa7f3be99a67d67083c956ff0218dba6e76ba3c0c5e12aa847b3a.json
-new file mode 100644
-index 0000000..f4c88bf
---- /dev/null
-+++ b/backend/.sqlx/query-f2394fb5d5eaa7f3be99a67d67083c956ff0218dba6e76ba3c0c5e12aa847b3a.json
-@@ -0,0 +1,22 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT email FROM \"User\" WHERE email = $1",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "email",
-+        "type_info": "Text"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false
-+    ]
-+  },
-+  "hash": "f2394fb5d5eaa7f3be99a67d67083c956ff0218dba6e76ba3c0c5e12aa847b3a"
-+}
-diff --git a/backend/Cargo.toml b/backend/Cargo.toml
-index 3219603..c63ff92 100644
---- a/backend/Cargo.toml
-+++ b/backend/Cargo.toml
-@@ -1,10 +1,29 @@
- [package]
- name = "backend"
- version = "0.1.0"
--edition = "2024"
-+edition = "2021"   # keep 2021 for stability, 2024 is still nightly
- 
- [dependencies]
--actix-web = "4.11.0"
--tokio = "1.47.1"
--serde = { version = "1.0", features = ["derive"] }
--serde_json = "1.0"
-+actix-web = "4"
-+tokio = { version = "1", features = ["full"] }
-+serde = { version = "1", features = ["derive"] }
-+serde_json = "1"
-+jsonwebtoken = "9"
-+chrono = { version = "0.4", features = ["serde"] }
-+dotenv = "0.15"
-+bcrypt = "0.16"
-+sqlx = { version = "0.8.6", features = ["postgres", "runtime-tokio-rustls", "uuid", "chrono"] }
-+uuid = { version = "1", features = ["serde", "v4"] }
-+ed25519-dalek = "1.0.1"
-+bs58 = "0.4"
-+env = "1.0.1"
-+futures-util = "0.3.31"
-+reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
-+hex = "0.4"
-+store = { path = "../store" }
-+lazy_static = "1.5.0"
-+base64 = "0.22.1"
-+bincode = "1.3"
-+
-+solana-client = "2.2.2"
-+solana-sdk = "2.2.2"
-diff --git a/backend/src/main.rs b/backend/src/main.rs
-index a6f5f26..cc0fefc 100644
---- a/backend/src/main.rs
-+++ b/backend/src/main.rs
-@@ -1,21 +1,35 @@
--use actix_web::{App, HttpServer};
-+use actix_web::{web, App, HttpServer};
-+use dotenv::dotenv;
-+use sqlx::PgPool;
-+use std::env;
- 
- mod routes;
- use routes::*;
- 
- #[actix_web::main]
- async fn main() -> std::io::Result<()> {
--    HttpServer::new(|| {
-+    dotenv().ok();
-+
-+    let db_url = "postgres://postgres:postgres@localhost:5432/universal";
-+    let pool = PgPool::connect(db_url).await.expect("db connect");
-+
-+    let port = env::var("PORT").unwrap_or_else(|_| "3000".to_string());
-+    let addr = format!("0.0.0.0:{port}");
-+    println!("API up at http://{addr}");
-+
-+    HttpServer::new(move || {
-         App::new()
--            .service(sign_up)  
-+            .app_data(web::Data::new(pool.clone()))
-+            .service(sign_up)
-             .service(sign_in)
-             .service(get_user)
-             .service(quote)
-             .service(swap)
--            .service(sol_balance)
--            .service(token_balance)
-+            .service(send)
-+            .service(get_sol_balance)
-+            .service(get_token_balances)
-     })
--    .bind("127.0.0.1:8080")?
-+    .bind(addr)?
-     .run()
-     .await
- }
-diff --git a/backend/src/routes/auth_middleware.rs b/backend/src/routes/auth_middleware.rs
-new file mode 100644
-index 0000000..b6701f9
---- /dev/null
-+++ b/backend/src/routes/auth_middleware.rs
-@@ -0,0 +1,23 @@
-+use actix_web::{FromRequest, HttpRequest};
-+use futures_util::future::{Ready, ready};
-+
-+pub struct AuthToken(pub String);
-+
-+impl FromRequest for AuthToken {
-+    type Error = actix_web::Error;
-+    type Future = Ready<Result<Self, Self::Error>>;
-+
-+    fn from_request(req: &HttpRequest, _: &mut actix_web::dev::Payload) -> Self::Future {
-+        if let Some(hv) = req.headers().get("Authorization") {
-+            if let Ok(auth_header) = hv.to_str() {
-+                if auth_header.starts_with("Bearer ") {
-+                    let token = &auth_header[7..]; 
-+                    return ready(Ok(AuthToken(token.to_string())));
-+                }
-+            }
-+        }
-+        ready(Err(actix_web::error::ErrorUnauthorized(
-+            "Missing or invalid Authorization header",
-+        )))
-+    }
-+}
-diff --git a/backend/src/routes/mod.rs b/backend/src/routes/mod.rs
-index 810f7d4..03bb54b 100644
---- a/backend/src/routes/mod.rs
-+++ b/backend/src/routes/mod.rs
-@@ -1,5 +1,6 @@
--pub mod user;
-+pub mod auth_middleware;
- pub mod solana;
-+pub mod user;
- 
--pub use user::*;
--pub use solana::*;
-+pub use solana::{quote, swap, send, get_sol_balance, get_token_balances, sign_up, sign_in};
-+pub use user::{get_user};
-diff --git a/backend/src/routes/solana.rs b/backend/src/routes/solana.rs
-index cee520e..d7fd357 100644
---- a/backend/src/routes/solana.rs
-+++ b/backend/src/routes/solana.rs
-@@ -1,61 +1,777 @@
--use actix_web::{web, HttpResponse, Result};
-+use actix_web::{get, post};
-+use actix_web::{web, Error as ActixError, HttpResponse, Result};
-+use bcrypt::{hash, verify};
-+use chrono::{Duration, Utc};
-+use jsonwebtoken::{decode, encode, DecodingKey, EncodingKey, Header, Validation};
-+use reqwest::Client;
- use serde::{Deserialize, Serialize};
-+use sqlx::PgPool;
-+use std::{collections::HashMap, env, str::FromStr, sync::Mutex};
-+use uuid::Uuid;
-+
-+use crate::auth_middleware::AuthToken;
-+use crate::user::UserResponse;
-+
-+use lazy_static::lazy_static;
-+lazy_static! {
-+    static ref QUOTES: Mutex<HashMap<String, serde_json::Value>> = Mutex::new(HashMap::new());
-+}
-+const JUP_QUOTE_URL: &str = "https://lite-api.jup.ag/swap/v1/quote";
-+const JUP_SWAP_URL: &str = "https://quote-api.jup.ag/v6/swap";
- 
- #[derive(Deserialize)]
--pub struct QuoteRequest {
-+pub struct SignUpRequest {
-+    pub username: String,
-+    pub password: String,
- }
- 
--#[derive(Serialize, Deserialize)]
--pub struct QuoteResponse {
-+#[derive(Serialize, Deserialize, Clone)]
-+pub struct CommitmentsMsg {
-+    pub id: u16,
-+    pub commitments: serde_json::Value,
- }
- 
-+#[derive(Serialize, Deserialize, Clone)]
-+pub struct ShareMsg {
-+    pub id: u16,
-+    pub share: serde_json::Value, // same idea
-+}
- 
- #[derive(Deserialize)]
--pub struct SwapRequest {
-+pub struct SignInRequest {
-+    pub username: String,
-+    pub password: String,
- }
- 
- #[derive(Serialize)]
--pub struct SwapResponse {
-+pub struct AuthResponse {
-+    pub token: String,
- }
- 
-+
- #[derive(Serialize)]
--pub struct BalanceResponse {
-+pub struct SignupResponse {
-+    message: String,
-+}
-+
-+#[derive(Debug, Serialize, Deserialize)]
-+struct Claims {
-+    sub: String,
-+    exp: usize,
-+}
-+
-+
-+
-+#[derive(Deserialize)]
-+pub struct QuoteRequest {
-+    #[serde(rename = "inputMint")]
-+    pub input_mint: String,
-+    #[serde(rename = "outputMint")]
-+    pub output_mint: String,
-+    #[serde(rename = "inAmount")]
-+    pub in_amount: u64,
- }
- 
- #[derive(Serialize)]
--pub struct TokenBalanceResponse {
-+pub struct QuoteResponse {
-+    #[serde(rename = "outAmount")]
-+    pub out_amount: u64,
-+    pub id: String,
- }
- 
--#[actix_web::post("/quote")]
--pub async fn quote(req: web::Json<QuoteRequest>) -> Result<HttpResponse> {
--    let response = QuoteResponse {};
--    
--    Ok(HttpResponse::Ok().json(response))
-+#[derive(Deserialize)]
-+pub struct SwapRequest {
-+    pub id: String,
- }
- 
--#[actix_web::post("/swap")]
--pub async fn swap(req: web::Json<SwapRequest>) -> Result<HttpResponse> {
--    
--    let response = SwapResponse {};
--    
--    Ok(HttpResponse::Ok().json(response))
-+
-+#[derive(Deserialize)]
-+pub struct SendRequest {
-+    pub to: String,
-+    pub amount: u64,
-+    pub mint: Option<String>,
- }
- 
--#[actix_web::get("/sol-balance/{pubkey}")]
--pub async fn sol_balance() -> Result<HttpResponse> {
--    
--    let response = BalanceResponse {
-+#[derive(Deserialize)]
-+struct AggregateSignatureResponse {
-+    signature_hex: String,
-+    signature_base64: String,
-+    solana_signature: String,
-+}
-+
-+fn generate_jwt(email: &str) -> String {
-+    let secret = env::var("JWT_SECRET").unwrap_or_else(|_| "secret".to_string());
-+    let expiration = Utc::now()
-+        .checked_add_signed(Duration::hours(24))
-+        .expect("valid ts")
-+        .timestamp() as usize;
-+
-+    let claims = Claims {
-+        sub: email.to_owned(),
-+        exp: expiration,
-     };
--    
--    Ok(HttpResponse::Ok().json(response))
-+    encode(
-+        &Header::default(),
-+        &claims,
-+        &EncodingKey::from_secret(secret.as_ref()),
-+    )
-+    .expect("JWT should be generated")
- }
- 
--#[actix_web::get("/token-balance/{pubkey}/{mint}")]
--pub async fn token_balance() -> Result<HttpResponse> {    
--    
--    let response = TokenBalanceResponse {
-+fn validate_jwt(token: &str) -> Option<String> {
-+    let secret = env::var("JWT_SECRET").unwrap_or("secret".to_string());
-+
-+    match decode::<Claims>(
-+        token,
-+        &DecodingKey::from_secret(secret.as_ref()),
-+        &Validation::default(),
-+    ) {
-+        Ok(data) => Some(data.claims.sub),
-+        Err(e) => {
-+            eprintln!("JWT decode error: {:?}", e);
-+            None
-+        }
-+    }
-+}
-+
-+fn mpc_nodes() -> (Vec<String>, Vec<u16>) {
-+    let nodes = vec![
-+        env::var("MPC_NODE_1").unwrap_or_else(|_| "http://127.0.0.1:8081".to_string()),
-+        env::var("MPC_NODE_2").unwrap_or_else(|_| "http://127.0.0.1:8082".to_string()),
-+        env::var("MPC_NODE_3").unwrap_or_else(|_| "http://127.0.0.1:8083".to_string()),
-+    ];
-+    let parties = vec![1u16, 2u16, 3u16];
-+    (nodes, parties)
-+}
-+
-+#[derive(Deserialize)]
-+struct PubkeyResponse {
-+    pub pubkey: String,
-+}
-+
-+#[post("/api/v1/signup")]
-+pub async fn sign_up(
-+    pool: web::Data<PgPool>,
-+    req: web::Json<SignUpRequest>,
-+) -> Result<HttpResponse, ActixError> {
-+    let client = Client::new();
-+    let (nodes, _) = mpc_nodes();
-+
-+    let mut all_r1: Vec<serde_json::Value> = Vec::new();
-+    for url in &nodes {
-+        let resp = client
-+            .post(format!("{url}/dkg-round1"))
-+            .send()
-+            .await
-+            .map_err(|e| actix_web::error::ErrorInternalServerError(format!("r1 error: {e}")))?;
-+        let text = resp.text().await.unwrap_or_default();
-+        let r1: serde_json::Value = serde_json::from_str(&text).map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("bad r1 json: {e}, body={text}"))
-+        })?;
-+        all_r1.push(r1);
-+    }
-+
-+    let mut all_r2: Vec<serde_json::Value> = Vec::new();
-+    for (i, url) in nodes.iter().enumerate() {
-+        let peers: Vec<_> = all_r1
-+            .iter()
-+            .enumerate()
-+            .filter(|(j, _)| *j != i)
-+            .map(|(_, v)| v.clone())
-+            .collect();
-+
-+        let resp = client
-+            .post(format!("{url}/dkg-round2-init"))
-+            .json(&peers)
-+            .send()
-+            .await
-+            .map_err(|e| {
-+                actix_web::error::ErrorInternalServerError(format!("r2 init error: {e}"))
-+            })?;
-+        let text = resp.text().await.unwrap_or_default();
-+        let r2: serde_json::Value = serde_json::from_str(&text).map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("bad r2 json: {e}, body={text}"))
-+        })?;
-+        all_r2.push(r2);
-+    }
-+
-+    for (from_idx, r2) in all_r2.iter().enumerate() {
-+        let from_id = (from_idx + 1) as u16;
-+        if let Some(pkgs) = r2["packages"].as_object() {
-+            for (to_str, pkg_hex) in pkgs {
-+                let to_id: usize = to_str.parse().unwrap();
-+                let url = &nodes[to_id - 1];
-+                let body = serde_json::json!({ "from": from_id, "pkg_hex": pkg_hex });
-+                client
-+                    .post(format!("{url}/dkg-round2-recv"))
-+                    .json(&body)
-+                    .send()
-+                    .await
-+                    .map_err(|e| {
-+                        actix_web::error::ErrorInternalServerError(format!("r2 recv error: {e}"))
-+                    })?;
-+            }
-+        }
-+    }
-+
-+    // Finalize DKG on all nodes    t t
-+    for url in &nodes {
-+        let finalize_resp = client
-+            .post(format!("{url}/dkg-finalize"))
-+            .send()
-+            .await
-+            .map_err(|e| {
-+                actix_web::error::ErrorInternalServerError(format!("finalize error: {e}"))
-+            })?;
-         
-+        if !finalize_resp.status().is_success() {
-+            let error_text = finalize_resp.text().await.unwrap_or_default();
-+            return Err(actix_web::error::ErrorInternalServerError(format!(
-+                "DKG finalize failed on {}: {}", url, error_text
-+            )));
-+        }
-+    }
-+
-+    // Get the aggregated public key
-+    let pubkey_resp = client
-+        .get(format!("{}/pubkey", &nodes[0]))
-+        .send()
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("pubkey error: {e}")))?;
-+
-+    let text = pubkey_resp.text().await.unwrap_or_default();
-+    println!("DEBUG /pubkey body = {}", text);
-+
-+    let pk_obj: PubkeyResponse = serde_json::from_str(&text).map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("pubkey parse error: {e}, body={text}"))
-+    })?;
-+    let agg_pubkey = pk_obj.pubkey;
-+
-+    // DKG state is now automatically saved by each MPC node during dkg_finalize
-+    // No need for external save calls - this eliminates timing issues
-+    println!("DKG completed successfully - state automatically saved by MPC nodes");
-+
-+    let password_hash = hash(&req.password, bcrypt::DEFAULT_COST)
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("hash error: {e}")))?;
-+    let user_id = Uuid::new_v4();
-+
-+    sqlx::query!(
-+        "INSERT INTO \"User\" (id, email, password, createdat, updatedat, publickey)
-+         VALUES ($1, $2, $3, NOW(), NOW(), $4)",
-+        user_id,
-+        &req.username,
-+        password_hash,
-+        agg_pubkey
-+    )
-+    .execute(pool.get_ref())
-+    .await
-+    .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB insert error: {e}")))?;
-+
-+        // Note: DKG state is now automatically saved during dkg_finalize with placeholder email
-+    // TODO: In production, add an email-only update endpoint to replace placeholder with real email
-+    println!("DKG state auto-saved with placeholder email - consider adding email update endpoint for production");
-+
-+    Ok(HttpResponse::Ok().json(SignupResponse {
-+        message: "signed up successfully".into(),
-+    }))
-+}
-+
-+#[post("/api/v1/signin")]
-+pub async fn sign_in(
-+    pool: web::Data<PgPool>,
-+    req: web::Json<SignInRequest>,
-+) -> Result<HttpResponse, ActixError> {
-+    // Fetch user by email
-+    let row = sqlx::query!(
-+        r#"
-+        SELECT id, email, password
-+        FROM "User"
-+        WHERE email = $1
-+        "#,
-+        req.username
-+    )
-+    .fetch_optional(pool.get_ref())
-+    .await
-+    .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB query error: {e}")))?;
-+
-+    if let Some(user) = row {
-+        if verify(&req.password, &user.password).unwrap_or(false) {
-+            let jwt = generate_jwt(&user.email);
-+
-+            return Ok(HttpResponse::Ok().json(AuthResponse {
-+                token: jwt,
-+            }));
-+        }
-+    }
-+
-+    Err(actix_web::error::ErrorUnauthorized("Invalid credentials"))
-+}
-+
-+#[get("/api/v1/user")]
-+pub async fn get_user(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+) -> Result<HttpResponse, ActixError> {
-+    if let Some(email) = validate_jwt(&auth.0) {
-+        let row = sqlx::query!("SELECT email FROM \"User\" WHERE email = $1", email)
-+            .fetch_optional(pool.get_ref())
-+            .await
-+            .map_err(|e| actix_web::error::ErrorInternalServerError(e))?;
-+        if let Some(user) = row {
-+            return Ok(HttpResponse::Ok().json(UserResponse { email: user.email }));
-+        }
-+    }
-+    Err(actix_web::error::ErrorUnauthorized("Unauthorized"))
-+}
-+
-+#[post("/api/v1/quote")]
-+pub async fn quote(req: web::Json<QuoteRequest>) -> Result<HttpResponse, ActixError> {
-+    if req.in_amount == 0 {
-+        return Ok(HttpResponse::BadRequest().body("Invalid input amount"));
-+    }
-+    let client = Client::new();
-+    let url = format!(
-+        "{JUP_QUOTE_URL}?inputMint={}&outputMint={}&amount={}&slippageBps=50",
-+        req.input_mint, req.output_mint, req.in_amount
-+    );
-+    let res = client.get(&url).send().await.map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("Jupiter quote error: {e}"))
-+    })?;
-+    if !res.status().is_success() {
-+        return Ok(HttpResponse::BadRequest().body("Invalid input or insufficient balance"));
-+    }
-+    let data: serde_json::Value = res.json().await.map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("Jupiter parse error: {e}"))
-+    })?;
-+    let out_amount: u64 = data["otherAmountThreshold"]
-+        .as_str()
-+        .unwrap_or("0")
-+        .parse()
-+        .unwrap_or(0);
-+    if out_amount == 0 {
-+        return Ok(HttpResponse::BadRequest().body("Invalid quote from Jupiter"));
-+    }
-+    let id = Uuid::new_v4().to_string();
-+    QUOTES.lock().unwrap().insert(id.clone(), data);
-+    Ok(HttpResponse::Ok().json(QuoteResponse {
-+        out_amount: out_amount,
-+        id,
-+    }))
-+}
-+
-+use base64;
-+use bincode;
-+use solana_client::nonblocking::rpc_client::RpcClient;
-+use solana_sdk::transaction::VersionedTransaction;
-+
-+#[post("/api/v1/swap")]
-+pub async fn swap(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+    req: web::Json<SwapRequest>,
-+) -> Result<HttpResponse, ActixError> {
-+    if req.id.is_empty() {
-+        return Ok(HttpResponse::BadRequest().body("Invalid input"));
-+    }
-+
-+    // Get logged-in user email from JWT
-+    let email =
-+        validate_jwt(&auth.0).ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
-+
-+    // Fetch user's aggregate pubkey from DB
-+    let row = sqlx::query!(r#"SELECT publickey FROM "User" WHERE email = $1"#, email)
-+        .fetch_one(pool.get_ref())
-+        .await
-+        .map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("DB fetch user pubkey error: {e}"))
-+        })?;
-+    let user_pubkey = row.publickey;
-+
-+    // Lookup cached quote
-+    let stored = { QUOTES.lock().unwrap().get(&req.id).cloned() };
-+    if stored.is_none() {
-+        return Ok(HttpResponse::BadRequest().body("Quote not found or expired"));
-+    }
-+
-+    // Ask Jupiter for swap tx
-+    let body = serde_json::json!({
-+        "quoteResponse": stored.unwrap(),
-+        "userPublicKey": user_pubkey,
-+        "wrapUnwrapSOL": true,
-+        "useSharedAccounts": false,
-+        "asLegacyTransaction": true,
-+        "restrictIntermediateTokens": true
-+    });
-+
-+    println!("Requesting Jupiter swap with body={}", body);
-+
-+    let client = Client::new();
-+    let res = client
-+        .post(JUP_SWAP_URL)
-+        .json(&body)
-+        .send()
-+        .await
-+        .map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("Jupiter swap error: {e}"))
-+        })?;
-+
-+    let status = res.status();
-+    let text = res.text().await.unwrap_or_default();
-+    println!("Jupiter raw response: status={}, body={}", status, text);
-+
-+    if !status.is_success() {
-+        return Ok(HttpResponse::BadRequest().body("Swap request failed"));
-+    }
-+
-+    let data: serde_json::Value = serde_json::from_str(&text).map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("Swap parse error: {e}, body={text}"))
-+    })?;
-+
-+    let tx_b64 = data["swapTransaction"].as_str().ok_or_else(|| {
-+        actix_web::error::ErrorInternalServerError("No swapTransaction in response")
-+    })?;
-+
-+    // Decode Jupiter tx into VersionedTransaction
-+    let tx_bytes = base64::Engine::decode(&base64::engine::general_purpose::STANDARD, tx_b64).map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("base64 decode error: {e}"))
-+    })?;
-+    let mut tx: VersionedTransaction = bincode::deserialize(&tx_bytes).map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("tx deserialize error: {e}"))
-+    })?;
-+
-+    // Ensure DKG state is loaded before signing
-+    ensure_dkg_loaded(&user_pubkey).await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}")))?;
-+
-+    //  MPC sign message (use Jupiter's blockhash, don't override)
-+    let msg_hex = hex::encode(tx.message.serialize());
-+    println!(
-+        "Message hex (first 64 chars): {}",
-+        &msg_hex[..64.min(msg_hex.len())]
-+    );
-+
-+    let sig = mpc_sign_message(msg_hex)
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC sign failed: {e}")))?;
-+    tx.signatures = vec![sig]; // Replace Jupiter’s placeholder
-+
-+    // Send to Solana RPC
-+    let rpc_url = env::var("SOLANA_RPC_URL")
-+        .unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
-+    let rpc = RpcClient::new(rpc_url);
-+
-+    let signature = rpc
-+        .send_and_confirm_transaction(&tx)
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Send tx error: {e}")))?;
-+
-+    println!("Swap sent successfully. Signature={}", signature);
-+
-+    #[derive(Serialize)]
-+    struct SwapResponseSigned {
-+        message: String,
-+        signature: String,
-+    }
-+
-+    Ok(HttpResponse::Ok().json(SwapResponseSigned {
-+        message: "Swap executed".into(),
-+        signature: signature.to_string(),
-+    }))
-+}
-+
-+use solana_sdk::{
-+    pubkey::Pubkey, signature::Signature, system_instruction, transaction::Transaction,
-+};
-+
-+#[derive(Serialize)]
-+struct SignRound2Request {
-+    message: String,
-+    message_format: String,
-+    commitments: Vec<CommitmentsMsg>,
-+}
-+
-+#[post("/api/v1/send")]
-+pub async fn send(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+    req: web::Json<SendRequest>,
-+) -> Result<HttpResponse, ActixError> {
-+    if req.to.is_empty() || req.amount == 0 {
-+        return Ok(HttpResponse::BadRequest().body("Invalid input"));
-+    }
-+
-+    let email =
-+        validate_jwt(&auth.0).ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
-+    let row = sqlx::query!("SELECT publickey FROM \"User\" WHERE email = $1", email)
-+        .fetch_one(pool.get_ref())
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB error: {e}")))?;
-+    let from_pubkey = Pubkey::from_str(&row.publickey)
-+        .map_err(|_| actix_web::error::ErrorInternalServerError("Bad stored publicKey"))?;
-+
-+    let rpc_url =
-+        env::var("SOLANA_RPC_URL").unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
-+    let rpc = RpcClient::new(rpc_url);
-+
-+    let blockhash = rpc
-+        .get_latest_blockhash()
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Blockhash error: {e}")))?;
-+
-+    let to_pubkey = Pubkey::from_str(&req.to)
-+        .map_err(|_| actix_web::error::ErrorInternalServerError("Invalid recipient pubkey"))?;
-+
-+    let instructions = if let Some(_mint) = &req.mint {
-+        // For now, return an error for token transfers as they require more complex setup
-+        return Ok(HttpResponse::BadRequest().body("Token transfers not yet implemented"));
-+    } else {
-+        // SOL transfer
-+        vec![system_instruction::transfer(&from_pubkey, &to_pubkey, req.amount)]
-     };
--    
--    Ok(HttpResponse::Ok().json(response))
-+
-+    let mut tx = Transaction::new_with_payer(&instructions, Some(&from_pubkey));
-+    tx.message.recent_blockhash = blockhash;
-+
-+    // Ensure DKG state is loaded before signing
-+    ensure_dkg_loaded(&row.publickey).await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}")))?;
-+
-+    let msg_hex = hex::encode(tx.message_data());
-+    let sig = mpc_sign_message(msg_hex)
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC sign failed: {e}")))?;
-+    tx.signatures = vec![sig];
-+
-+    let signature = rpc.send_and_confirm_transaction(&tx).await.map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("Transaction error: {e}"))
-+    })?;
-+
-+    #[derive(Serialize)]
-+    struct SendResponse {
-+        message: String,
-+        signature: String,
-+    }
-+
-+    Ok(HttpResponse::Ok().json(SendResponse {
-+        message: "Transaction sent".into(),
-+        signature: signature.to_string(),
-+    }))
-+}
-+
-+async fn ensure_dkg_loaded(user_pubkey: &str) -> Result<(), String> {
-+    let client = reqwest::Client::new();
-+    let (nodes, _) = mpc_nodes();
-+
-+    for url in &nodes {
-+        let load_resp = client
-+            .post(format!("{url}/load-dkg"))
-+            .json(&serde_json::json!({
-+                "pubkey": user_pubkey
-+            }))
-+            .send()
-+            .await
-+            .map_err(|e| format!("Failed to load DKG state from {}: {}", url, e))?;
-+
-+        let load_result: serde_json::Value = load_resp.json().await
-+            .map_err(|e| format!("Failed to parse load response from {}: {}", url, e))?;
-+
-+        if load_result["status"] == "not_found" {
-+            return Err(format!("DKG state not found for pubkey {} on node {}", user_pubkey, url));
-+        }
-+
-+        println!("Loaded DKG state from {}: {:?}", url, load_result);
-+    }
-+
-+    Ok(())
-+}
-+
-+pub async fn mpc_sign_message(msg_hex: String) -> Result<Signature, String> {
-+    let client = reqwest::Client::new();
-+    let (nodes, _) = mpc_nodes();
-+
-+    let mut commitments: Vec<CommitmentsMsg> = Vec::new();
-+    for url in &nodes {
-+        let resp = client
-+            .post(format!("{url}/sign-round1"))
-+            .json(&serde_json::json!({
-+                "message": msg_hex,
-+                "message_format": "hex"
-+            }))
-+            .send()
-+            .await
-+            .map_err(|e| format!("step1 error on {url}: {e}"))?;
-+
-+        let status = resp.status();
-+        let text = resp.text().await.unwrap_or_else(|_| "<no body>".into());
-+        println!("DEBUG Step1 {} => status: {}, body: {}", url, status, text);
-+
-+        if !status.is_success() {
-+            return Err(format!(
-+                "step1 non-200 on {url} (status {status}, body {text})"
-+            ));
-+        }
-+
-+        let s: CommitmentsMsg = serde_json::from_str(&text)
-+            .map_err(|e| format!("step1 bad json on {url}: {e}, body={text}"))?;
-+        commitments.push(s);
-+    }
-+
-+    let mut shares: Vec<ShareMsg> = Vec::new();
-+    for url in &nodes {
-+        let resp = client
-+            .post(format!("{url}/sign-round2"))
-+            .json(&serde_json::json!({
-+                "message": msg_hex,
-+                "message_format": "hex",
-+                "commitments": commitments
-+            }))
-+            .send()
-+            .await
-+            .map_err(|e| format!("step2 error on {url}: {e}"))?;
-+
-+        let status = resp.status();
-+        let text = resp.text().await.unwrap_or_else(|_| "<no body>".into());
-+        println!("DEBUG Step2 {} => status: {}, body: {}", url, status, text);
-+
-+        if !status.is_success() {
-+            return Err(format!(
-+                "step2 non-200 on {url} (status {status}, body {text})"
-+            ));
-+        }
-+
-+        let s2: ShareMsg = serde_json::from_str(&text)
-+            .map_err(|e| format!("step2 bad json on {url}: {e}, body={text}"))?;
-+        shares.push(s2);
-+    }
-+
-+    let resp = client
-+        .post(format!("{}/aggregate-signatures", &nodes[0]))
-+        .json(&serde_json::json!({
-+            "message": msg_hex,
-+            "message_format": "hex",
-+            "commitments": commitments,
-+            "shares": shares
-+        }))
-+        .send()
-+        .await
-+        .map_err(|e| format!("aggregate error: {e}"))?;
-+
-+    let status = resp.status();
-+    let text = resp.text().await.unwrap_or_else(|_| "<no body>".into());
-+    println!("DEBUG Aggregate => status: {}, body: {}", status, text);
-+
-+    if !status.is_success() {
-+        return Err(format!("aggregate non-200 (status {status}, body {text})"));
-+    }
-+
-+    let agg: AggregateSignatureResponse =
-+        serde_json::from_str(&text).map_err(|e| format!("aggregate bad json: {e}, body={text}"))?;
-+
-+    Signature::from_str(&agg.solana_signature)
-+        .map_err(|_| format!("invalid base58 signature: {}", agg.solana_signature))
-+}
-+
-+#[derive(Serialize)]
-+pub struct SolBalanceResponse {
-+    pub balance: u64,
-+}
-+
-+#[derive(Serialize)]
-+pub struct TokenBalance {
-+    pub balance: u64,
-+    #[serde(rename = "tokenMint")]
-+    pub token_mint: String,
-+    pub symbol: String,
-+    pub decimals: u8,
-+}
-+
-+#[derive(Serialize)]
-+pub struct TokenBalancesResponse {
-+    pub balances: Vec<TokenBalance>,
-+}
-+
-+#[get("/api/v1/balance/sol")]
-+pub async fn get_sol_balance(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+) -> Result<HttpResponse, ActixError> {
-+    let email = validate_jwt(&auth.0)
-+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
-+
-+    let row = sqlx::query!("SELECT publickey FROM \"User\" WHERE email = $1", email)
-+        .fetch_one(pool.get_ref())
-+        .await
-+        .map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("DB fetch user pubkey error: {e}"))
-+        })?;
-+
-+    let user_pubkey = Pubkey::from_str(&row.publickey)
-+        .map_err(|_| actix_web::error::ErrorInternalServerError("Invalid stored publicKey"))?;
-+
-+    let rpc_url = env::var("SOLANA_RPC_URL")
-+        .unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
-+    let rpc = RpcClient::new(rpc_url);
-+
-+    let balance = rpc
-+        .get_balance(&user_pubkey)
-+        .await
-+        .map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("Failed to get balance: {e}"))
-+        })?;
-+
-+    Ok(HttpResponse::Ok().json(SolBalanceResponse { balance }))
-+}
-+
-+#[get("/api/v1/balance/tokens")]
-+pub async fn get_token_balances(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+) -> Result<HttpResponse, ActixError> {
-+    let email = validate_jwt(&auth.0)
-+        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
-+
-+    // Get user's public key from database
-+    let user_row = sqlx::query!("SELECT publickey FROM \"User\" WHERE email = $1", email)
-+        .fetch_one(pool.get_ref())
-+        .await
-+        .map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("DB fetch user pubkey error: {e}"))
-+        })?;
-+
-+    let user_pubkey = user_row.publickey;
-+
-+    // Fetch token balances from database (populated by indexer)
-+    let balance_rows = sqlx::query!(
-+        r#"
-+        SELECT
-+            b.amount as balance,
-+            a.mintAddress as token_mint,
-+            a.symbol,
-+            a.decimals
-+        FROM "Balance" b
-+        JOIN "User" u ON b.userId = u.id
-+        JOIN "Asset" a ON b.assetId = a.id
-+        WHERE u.publickey = $1
-+        ORDER BY b.amount DESC
-+        "#,
-+        user_pubkey
-+    )
-+    .fetch_all(pool.get_ref())
-+    .await
-+    .map_err(|e| {
-+        actix_web::error::ErrorInternalServerError(format!("DB fetch token balances error: {e}"))
-+    })?;
-+
-+    // Convert database rows to response format
-+    let mut balances = Vec::new();
-+    for row in balance_rows {
-+        balances.push(TokenBalance {
-+            balance: row.balance as u64,
-+            token_mint: row.token_mint,
-+            symbol: row.symbol,
-+            decimals: row.decimals as u8,
-+        });
-+    }
-+
-+    Ok(HttpResponse::Ok().json(TokenBalancesResponse { balances }))
- }
-diff --git a/backend/src/routes/user.rs b/backend/src/routes/user.rs
-index 744f978..b9e06e4 100644
---- a/backend/src/routes/user.rs
-+++ b/backend/src/routes/user.rs
-@@ -1,20 +1,32 @@
-+use crate::auth_middleware::AuthToken;
-+use actix_web::{get, post};
- use actix_web::{web, HttpResponse, Result};
-+use bcrypt::{hash, verify};
-+use chrono::{Duration, Utc};
-+use jsonwebtoken::{decode, encode, DecodingKey, EncodingKey, Header, Validation};
-+use reqwest::Client;
- use serde::{Deserialize, Serialize};
-+use sqlx::PgPool;
-+use std::env;
-+use uuid::Uuid;
-+use store::mpc_key::SaveKeyRequest;
-+use store::mpc_store::MPCStore;
- 
- #[derive(Deserialize)]
- pub struct SignUpRequest {
--    pub email: String,
-+    pub username: String,
-     pub password: String,
- }
- 
- #[derive(Deserialize)]
- pub struct SignInRequest {
--    pub email: String,
-+    pub username: String,
-     pub password: String,
- }
- 
- #[derive(Serialize)]
- pub struct UserResponse {
-+    pub email: String,
- }
- 
- #[derive(Serialize)]
-@@ -27,31 +39,269 @@ pub struct SignupResponse {
-     message: String,
- }
- 
--#[actix_web::post("/signup")]
--pub async fn sign_up(req: web::Json<SignUpRequest>) -> Result<HttpResponse> {
--    let response = SignupResponse {
--        message: "User created successfully".to_string(),
--    };
--    
--    Ok(HttpResponse::Created().json(response))
-+#[derive(Debug, Serialize, Deserialize)]
-+struct Claims {
-+    sub: String,
-+    exp: usize,
- }
- 
--#[actix_web::post("/signin")]
--pub async fn sign_in(req: web::Json<SignInRequest>) -> Result<HttpResponse> {
--    let response = AuthResponse {
--        token: "temporary_token".to_string(),
--    };
--    
--    Ok(HttpResponse::Ok().json(response))
-+#[derive(Deserialize, Serialize)]
-+struct Round1Resp {
-+    id: u16,
-+    pkg_hex: String,
- }
- 
--#[actix_web::get("/user/{id}")]
--pub async fn get_user(path: web::Path<u32>) -> Result<HttpResponse> {
--    let user_id = path.into_inner();
--    
--    let user = UserResponse {
--       
-+#[derive(Deserialize)]
-+struct Round2InitResp {
-+    from: u16,
-+    packages: std::collections::HashMap<u16, String>,
-+}
-+
-+#[derive(Deserialize, Serialize)]
-+struct Round2RecvReq {
-+    from: u16,
-+    pkg_hex: String,
-+}
-+
-+#[derive(Deserialize)]
-+struct Round2Resp {
-+    id: u16,
-+    pubkey: String,
-+}
-+
-+fn generate_jwt(email: &str) -> String {
-+    let secret = env::var("JWT_SECRET").unwrap_or("secret".to_string());
-+
-+    let expiration = Utc::now()
-+        .checked_add_signed(Duration::hours(24))
-+        .expect("valid timestamp")
-+        .timestamp() as usize;
-+
-+    let claims = Claims {
-+        sub: email.to_owned(),
-+        exp: expiration,
-     };
-+
-+    encode(
-+        &Header::default(),
-+        &claims,
-+        &EncodingKey::from_secret(secret.as_ref()),
-+    )
-+    .expect("JWT should be generated")
-+}
-+
-+fn validate_jwt(token: &str) -> Option<String> {
-+    let secret = env::var("JWT_SECRET").unwrap_or("secret".to_string());
-+
-+    match decode::<Claims>(
-+        token,
-+        &DecodingKey::from_secret(secret.as_ref()),
-+        &Validation::default(),
-+    ) {
-+        Ok(data) => Some(data.claims.sub),
-+        Err(_) => None,
-+    }
-+}
-+
-+#[post("/api/v1/signup")]
-+pub async fn sign_up(
-+    pool: web::Data<PgPool>,
-+    req: web::Json<SignUpRequest>,
-+) -> Result<HttpResponse, actix_web::Error> {
-+    let existing = sqlx::query!("SELECT id FROM \"User\" WHERE email = $1", &req.username)
-+        .fetch_optional(pool.get_ref())
-+        .await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB error: {}", e)))?;
-+
-+    if existing.is_some() {
-+        return Err(actix_web::error::ErrorUnauthorized("User already exists"));
-+    }
-+
-+    let password_hash = hash(&req.password, bcrypt::DEFAULT_COST)
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Hash error: {}", e)))?;
-+
-+    let client = Client::new();
-+    let mpc_nodes = vec![
-+        ("http://127.0.0.1:8081", 1),
-+        ("http://127.0.0.1:8082", 2),
-+        ("http://127.0.0.1:8083", 3),
-+    ];
-+
-+    let mut all_r1: Vec<Round1Resp> = Vec::new();
-+    for (url, _) in &mpc_nodes {
-+        let resp = client
-+            .post(format!("{url}/dkg-round1"))
-+            .send()
-+            .await
-+            .map_err(|e| {
-+                actix_web::error::ErrorInternalServerError(format!("MPC /dkg-round1 failed: {}", e))
-+            })?;
-+
-+        let r1_resp: Round1Resp = resp.json().await.map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!("Invalid /dkg-round1 resp: {}", e))
-+        })?;
-+
-+        all_r1.push(r1_resp);
-+    }
-+
-+    println!("Collected {} Round-1 packages", all_r1.len());
-+
-+    let mut all_r2_responses: Vec<Round2InitResp> = Vec::new();
-+    for (url, _) in &mpc_nodes {
-+        let r2_resp = client
-+            .post(format!("{url}/dkg-round2-init"))
-+            .json(&all_r1)
-+            .send()
-+            .await
-+            .map_err(|e| {
-+                actix_web::error::ErrorInternalServerError(format!(
-+                    "MPC /dkg-round2-init failed on {url}: {}",
-+                    e
-+                ))
-+            })?;
-+
-+        let r2_json: Round2InitResp = r2_resp.json().await.map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!(
-+                "Invalid /dkg-round2-init resp from {url}: {}",
-+                e
-+            ))
-+        })?;
-+
-+        all_r2_responses.push(r2_json);
-+    }
-+
-+    for (url, node_id) in &mpc_nodes {
-+        for r2_resp in &all_r2_responses {
-+            if r2_resp.from != *node_id {
-+                for (recipient_id, pkg_hex) in &r2_resp.packages {
-+                    if *recipient_id == *node_id {
-+                        let recv_req = Round2RecvReq {
-+                            from: r2_resp.from,
-+                            pkg_hex: pkg_hex.clone(),
-+                        };
-+                        
-+                        let _ = client
-+                            .post(format!("{url}/dkg-round2-recv"))
-+                            .json(&recv_req)
-+                            .send()
-+                            .await;
-+                    }
-+                }
-+            }
-+        }
-+    }
-+
-+    let mut agg_pk_str = String::new();
-+    for (url, _) in &mpc_nodes {
-+        let finalize_resp = client
-+            .post(format!("{url}/dkg-finalize"))
-+            .send()
-+            .await
-+            .map_err(|e| {
-+                actix_web::error::ErrorInternalServerError(format!(
-+                    "MPC /dkg-finalize failed on {url}: {}",
-+                    e
-+                ))
-+            })?;
-+
-+        let finalize_json: Round2Resp = finalize_resp.json().await.map_err(|e| {
-+            actix_web::error::ErrorInternalServerError(format!(
-+                "Invalid /dkg-finalize resp from {url}: {}",
-+                e
-+            ))
-+        })?;
-+
-+        println!("Node {url} finalized with pubkey = {}", finalize_json.pubkey);
-+        agg_pk_str = finalize_json.pubkey;
-+    }
-+
-+    let mpc1_pool = PgPool::connect("postgres://mpc:mpcpass@localhost:5433/mpc1").await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC1 DB connect error: {}", e)))?;
-+    let mpc2_pool = PgPool::connect("postgres://mpc:mpcpass@localhost:5434/mpc2").await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC2 DB connect error: {}", e)))?;
-+    let mpc3_pool = PgPool::connect("postgres://mpc:mpcpass@localhost:5435/mpc3").await
-+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC3 DB connect error: {}", e)))?;
-+
-+    let mpc1_store = MPCStore::new(mpc1_pool);
-+    let mpc2_store = MPCStore::new(mpc2_pool);
-+    let mpc3_store = MPCStore::new(mpc3_pool);
-+
-+    for (_, node_id) in &mpc_nodes {
-+        let key_pkg = vec![1, 2, 3, 4]; // This should be the actual key package from DKG
-+        let save_req = SaveKeyRequest {
-+            pubkey: agg_pk_str.clone(),
-+            user_email: req.username.clone(),
-+            node_id: *node_id as i16,
-+            key_pkg,
-+        };
-+
-+        match node_id {
-+            1 => mpc1_store.save_mpc_key(save_req).await
-+                .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC1 save error: {}", e)))?,
-+            2 => mpc2_store.save_mpc_key(save_req).await
-+                .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC2 save error: {}", e)))?,
-+            3 => mpc3_store.save_mpc_key(save_req).await
-+                .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC3 save error: {}", e)))?,
-+            _ => return Err(actix_web::error::ErrorInternalServerError("Invalid node ID".to_string())),
-+        }
-+    }
-     
--    Ok(HttpResponse::Ok().json(user))
-+    let user_id = Uuid::new_v4();
-+    sqlx::query!(
-+        "INSERT INTO \"User\" (id, email, password, createdAt, updatedAt, publicKey)
-+         VALUES ($1, $2, $3, NOW(), NOW(), $4)",
-+        user_id,
-+        &req.username,
-+        password_hash,
-+        agg_pk_str
-+    )
-+    .execute(pool.get_ref())
-+    .await
-+    .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB insert error: {}", e)))?;
-+
-+    Ok(HttpResponse::Ok().json(SignupResponse {
-+        message: "signed up successfully".to_string(),
-+    }))
-+}
-+
-+#[post("/api/v1/signin")]
-+pub async fn sign_in(
-+    pool: web::Data<PgPool>,
-+    req: web::Json<SignInRequest>,
-+) -> Result<HttpResponse, actix_web::Error> {
-+    let row = sqlx::query!(
-+        "SELECT email, password FROM \"User\" WHERE email = $1",
-+        req.username
-+    )
-+    .fetch_optional(pool.get_ref())
-+    .await
-+    .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB query error: {}", e)))?;
-+
-+    if let Some(user) = row {
-+        if verify(&req.password, &user.password).unwrap_or(false) {
-+            let jwt = generate_jwt(&user.email);
-+            return Ok(HttpResponse::Ok().json(AuthResponse { token: jwt }));
-+        }
-+    }
-+
-+    Err(actix_web::error::ErrorUnauthorized("Invalid credentials"))
-+}
-+
-+#[get("/api/v1/user")]
-+pub async fn get_user(
-+    pool: web::Data<PgPool>,
-+    auth: AuthToken,
-+) -> Result<HttpResponse, actix_web::Error> {
-+    if let Some(email) = validate_jwt(&auth.0) {
-+        let row = sqlx::query!("SELECT email FROM \"User\" WHERE email = $1", email)
-+            .fetch_optional(pool.get_ref())
-+            .await
-+            .map_err(|e| actix_web::error::ErrorInternalServerError(e))?;
-+
-+        if let Some(user) = row {
-+            return Ok(HttpResponse::Ok().json(UserResponse { email: user.email }));
-+        }
-+    }
-+
-+    Err(actix_web::error::ErrorUnauthorized("Unauthorized"))
- }
-diff --git a/docker-compose.yml b/docker-compose.yml
-new file mode 100644
-index 0000000..5c947cf
---- /dev/null
-+++ b/docker-compose.yml
-@@ -0,0 +1,44 @@
-+services:
-+  pg_mpc1:
-+    image: postgres:15
-+    container_name: pg_mpc1
-+    environment:
-+      POSTGRES_USER: mpc
-+      POSTGRES_PASSWORD: mpcpass
-+      POSTGRES_DB: mpc1
-+    ports:
-+      - "5433:5432"
-+    volumes:
-+      - pgdata_mpc1:/var/lib/postgresql/data
-+      - ./store/migrations:/docker-entrypoint-initdb.d:ro
-+
-+  pg_mpc2:
-+    image: postgres:15
-+    container_name: pg_mpc2
-+    environment:
-+      POSTGRES_USER: mpc
-+      POSTGRES_PASSWORD: mpcpass
-+      POSTGRES_DB: mpc2
-+    ports:
-+      - "5434:5432"
-+    volumes:
-+      - pgdata_mpc2:/var/lib/postgresql/data
-+      - ./store/migrations:/docker-entrypoint-initdb.d:ro
-+
-+  pg_mpc3:
-+    image: postgres:15
-+    container_name: pg_mpc3
-+    environment:
-+      POSTGRES_USER: mpc
-+      POSTGRES_PASSWORD: mpcpass
-+      POSTGRES_DB: mpc3
-+    ports:
-+      - "5435:5432"
-+    volumes:
-+      - pgdata_mpc3:/var/lib/postgresql/data
-+      - ./store/migrations:/docker-entrypoint-initdb.d:ro
-+
-+volumes:
-+  pgdata_mpc1:
-+  pgdata_mpc2:
-+  pgdata_mpc3:
-diff --git a/indexer/.sqlx/query-3faacadd8356f58408fc1e3fbdd5817313c7dd42422c9f63c4e657b9edf82ab9.json b/indexer/.sqlx/query-3faacadd8356f58408fc1e3fbdd5817313c7dd42422c9f63c4e657b9edf82ab9.json
-new file mode 100644
-index 0000000..9d0e4e0
---- /dev/null
-+++ b/indexer/.sqlx/query-3faacadd8356f58408fc1e3fbdd5817313c7dd42422c9f63c4e657b9edf82ab9.json
-@@ -0,0 +1,15 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "\n                            UPDATE \"Balance\"\n                            SET amount = $1, updatedAt = NOW()\n                            WHERE userId = (SELECT id FROM \"User\" WHERE publicKey = $2)\n                            AND assetId = (SELECT id FROM \"Asset\" WHERE mintAddress = 'So11111111111111111111111111111111111111112')\n                            ",
-+  "describe": {
-+    "columns": [],
-+    "parameters": {
-+      "Left": [
-+        "Int8",
-+        "Text"
-+      ]
-+    },
-+    "nullable": []
-+  },
-+  "hash": "3faacadd8356f58408fc1e3fbdd5817313c7dd42422c9f63c4e657b9edf82ab9"
-+}
-diff --git a/indexer/.sqlx/query-c4e8bdf3e31f8930e6877fdb7a904cb14e2cf76033c7dbd2674fb479d0722ada.json b/indexer/.sqlx/query-c4e8bdf3e31f8930e6877fdb7a904cb14e2cf76033c7dbd2674fb479d0722ada.json
-new file mode 100644
-index 0000000..883fc2e
---- /dev/null
-+++ b/indexer/.sqlx/query-c4e8bdf3e31f8930e6877fdb7a904cb14e2cf76033c7dbd2674fb479d0722ada.json
-@@ -0,0 +1,20 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT publickey FROM \"User\"",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "publickey",
-+        "type_info": "Text"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": []
-+    },
-+    "nullable": [
-+      false
-+    ]
-+  },
-+  "hash": "c4e8bdf3e31f8930e6877fdb7a904cb14e2cf76033c7dbd2674fb479d0722ada"
-+}
-diff --git a/indexer/Cargo.toml b/indexer/Cargo.toml
-index 48fa0ee..02696f8 100644
---- a/indexer/Cargo.toml
-+++ b/indexer/Cargo.toml
-@@ -4,8 +4,15 @@ version = "0.1.0"
- edition = "2024"
- 
- [dependencies]
--tokio = { version = "1.0", features = ["full"] }
--tonic = "0.14.2"
--bytes = "1.10.1"
--futures = "0.3.31"
--yellowstone-grpc-proto = "9.0.0"
-+tokio = { version = "1", features = ["full"] }
-+anyhow = "1"
-+sqlx = { version = "0.8.6", features = ["postgres", "runtime-tokio-rustls", "uuid", "chrono"] }
-+solana-sdk = "2.2.2"
-+solana-client = "2.2.2"
-+dotenvy = "0.15"
-+reqwest = { version = "0.12", features = ["json"] }
-+serde_json = "1"
-+
-+[[bin]]
-+name = "test_indexer"
-+path = "src/test_indexer.rs"
-diff --git a/indexer/src/main.rs b/indexer/src/main.rs
-index 74ef4c4..549890d 100644
---- a/indexer/src/main.rs
-+++ b/indexer/src/main.rs
-@@ -1,12 +1,522 @@
--use yellowstone::GeyserGrpcClient;
--pub mod yellowstone;
-+use dotenvy::dotenv;
-+use solana_sdk::pubkey::Pubkey;
-+use sqlx::PgPool;
-+use std::collections::HashSet;
-+use std::env;
-+use std::str::FromStr;
-+use reqwest::Client;
-+use solana_client::rpc_client::RpcClient;
-+
-+// Helius API structures - using flexible JSON parsing
-+type HeliusTransaction = serde_json::Value;
-+type NativeTransfer = serde_json::Value;
-+type TokenTransfer = serde_json::Value;
-+
-+async fn load_tracked_pubkeys(pool: &PgPool) -> anyhow::Result<HashSet<Pubkey>> {
-+    let rows = sqlx::query!(r#"SELECT publickey FROM "User""#)
-+        .fetch_all(pool)
-+        .await?;
-+    
-+    let mut tracked_pubkeys = HashSet::new();
-+    let mut invalid_pubkeys = Vec::new();
-+    
-+    for row in rows {
-+        match Pubkey::from_str(&row.publickey) {
-+            Ok(pubkey) => {
-+                tracked_pubkeys.insert(pubkey);
-+            }
-+            Err(e) => {
-+                invalid_pubkeys.push((row.publickey, e));
-+            }
-+        }
-+    }
-+    
-+    if !invalid_pubkeys.is_empty() {
-+        eprintln!("Warning: Found {} invalid public keys in database:", invalid_pubkeys.len());
-+        for (pubkey_str, error) in invalid_pubkeys {
-+            eprintln!("  - {}: {}", pubkey_str, error);
-+        }
-+    }
-+    
-+    Ok(tracked_pubkeys)
-+}
-+
-+async fn ensure_sol_asset(pool: &PgPool) -> anyhow::Result<()> {
-+    const SOL_MINT: &str = "So11111111111111111111111111111111111111112";
-+    
-+    // Check if SOL asset exists
-+    let exists = sqlx::query!(
-+        r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#,
-+        SOL_MINT
-+    )
-+    .fetch_optional(pool)
-+    .await?;
-+    
-+    if exists.is_none() {
-+        println!("SOL asset not found, creating it...");
-+        sqlx::query!(
-+            r#"
-+            INSERT INTO "Asset" (mintAddress, decimals, name, symbol, logoUrl)
-+            VALUES ($1, $2, $3, $4, $5)
-+            "#,
-+            SOL_MINT,
-+            9i32,
-+            "Solana",
-+            "SOL",
-+            "https://raw.githubusercontent.com/solana-labs/token-list/main/assets/mainnet/So11111111111111111111111111111111111111112/logo.png"
-+        )
-+        .execute(pool)
-+        .await?;
-+        println!("SOL asset created successfully");
-+    } else {
-+        println!("SOL asset already exists");
-+    }
-+    
-+    Ok(())
-+}
-+
-+async fn fetch_helius_transactions(
-+    client: &Client,
-+    api_key: &str,
-+    address: &str,
-+    network: &str,
-+) -> anyhow::Result<Vec<HeliusTransaction>> {
-+    let url = format!(
-+        "https://api.helius.xyz/v0/addresses/{}/transactions?api-key={}&cluster={}",
-+        address, api_key, network
-+    );
-+    
-+    println!("🔍 Fetching transactions for {} on {}: {}", address, network, url);
-+    
-+    let response = client
-+        .get(&url)
-+        .send()
-+        .await?;
-+    
-+    let status = response.status();
-+    if !status.is_success() {
-+        let error_text = response.text().await.unwrap_or_default();
-+        return Err(anyhow::anyhow!("Helius API error: {} - {}", status, error_text));
-+    }
-+    
-+    let response_text = response.text().await?;
-+    println!("Raw Helius API response: {}", &response_text[..500.min(response_text.len())]);
-+    
-+    let transactions: Vec<HeliusTransaction> = serde_json::from_str(&response_text)?;
-+    println!("📊 Parsed {} transactions", transactions.len());
-+    Ok(transactions)
-+}
-+
-+async fn fetch_transaction_by_signature(
-+    client: &Client,
-+    api_key: &str,
-+    signature: &str,
-+    network: &str,
-+) -> anyhow::Result<Option<HeliusTransaction>> {
-+    let url = format!(
-+        "https://api.helius.xyz/v0/transactions?api-key={}&cluster={}&signature={}",
-+        api_key, network, signature
-+    );
-+    
-+    println!("🔍 Looking up transaction by signature: {}", url);
-+    
-+    let response = client
-+        .get(&url)
-+        .send()
-+        .await?;
-+    
-+    let status = response.status();
-+    if !status.is_success() {
-+        let error_text = response.text().await.unwrap_or_default();
-+        return Err(anyhow::anyhow!("Helius API error: {} - {}", status, error_text));
-+    }
-+    
-+    let response_text = response.text().await?;
-+    println!("Raw transaction lookup response: {}", &response_text[..500.min(response_text.len())]);
-+    
-+    let transactions: Vec<HeliusTransaction> = serde_json::from_str(&response_text)?;
-+    Ok(transactions.into_iter().next())
-+}
-+
-+async fn fetch_rpc_transactions(
-+    rpc_client: &RpcClient,
-+    address: &Pubkey,
-+) -> anyhow::Result<Vec<serde_json::Value>> {
-+    println!("🔍 Fetching transactions from Solana RPC for {}", address);
-+    
-+    // Get the current SOL balance from the blockchain
-+    let current_balance = rpc_client
-+        .get_balance(address)
-+        .map_err(|e| anyhow::anyhow!("RPC error getting balance: {}", e))?;
-+    
-+    println!("💰 Current SOL balance for {}: {} lamports", address, current_balance);
-+    
-+    // Create a single transaction entry that represents the current balance sync
-+    let mut tx_json = serde_json::Map::new();
-+    tx_json.insert("signature".to_string(), serde_json::Value::String("balance_sync".to_string()));
-+    tx_json.insert("slot".to_string(), serde_json::Value::Number(serde_json::Number::from(0)));
-+    tx_json.insert("blockTime".to_string(), serde_json::Value::Number(serde_json::Number::from(0)));
-+    
-+    // Create a native transfer entry with the current balance
-+    let mut native_transfers = Vec::new();
-+    let mut transfer = serde_json::Map::new();
-+    transfer.insert("fromUserAccount".to_string(), serde_json::Value::String(address.to_string()));
-+    transfer.insert("toUserAccount".to_string(), serde_json::Value::String(address.to_string()));
-+    transfer.insert("amount".to_string(), serde_json::Value::Number(serde_json::Number::from(current_balance)));
-+    native_transfers.push(serde_json::Value::Object(transfer));
-+    tx_json.insert("nativeTransfers".to_string(), serde_json::Value::Array(native_transfers));
-+    
-+    let mut transactions = Vec::new();
-+    transactions.push(serde_json::Value::Object(tx_json));
-+    
-+    Ok(transactions)
-+}
-+
-+async fn process_helius_transaction(
-+    pool: &PgPool,
-+    transaction: &HeliusTransaction,
-+    tracked_pubkeys: &HashSet<Pubkey>,
-+) -> anyhow::Result<()> {
-+    // Process native transfers (SOL)
-+    if let Some(native_transfers) = transaction.get("nativeTransfers").and_then(|v| v.as_array()) {
-+        for transfer in native_transfers {
-+            if let (Some(from_pubkey), Some(to_pubkey), Some(amount)) = (
-+                transfer.get("fromUserAccount").and_then(|v| v.as_str()),
-+                transfer.get("toUserAccount").and_then(|v| v.as_str()),
-+                transfer.get("amount").and_then(|v| v.as_u64()),
-+            ) {
-+                // Check if either account is tracked
-+                let from_tracked = Pubkey::from_str(from_pubkey).ok()
-+                    .map(|pk| tracked_pubkeys.contains(&pk))
-+                    .unwrap_or(false);
-+                let to_tracked = Pubkey::from_str(to_pubkey).ok()
-+                    .map(|pk| tracked_pubkeys.contains(&pk))
-+                    .unwrap_or(false);
-+                
-+                if from_tracked || to_tracked {
-+                    if amount > 0 {
-+                        println!("Processing SOL transfer: {} -> {} ({} lamports)", 
-+                            from_pubkey, to_pubkey, amount);
-+                        
-+                        // Check if this is a balance sync (same from/to address)
-+                        if from_pubkey == to_pubkey {
-+                            // This is a balance sync, set the balance directly
-+                            println!("🔄 Syncing SOL balance for {}: {} lamports", from_pubkey, amount);
-+                            sync_sol_balance(pool, from_pubkey, amount as i64).await?;
-+                        } else {
-+                            // This is a real transfer, update balances
-+                            if from_tracked {
-+                                update_sol_balance(pool, from_pubkey, -(amount as i64)).await?;
-+                            }
-+                            if to_tracked {
-+                                update_sol_balance(pool, to_pubkey, amount as i64).await?;
-+                            }
-+                        }
-+                    } else {
-+                        println!("Skipping zero-amount transfer: {} -> {} ({} lamports)", 
-+                            from_pubkey, to_pubkey, amount);
-+                    }
-+                }
-+            }
-+        }
-+    }
-+    
-+    // Process token transfers
-+    if let Some(token_transfers) = transaction.get("tokenTransfers").and_then(|v| v.as_array()) {
-+        for transfer in token_transfers {
-+            if let (Some(from_pubkey), Some(to_pubkey), Some(token_amount), Some(mint)) = (
-+                transfer.get("fromUserAccount").and_then(|v| v.as_str()),
-+                transfer.get("toUserAccount").and_then(|v| v.as_str()),
-+                transfer.get("tokenAmount").and_then(|v| v.as_str()),
-+                transfer.get("mint").and_then(|v| v.as_str()),
-+            ) {
-+                // Check if either account is tracked
-+                let from_tracked = Pubkey::from_str(from_pubkey).ok()
-+                    .map(|pk| tracked_pubkeys.contains(&pk))
-+                    .unwrap_or(false);
-+                let to_tracked = Pubkey::from_str(to_pubkey).ok()
-+                    .map(|pk| tracked_pubkeys.contains(&pk))
-+                    .unwrap_or(false);
-+                
-+                if from_tracked || to_tracked {
-+                    println!("Processing token transfer: {} -> {} ({} {})", 
-+                        from_pubkey, to_pubkey, token_amount, mint);
-+                    
-+                    // Update token balance for the tracked account
-+                    if from_tracked {
-+                        update_token_balance(pool, from_pubkey, mint, 
-+                            -(token_amount.parse::<i64>().unwrap_or(0))).await?;
-+                    }
-+                    if to_tracked {
-+                        update_token_balance(pool, to_pubkey, mint, 
-+                            token_amount.parse::<i64>().unwrap_or(0)).await?;
-+                    }
-+                }
-+            }
-+        }
-+    }
-+    
-+    Ok(())
-+}
-+
-+async fn update_sol_balance(pool: &PgPool, pubkey: &str, amount_change: i64) -> anyhow::Result<()> {
-+    // Get current balance
-+    let current_balance = sqlx::query!(
-+        r#"
-+        SELECT b.amount 
-+        FROM "Balance" b
-+        JOIN "User" u ON b.userId = u.id
-+        JOIN "Asset" a ON b.assetId = a.id
-+        WHERE u.publickey = $1 AND a.mintAddress = 'So11111111111111111111111111111111111111112'
-+        "#,
-+        pubkey
-+    )
-+    .fetch_optional(pool)
-+    .await?
-+    .map(|row| row.amount)
-+    .unwrap_or(0);
-+    
-+    let new_balance = (current_balance + amount_change).max(0);
-+    
-+    // Use upsert to create or update balance
-+    sqlx::query!(
-+        r#"
-+        INSERT INTO "Balance" (userId, assetId, amount, createdAt, updatedAt)
-+        VALUES (
-+            (SELECT id FROM "User" WHERE publickey = $1),
-+            (SELECT id FROM "Asset" WHERE mintAddress = 'So11111111111111111111111111111111111111112'),
-+            $2,
-+            NOW(),
-+            NOW()
-+        )
-+        ON CONFLICT (userId, assetId)
-+        DO UPDATE SET amount = $2, updatedAt = NOW()
-+        "#,
-+        pubkey,
-+        new_balance
-+    )
-+    .execute(pool)
-+    .await?;
-+    
-+    println!("Updated SOL balance for {}: {} lamports", pubkey, new_balance);
-+    Ok(())
-+}
-+
-+async fn sync_sol_balance(pool: &PgPool, pubkey: &str, balance: i64) -> anyhow::Result<()> {
-+    // Set the balance directly (for balance sync)
-+    sqlx::query!(
-+        r#"
-+        INSERT INTO "Balance" (userId, assetId, amount, createdAt, updatedAt)
-+        VALUES (
-+            (SELECT id FROM "User" WHERE publickey = $1),
-+            (SELECT id FROM "Asset" WHERE mintAddress = 'So11111111111111111111111111111111111111112'),
-+            $2,
-+            NOW(),
-+            NOW()
-+        )
-+        ON CONFLICT (userId, assetId)
-+        DO UPDATE SET amount = $2, updatedAt = NOW()
-+        "#,
-+        pubkey,
-+        balance
-+    )
-+    .execute(pool)
-+    .await?;
-+    
-+    println!("🔄 Synced SOL balance for {}: {} lamports", pubkey, balance);
-+    Ok(())
-+}
-+
-+async fn update_token_balance(pool: &PgPool, pubkey: &str, mint: &str, amount_change: i64) -> anyhow::Result<()> {
-+    // Ensure asset exists
-+    let asset_exists = sqlx::query!(
-+        r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#,
-+        mint
-+    )
-+    .fetch_optional(pool)
-+    .await?;
-+    
-+    if asset_exists.is_none() {
-+        // Create asset if it doesn't exist
-+        sqlx::query!(
-+            r#"
-+            INSERT INTO "Asset" (mintAddress, decimals, name, symbol)
-+            VALUES ($1, $2, $3, $4)
-+            "#,
-+            mint,
-+            6i32, // Default decimals
-+            "Unknown Token",
-+            "UNK"
-+        )
-+        .execute(pool)
-+        .await?;
-+    }
-+    
-+    // Get current balance
-+    let current_balance = sqlx::query!(
-+        r#"
-+        SELECT b.amount 
-+        FROM "Balance" b
-+        JOIN "User" u ON b.userId = u.id
-+        JOIN "Asset" a ON b.assetId = a.id
-+        WHERE u.publickey = $1 AND a.mintAddress = $2
-+        "#,
-+        pubkey,
-+        mint
-+    )
-+    .fetch_optional(pool)
-+    .await?
-+    .map(|row| row.amount)
-+    .unwrap_or(0);
-+    
-+    let new_balance = (current_balance + amount_change).max(0);
-+    
-+    sqlx::query!(
-+        r#"
-+        UPDATE "Balance"
-+        SET amount = $1, updatedAt = NOW()
-+        WHERE userId = (SELECT id FROM "User" WHERE publickey = $2)
-+        AND assetId = (SELECT id FROM "Asset" WHERE mintAddress = $3)
-+        "#,
-+        new_balance,
-+        pubkey,
-+        mint
-+    )
-+    .execute(pool)
-+    .await?;
-+    
-+    println!("Updated token balance for {}: {} {}", pubkey, new_balance, mint);
-+    Ok(())
-+}
- 
- #[tokio::main]
--async fn main() {   
--    let client = GeyserGrpcClient::new(HealthClient::new(), GeyserClient::new());
--    client.health_check().await;
-+async fn main() -> anyhow::Result<()> {
-+    dotenv().ok();
-+
-+    let db_url = env::var("DATABASE_URL")?;
-+    let pool = PgPool::connect(&db_url).await?;
- 
-+    let mut tracked_pubkeys = load_tracked_pubkeys(&pool).await?;
-+    println!("Loaded {} valid tracked pubkeys from DB", tracked_pubkeys.len());
-+
-+    if tracked_pubkeys.is_empty() {
-+        println!("No users found in database. Indexer will wait for users to be added.");
-+        println!("You can add users through the backend API, and the indexer will automatically start tracking them.");
-+    }
-+
-+    // Ensure SOL asset exists in database
-+    ensure_sol_asset(&pool).await?;
-+
-+    let helius_api_key = env::var("HELIUS_API_KEY").unwrap_or_else(|_| "63ae33a5-5979-4f58-bca8-9140418a4a8b".to_string());
-+    let network = env::var("SOLANA_NETWORK").unwrap_or_else(|_| "devnet".to_string());
-+    
-+    // Create HTTP client for Helius API
-+    let http_client = Client::new();
-     
-+    // Create RPC client as fallback
-+    let rpc_url = match network.as_str() {
-+        "devnet" => "https://api.devnet.solana.com",
-+        "mainnet-beta" => "https://api.mainnet-beta.solana.com",
-+        _ => "https://api.devnet.solana.com",
-+    };
-+    let rpc_client = RpcClient::new(rpc_url.to_string());
- 
-+    println!("🚀 Starting Helius-based indexer...");
-+    println!("Using Helius API key: {}...", &helius_api_key[..8]);
-+    println!("🌐 Network: {}", network);
- 
-+    // Test lookup of the specific transaction signature
-+    let test_signature = "3aPkVkR8fj4qsycKVTaD2BtekbAM1fgfeCrMqdtW3RS2yXDXJxyyS39TsLrs41RA2MdbQbcDsyNrRaVNB68ED3Lb";
-+    println!("🧪 Testing transaction lookup for signature: {}", test_signature);
-+    match fetch_transaction_by_signature(&http_client, &helius_api_key, test_signature, &network).await {
-+        Ok(Some(transaction)) => {
-+            println!("✅ Found transaction by signature!");
-+            if let Err(e) = process_helius_transaction(&pool, &transaction, &tracked_pubkeys).await {
-+                eprintln!("❌ Error processing test transaction: {}", e);
-+            }
-+        }
-+        Ok(None) => {
-+            println!("❌ Transaction not found by signature");
-+        }
-+        Err(e) => {
-+            eprintln!("❌ Error looking up transaction by signature: {}", e);
-+        }
-+    }
-+
-+    // Main indexing loop
-+    let mut last_refresh = std::time::Instant::now();
-+    let mut last_helius_poll = std::time::Instant::now();
-+    let refresh_interval = std::time::Duration::from_secs(60); // Refresh every minute
-+    let helius_poll_interval = std::time::Duration::from_secs(30); // Poll Helius every 30 seconds
-+    
-+    loop {
-+        // Refresh tracked pubkeys periodically
-+        if last_refresh.elapsed() >= refresh_interval {
-+            println!("🔄 Refreshing tracked public keys from database...");
-+            let new_tracked_pubkeys = load_tracked_pubkeys(&pool).await?;
-+            if new_tracked_pubkeys.len() != tracked_pubkeys.len() {
-+                println!("✅ Found {} new public keys to track (total: {})", 
-+                    new_tracked_pubkeys.len() - tracked_pubkeys.len(), 
-+                    new_tracked_pubkeys.len());
-+                tracked_pubkeys = new_tracked_pubkeys;
-+            }
-+            last_refresh = std::time::Instant::now();
-+        }
-+        
-+        // Poll Helius API for recent transactions
-+        if last_helius_poll.elapsed() >= helius_poll_interval {
-+            println!("📡 Polling for recent transactions...");
-+            for pubkey in &tracked_pubkeys {
-+                let pubkey_str = pubkey.to_string();
-+                
-+                // Try Helius API first
-+                let mut transactions = Vec::new();
-+                match fetch_helius_transactions(&http_client, &helius_api_key, &pubkey_str, &network).await {
-+                    Ok(helius_txs) => {
-+                        if !helius_txs.is_empty() {
-+                            println!("📊 Found {} transactions via Helius for {}", helius_txs.len(), pubkey_str);
-+                            transactions = helius_txs;
-+                        } else {
-+                            println!("⚠️  Helius returned empty results for {}, trying RPC fallback...", pubkey_str);
-+                            // Fallback to RPC
-+                            match fetch_rpc_transactions(&rpc_client, pubkey).await {
-+                                Ok(rpc_txs) => {
-+                                    println!("📊 Found {} transactions via RPC for {}", rpc_txs.len(), pubkey_str);
-+                                    transactions = rpc_txs;
-+                                }
-+                                Err(e) => {
-+                                    eprintln!("❌ Error fetching RPC transactions for {}: {}", pubkey_str, e);
-+                                }
-+                            }
-+                        }
-+                    }
-+                    Err(e) => {
-+                        eprintln!("❌ Error fetching Helius transactions for {}: {}, trying RPC fallback...", pubkey_str, e);
-+                        // Fallback to RPC
-+                        match fetch_rpc_transactions(&rpc_client, pubkey).await {
-+                            Ok(rpc_txs) => {
-+                                println!("📊 Found {} transactions via RPC for {}", rpc_txs.len(), pubkey_str);
-+                                transactions = rpc_txs;
-+                            }
-+                            Err(rpc_e) => {
-+                                eprintln!("❌ Error fetching RPC transactions for {}: {}", pubkey_str, rpc_e);
-+                            }
-+                        }
-+                    }
-+                }
-+                
-+                // Process all found transactions
-+                for transaction in &transactions {
-+                    if let Err(e) = process_helius_transaction(&pool, transaction, &tracked_pubkeys).await {
-+                        eprintln!("❌ Error processing transaction: {}", e);
-+                    }
-+                }
-+            }
-+            last_helius_poll = std::time::Instant::now();
-+        }
-+        
-+        // Sleep for a short interval before next poll
-+        tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
-+    }
- }
-+
-diff --git a/indexer/src/test_indexer.rs b/indexer/src/test_indexer.rs
-new file mode 100644
-index 0000000..60dfa93
---- /dev/null
-+++ b/indexer/src/test_indexer.rs
-@@ -0,0 +1,155 @@
-+use dotenvy::dotenv;
-+use solana_sdk::pubkey::Pubkey;
-+use sqlx::PgPool;
-+use std::collections::HashSet;
-+use std::env;
-+use std::str::FromStr;
-+use std::time::Duration;
-+use tokio::time::sleep;
-+
-+async fn load_tracked_pubkeys(pool: &PgPool) -> anyhow::Result<HashSet<Pubkey>> {
-+    let rows = sqlx::query!(r#"SELECT publickey FROM "User""#)
-+        .fetch_all(pool)
-+        .await?;
-+    
-+    let mut tracked_pubkeys = HashSet::new();
-+    let mut invalid_pubkeys = Vec::new();
-+    
-+    for row in rows {
-+        match Pubkey::from_str(&row.publickey) {
-+            Ok(pubkey) => {
-+                tracked_pubkeys.insert(pubkey);
-+            }
-+            Err(e) => {
-+                invalid_pubkeys.push((row.publickey, e));
-+            }
-+        }
-+    }
-+    
-+    if !invalid_pubkeys.is_empty() {
-+        eprintln!("Warning: Found {} invalid public keys in database:", invalid_pubkeys.len());
-+        for (pubkey_str, error) in invalid_pubkeys {
-+            eprintln!("  - {}: {}", pubkey_str, error);
-+        }
-+    }
-+    
-+    Ok(tracked_pubkeys)
-+}
-+
-+async fn ensure_sol_asset(pool: &PgPool) -> anyhow::Result<()> {
-+    const SOL_MINT: &str = "So11111111111111111111111111111111111111112";
-+    
-+    // Check if SOL asset exists
-+    let exists = sqlx::query!(
-+        r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#,
-+        SOL_MINT
-+    )
-+    .fetch_optional(pool)
-+    .await?;
-+    
-+    if exists.is_none() {
-+        println!("SOL asset not found, creating it...");
-+        sqlx::query!(
-+            r#"
-+            INSERT INTO "Asset" (mintAddress, decimals, name, symbol, logoUrl)
-+            VALUES ($1, $2, $3, $4, $5)
-+            "#,
-+            SOL_MINT,
-+            9i32,
-+            "Solana",
-+            "SOL",
-+            "https://raw.githubusercontent.com/solana-labs/token-list/main/assets/mainnet/So11111111111111111111111111111111111111112/logo.png"
-+        )
-+        .execute(pool)
-+        .await?;
-+        println!("SOL asset created successfully");
-+    } else {
-+        println!("SOL asset already exists");
-+    }
-+    
-+    Ok(())
-+}
-+
-+async fn simulate_account_update(pool: &PgPool, pubkey: &str, lamports: u64) -> anyhow::Result<()> {
-+    println!("Simulating account update for {}: {} lamports", pubkey, lamports);
-+
-+    match sqlx::query!(
-+        r#"
-+        UPDATE "Balance"
-+        SET amount = $1, updatedAt = NOW()
-+        WHERE userId = (SELECT id FROM "User" WHERE publickey = $2)
-+        AND assetId = (SELECT id FROM "Asset" WHERE mintAddress = 'So11111111111111111111111111111111111111112')
-+        "#,
-+        lamports as i64,
-+        pubkey
-+    )
-+    .execute(pool)
-+    .await {
-+        Ok(result) => {
-+            if result.rows_affected() > 0 {
-+                println!("✅ Updated balance for account {}: {} lamports", pubkey, lamports);
-+            } else {
-+                println!("⚠️  No balance record found for account {} - user may not exist or SOL asset not configured", pubkey);
-+            }
-+        }
-+        Err(e) => {
-+            eprintln!("❌ Failed to update balance for account {}: {}", pubkey, e);
-+        }
-+    }
-+    
-+    Ok(())
-+}
-+
-+#[tokio::main]
-+async fn main() -> anyhow::Result<()> {
-+    dotenv().ok();
-+
-+    let db_url = env::var("DATABASE_URL")?;
-+    let pool = PgPool::connect(&db_url).await?;
-+
-+    let tracked_pubkeys = load_tracked_pubkeys(&pool).await?;
-+    println!("Loaded {} valid tracked pubkeys from DB", tracked_pubkeys.len());
-+
-+    if tracked_pubkeys.is_empty() {
-+        println!("No users found in database. Test will simulate with a dummy account.");
-+        // Create a dummy account for testing
-+        let dummy_pubkey = "11111111111111111111111111111111";
-+        println!("Using dummy account: {}", dummy_pubkey);
-+    }
-+
-+    // Ensure SOL asset exists in database
-+    ensure_sol_asset(&pool).await?;
-+
-+    println!("\n🧪 Starting indexer simulation test...");
-+    println!("This will simulate account updates every 5 seconds for 30 seconds\n");
-+
-+    let mut counter = 0;
-+
-+    for pubkey in &tracked_pubkeys {
-+        if counter >= 3 { // Test only first 3 accounts
-+            break;
-+        }
-+        
-+        let pubkey_str = pubkey.to_string();
-+        let base_lamports = 1000000000; // 1 SOL in lamports
-+        let random_lamports = base_lamports + (counter * 100000000); // Add some variation
-+        
-+        println!("📊 Testing account: {}", pubkey_str);
-+        
-+        // Simulate account update
-+        simulate_account_update(&pool, &pubkey_str, random_lamports).await?;
-+        
-+        counter += 1;
-+        sleep(Duration::from_secs(2)).await; // Small delay between accounts
-+    }
-+
-+    println!("\n✅ Indexer simulation test completed!");
-+    println!("The indexer successfully:");
-+    println!("  - Connected to the database");
-+    println!("  - Loaded {} user public keys", tracked_pubkeys.len());
-+    println!("  - Ensured SOL asset exists");
-+    println!("  - Simulated account balance updates");
-+    println!("\nThe indexer is ready for production use with a working gRPC endpoint!");
-+
-+    Ok(())
-+}
-diff --git a/indexer/src/yellowstone.rs b/indexer/src/yellowstone.rs
-index 7d5e6c4..72079d1 100644
---- a/indexer/src/yellowstone.rs
-+++ b/indexer/src/yellowstone.rs
-@@ -8,19 +8,19 @@ use {
-     },
-     std::time::Duration,
-     tonic::{
-+        Request, Response, Status,
-         codec::{CompressionEncoding, Streaming},
--        metadata::{errors::InvalidMetadataValue, AsciiMetadataValue, MetadataValue},
-+        metadata::{AsciiMetadataValue, MetadataValue, errors::InvalidMetadataValue},
-         service::interceptor::InterceptedService,
-         transport::channel::{Channel, Endpoint},
--        Request, Response, Status,
-     },
--    tonic_health::pb::{health_client::HealthClient, HealthCheckRequest, HealthCheckResponse},
-+    tonic_health::pb::{HealthCheckRequest, HealthCheckResponse, health_client::HealthClient},
-     yellowstone_grpc_proto::prelude::{
--        geyser_client::GeyserClient, CommitmentLevel, GetBlockHeightRequest,
--        GetBlockHeightResponse, GetLatestBlockhashRequest, GetLatestBlockhashResponse,
--        GetSlotRequest, GetSlotResponse, GetVersionRequest, GetVersionResponse,
--        IsBlockhashValidRequest, IsBlockhashValidResponse, PingRequest, PongResponse,
--        SubscribeReplayInfoRequest, SubscribeReplayInfoResponse, SubscribeRequest, SubscribeUpdate,
-+        CommitmentLevel, GetBlockHeightRequest, GetBlockHeightResponse, GetLatestBlockhashRequest,
-+        GetLatestBlockhashResponse, GetSlotRequest, GetSlotResponse, GetVersionRequest,
-+        GetVersionResponse, IsBlockhashValidRequest, IsBlockhashValidResponse, PingRequest,
-+        PongResponse, SubscribeReplayInfoRequest, SubscribeReplayInfoResponse, SubscribeRequest,
-+        SubscribeUpdate, geyser_client::GeyserClient,
-     },
- };
- 
-@@ -494,4 +494,4 @@ mod tests {
-                 .to_owned()
-         );
-     }
--}
-\ No newline at end of file
-+}
-diff --git a/mpc/.DS_Store b/mpc/.DS_Store
-new file mode 100644
-index 0000000..8c8ac35
-Binary files /dev/null and b/mpc/.DS_Store differ
-diff --git a/mpc/Cargo.toml b/mpc/Cargo.toml
-index 2e79951..68c64ea 100644
---- a/mpc/Cargo.toml
-+++ b/mpc/Cargo.toml
-@@ -1,8 +1,33 @@
- [package]
- name = "mpc"
- version = "0.1.0"
--edition = "2024"
-+edition = "2021"
- 
- [dependencies]
--actix-web = "4.11.0"
--tokio = "1.47.1"
-+actix-web = "4"
-+tokio = { version = "1", features = ["full"] }
-+serde = { version = "1", features = ["derive"] }
-+serde_json = "1"
-+bs58 = "0.4"
-+solana-client = "2.2.2"
-+solana-sdk = "2.2.2"
-+zeroize = { workspace = true }
-+bincode = "1.3"
-+hex = "0.4"
-+reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
-+sha2 = "0.9"
-+env = "1.0.1"
-+uuid = { version = "1", features = ["serde", "v4"] }
-+base64 = "0.22.1"
-+ed25519-dalek = { version = "2", features = ["rand_core"] }
-+rand = "0.8"
-+frost-ed25519 = { version = "2.2", features = ["serde", "serialization"] }
-+frost-core = { version = "2.2.0", features = ["serde"] }
-+sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "postgres", "macros", "uuid", "chrono"] }
-+
-+[dev-dependencies]
-+solana-test-validator = "2.3.1"
-+solana-streamer = "2.3.1"
-+reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
-+assert_cmd = "2.0"
-+serde_json = "1.0"
-diff --git a/mpc/src/error.rs b/mpc/src/error.rs
-deleted file mode 100644
-index e69de29..0000000
-diff --git a/mpc/src/main.rs b/mpc/src/main.rs
-index f004c12..9d5d99e 100644
---- a/mpc/src/main.rs
-+++ b/mpc/src/main.rs
-@@ -1,49 +1,864 @@
--use actix_web::{web::{post}, App, Error, HttpResponse, HttpServer};
-+#![allow(non_snake_case)]
- 
--pub mod error;
--pub mod serialization;
--pub mod tss;
-+use actix_web::{get, post, web, App, HttpServer, Responder};
-+use frost_ed25519::Ed25519Sha512;
-+use serde::{Deserialize, Serialize};
-+use sqlx::{PgPool, Row};
-+use std::collections::{BTreeMap, HashMap};
-+use std::env;
-+use std::sync::Mutex;
-+use tss::{
-+    dkg_party_init, dkg_round1, DkgRound1, FrostParty,
-+    ThresholdParams,
-+};
- 
--#[actix_web::main]
--async fn main() -> Result<(), std::io::Error> {
--    HttpServer::new(|| {
--        App::new()
--        .route("/generate", post().to(generate))
--        .route("/send-single", post().to(send_single))
--        .route("/aggregate-keys", post().to(aggregate_keys))
--        .route("/agg-send-step1", post().to(agg_send_step1))
--        .route("/agg-send-step2", post().to(agg_send_step2))
--        .route(
--            "/aggregate-signatures-broadcast",
--            post().to(aggregate_signatures_broadcast),
-+use ed25519_dalek::{VerifyingKey as DalekPubkey};
-+use frost::{keys::dkg, round1, round2, Signature, SigningPackage};
-+use frost_core::Identifier;
-+use frost_ed25519 as frost;
-+use solana_sdk::pubkey::Pubkey;
-+
-+mod error {
-+    use actix_web::{HttpResponse, ResponseError};
-+    use std::fmt::{Display, Formatter, Result as FmtResult};
-+
-+    #[derive(Debug)]
-+    pub struct Error(pub String);
-+
-+    impl Display for Error {
-+        fn fmt(&self, f: &mut Formatter<'_>) -> FmtResult {
-+            write!(f, "{}", self.0)
-+        }
-+    }
-+
-+    impl ResponseError for Error {
-+        fn error_response(&self) -> HttpResponse {
-+            HttpResponse::BadRequest().body(self.0.clone())
-+        }
-+    }
-+
-+    impl From<&str> for Error {
-+        fn from(s: &str) -> Self {
-+            Self(s.to_string())
-+        }
-+    }
-+
-+    impl From<String> for Error {
-+        fn from(s: String) -> Self {
-+            Self(s)
-+        }
-+    }
-+}
-+use error::Error;
-+
-+mod tss;
-+
-+#[derive(Clone)]
-+pub struct MPCStore {
-+    pool: PgPool,
-+}
-+
-+impl MPCStore {
-+    pub async fn load_key_pkg(&self, pubkey_str: &str, node_id: i16) -> Result<Option<Vec<u8>>, Error> {
-+        let row = sqlx::query(
-+            r#"SELECT key_pkg FROM "MPCKeys" WHERE pubkey = $1 AND node_id = $2"#,
-         )
--    })
--    
--        .bind("127.0.0.1:8080")?
--        .run()
-+        .bind(pubkey_str)
-+        .bind(node_id)
-+        .fetch_optional(&self.pool)
-+        .await
-+        .map_err(|e| Error(format!("DB load_key_pkg error: {e}")))?;
-+
-+        Ok(row.map(|r| r.get::<Vec<u8>, _>("key_pkg")))
-+    }
-+
-+    pub async fn save_dkg_state(
-+        &self,
-+        pubkey_str: &str,
-+        user_email: &str,
-+        node_id: i16,
-+        key_package: &frost::keys::KeyPackage,
-+        pubkey_package: &frost::keys::PublicKeyPackage,
-+    ) -> Result<(), Error> {
-+        println!("DEBUG save_dkg_state: About to serialize key_package for node {}", node_id);
-+
-+        let key_pkg_bytes = key_package
-+            .serialize()
-+            .map_err(|e| Error(format!("Failed to serialize key package: {e:?}")))?;
-+        let pubkey_pkg_bytes = pubkey_package
-+            .serialize()
-+            .map_err(|e| Error(format!("Failed to serialize pubkey package: {e:?}")))?;
-+
-+        println!(
-+            "DEBUG save_dkg_state: key_pkg_bytes.len()={}, pubkey_pkg_bytes.len()={}",
-+            key_pkg_bytes.len(),
-+            pubkey_pkg_bytes.len()
-+        );
-+
-+        sqlx::query(
-+            r#"
-+            INSERT INTO "MPCKeys" (pubkey, user_email, node_id, key_pkg, pubkey_pkg, created_at)
-+            VALUES ($1, $2, $3, $4, $5, NOW())
-+            ON CONFLICT (pubkey, node_id)
-+            DO UPDATE SET 
-+                key_pkg = EXCLUDED.key_pkg,
-+                pubkey_pkg = EXCLUDED.pubkey_pkg,
-+                user_email = EXCLUDED.user_email,
-+                created_at = NOW()
-+            "#,
-+        )
-+        .bind(pubkey_str)
-+        .bind(user_email)
-+        .bind(node_id)
-+        .bind(key_pkg_bytes)
-+        .bind(pubkey_pkg_bytes)
-+        .execute(&self.pool)
-+        .await
-+        .map_err(|e| Error(format!("DB save_dkg_state error: {e}")))?;
-+
-+        println!("DEBUG save_dkg_state: Successfully saved to DB for node {}", node_id);
-+        Ok(())
-+    }
-+
-+    pub async fn load_dkg_packages(
-+        &self,
-+        pubkey_str: &str,
-+        node_id: i16,
-+    ) -> Result<Option<(frost::keys::KeyPackage, Option<frost::keys::PublicKeyPackage>)>, Error> {
-+        let row = sqlx::query(
-+            r#"SELECT key_pkg, pubkey_pkg FROM "MPCKeys" WHERE pubkey = $1 AND node_id = $2"#,
-+        )
-+        .bind(pubkey_str)
-+        .bind(node_id)
-+        .fetch_optional(&self.pool)
-         .await
-+        .map_err(|e| Error(format!("DB load_dkg_packages error: {e}")))?;
-+
-+        if let Some(r) = row {
-+            let key_pkg_bytes: Vec<u8> = r.get("key_pkg");
-+            let key_package = frost::keys::KeyPackage::deserialize(&key_pkg_bytes)
-+                .map_err(|e| Error(format!("Failed to deserialize key package: {e:?}")))?;
-+
-+            let pubkey_package = match r.try_get::<Vec<u8>, _>("pubkey_pkg") {
-+                Ok(bytes) => Some(
-+                    frost::keys::PublicKeyPackage::deserialize(&bytes)
-+                        .map_err(|e| Error(format!("Failed to deserialize pubkey package: {e:?}")))?,
-+                ),
-+                Err(_) => None,
-+            };
-+
-+            Ok(Some((key_package, pubkey_package)))
-+        } else {
-+            Ok(None)
-+        }
-+    }
-+}
-+
-+#[derive(Clone)]
-+struct AppState {
-+    params: ThresholdParams,
-+    party: Option<FrostParty>,
-+    round1: Option<DkgRound1>,
-+    pubkey_pkg: Option<frost::keys::PublicKeyPackage>,
-+    key_pkg: Option<frost::keys::KeyPackage>,
-+    nonces_by_key: HashMap<String, round1::SigningNonces>,
-+    commits_by_key: HashMap<String, round1::SigningCommitments>,
-+    sp_by_key: HashMap<String, SigningPackage>,
-+    shares_by_key: HashMap<String, BTreeMap<Identifier<Ed25519Sha512>, round2::SignatureShare>>,
-+    round2_pkgs: HashMap<
-+        Identifier<Ed25519Sha512>,
-+        BTreeMap<Identifier<Ed25519Sha512>, dkg::round2::Package>,
-+    >,
-+    all_r1: BTreeMap<Identifier<Ed25519Sha512>, dkg::round1::Package>,
-+    mpc_store: MPCStore,
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct Round1Resp {
-+    id: u16,
-+    pkg_hex: String,
-+}
-+
-+impl Round1Resp {
-+    fn from_pkg(id: u16, pkg: &dkg::round1::Package) -> Self {
-+        let bytes = pkg.serialize().expect("pkg serialize");
-+        Self {
-+            id,
-+            pkg_hex: hex::encode(bytes),
-+        }
-+    }
-+
-+    fn to_pkg(&self) -> Result<dkg::round1::Package, Error> {
-+        let bytes = hex::decode(&self.pkg_hex).map_err(|e| Error(format!("hex decode: {e}")))?;
-+        Ok(dkg::round1::Package::deserialize(&bytes)
-+            .map_err(|e| Error(format!("pkg deserialize: {e:?}")))?)
-+    }
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct Round2Resp {
-+    id: u16,
-+    pubkey: String,
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct SignRound1Req {
-+    message: String,
-+    message_format: Option<String>,
- }
- 
--async fn generate() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
-+#[derive(Serialize, Deserialize, Clone)]
-+struct CommitmentsMsg {
-+    id: u16,
-+    commitments: round1::SigningCommitments,
- }
- 
--async fn send_single() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
-+#[derive(Serialize, Deserialize, Clone)]
-+struct SignBuildReq {
-+    message: String,
-+    message_format: Option<String>,
-+    commitments: Vec<CommitmentsMsg>,
- }
- 
--async fn aggregate_keys() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
-+#[derive(Serialize, Deserialize, Clone)]
-+struct SignBuildResp {
-+    message_key: String,
-+    signer_ids: Vec<u16>,
- }
- 
--async fn agg_send_step1() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
-+#[derive(Serialize, Deserialize, Clone)]
-+struct SignRound2Req {
-+    message: String,
-+    message_format: Option<String>,
-+    commitments: Vec<CommitmentsMsg>,
- }
- 
--async fn agg_send_step2() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
-+#[derive(Serialize, Deserialize, Clone)]
-+struct ShareMsg {
-+    id: u16,
-+    share: round2::SignatureShare,
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct AggregateReq {
-+    message: String,
-+    message_format: Option<String>,
-+    commitments: Vec<CommitmentsMsg>,
-+    shares: Vec<ShareMsg>,
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct AggregateResp {
-+    signature_hex: String,
-+    signature_base64: String,
-+    solana_signature: String,
-+}
-+
-+
-+fn u16_from_id(id: &Identifier<Ed25519Sha512>) -> u16 {
-+    id.serialize()[0] as u16
-+}
-+
-+fn decode_message(s: &str, fmt: Option<&str>) -> Result<Vec<u8>, Error> {
-+    match fmt.unwrap_or("utf8") {
-+        "hex" => Ok(hex::decode(s).map_err(|e| Error(format!("hex decode: {e}")))?),
-+        "base64" => {
-+            use base64::Engine;
-+            Ok(base64::engine::general_purpose::STANDARD.decode(s).map_err(|e| Error(format!("base64 decode: {e}")))?)
-+        },
-+        "utf8" => Ok(s.as_bytes().to_vec()),
-+        other => Err(Error(format!("unsupported message_format: {other}"))),
-+    }
-+}
-+
-+fn msg_key(bytes: &[u8]) -> String {
-+    use sha2::{Digest, Sha256};
-+    hex::encode(Sha256::digest(bytes))
-+}
-+
-+#[post("/dkg-round1")]
-+async fn dkg_round1_route(state: web::Data<Mutex<AppState>>) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let pid: u16 = env::var("PARTY_ID").unwrap().parse().unwrap();
-+
-+    let mut party = dkg_party_init(pid, st.params)?;
-+    let r1 = dkg_round1(&mut party)?;
-+
-+    st.party = Some(party.clone());
-+    st.round1 = Some(r1.clone());
-+
-+    Ok(web::Json(Round1Resp::from_pkg(pid, &r1.pkg)))
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct Round2InitResp {
-+    from: u16,
-+    packages: HashMap<u16, String>, // recipient_id -> hex(pkg)
-+}
-+
-+#[derive(Serialize, Deserialize, Clone)]
-+struct Round2RecvReq {
-+    from: u16,
-+    pkg_hex: String,
-+}
-+
-+#[post("/dkg-round2-recv")]
-+async fn dkg_round2_recv(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<Round2RecvReq>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let party = st.party.clone().ok_or("party not init")?;
-+    let my_id = party.id;
-+
-+    let bytes = hex::decode(&req.pkg_hex).map_err(|e| Error(format!("hex decode: {e}")))?;
-+    let pkg = dkg::round2::Package::deserialize(&bytes)
-+        .map_err(|e| Error(format!("pkg deser: {e:?}")))?;
-+
-+    let sender_id: Identifier<Ed25519Sha512> = req
-+        .from
-+        .try_into()
-+        .map_err(|_| Error("bad sender id".into()))?;
-+
-+    st.round2_pkgs
-+        .entry(my_id)
-+        .or_default()
-+        .insert(sender_id, pkg);
-+
-+    let stored = st.round2_pkgs.get(&my_id).unwrap();
-+    println!(
-+        "Node {} stored round2 pkg from {} (now have {} pkgs: {:?})",
-+        u16_from_id(&my_id),
-+        req.from,
-+        stored.len(),
-+        stored.keys().map(|id| u16_from_id(id)).collect::<Vec<_>>()
-+    );
-+
-+    Ok("ok".to_string())
-+}
-+
-+#[post("/dkg-round2-init")]
-+async fn dkg_round2_init(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<Vec<Round1Resp>>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let mut party = st.party.clone().ok_or("party not init")?;
-+    let my_id = party.id;
-+
-+    let mut peer_r1: BTreeMap<Identifier<Ed25519Sha512>, dkg::round1::Package> = BTreeMap::new();
-+    for r in req.iter() {
-+        let rid: Identifier<Ed25519Sha512> =
-+            r.id.try_into()
-+                .map_err(|_| Error(format!("bad id {}", r.id)))?;
-+        if rid == my_id {
-+            continue;
-+        }
-+        peer_r1.insert(rid, r.to_pkg()?);
-+    }
-+
-+    println!(
-+        "Node {} received {} round1 pkgs",
-+        u16_from_id(&my_id),
-+        peer_r1.len()
-+    );
-+    for (id, _) in &peer_r1 {
-+        println!(" -> from {:?}", u16_from_id(id));
-+    }
-+
-+    if peer_r1.len() != (party.params.n as usize - 1) {
-+        return Err(Error(format!(
-+            "expected {} round1 pkgs, got {}",
-+            party.params.n - 1,
-+            peer_r1.len()
-+        )));
-+    }
-+
-+    let sec1 = party.take_round1_secret().ok_or("missing round1 secret")?;
-+    let (sec2, per_recipient) =
-+        dkg::part2(sec1, &peer_r1).map_err(|e| Error(format!("dkg part2: {e:?}")))?;
-+
-+    party.set_round2_secret(sec2);
-+    st.party = Some(party.clone());
-+
-+    for (id, pkg) in &peer_r1 {
-+        st.all_r1.insert(*id, pkg.clone());
-+    }
-+    if let Some(my_r1) = st.round1.clone() {
-+        st.all_r1.insert(my_r1.id, my_r1.pkg);
-+    }
-+
-+    for (recv, pkg) in &per_recipient {
-+        st.round2_pkgs
-+            .entry(my_id)
-+            .or_default()
-+            .insert(*recv, pkg.clone());
-+
-+        println!(
-+            "Node {} generated round2 pkg for {} (stored in my bucket)",
-+            u16_from_id(&my_id),
-+            u16_from_id(recv)
-+        );
-+    }
-+
-+    let mut map = HashMap::new();
-+    for (recv, pkg) in per_recipient {
-+        let bytes = pkg
-+            .serialize()
-+            .map_err(|e| Error(format!("ser r2: {e:?}")))?;
-+        map.insert(u16_from_id(&recv), hex::encode(bytes));
-+    }
-+
-+    Ok(web::Json(Round2InitResp {
-+        from: u16_from_id(&my_id),
-+        packages: map,
-+    }))
- }
- 
--async fn aggregate_signatures_broadcast() -> Result<HttpResponse, Error> {
--    Ok(HttpResponse::Ok().body("Hello, world!"))
--}
-\ No newline at end of file
-+#[derive(Serialize, Deserialize)]
-+struct SaveDkgRequest {
-+    user_email: String,
-+}
-+
-+#[post("/dkg-finalize")]
-+async fn dkg_finalize(state: web::Data<Mutex<AppState>>) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let mut party = st.party.clone().ok_or("party not init")?;
-+
-+    let sec2 = party.take_round2_secret().ok_or("missing round2 secret")?;
-+
-+    println!(
-+        "Node {} finalize: all_r1 has {} entries: {:?}",
-+        u16_from_id(&party.id),
-+        st.all_r1.len(),
-+        st.all_r1
-+            .keys()
-+            .map(|id| u16_from_id(id))
-+            .collect::<Vec<_>>()
-+    );
-+
-+    let mut r1_pkgs: BTreeMap<Identifier<Ed25519Sha512>, dkg::round1::Package> = st.all_r1.clone();
-+    r1_pkgs.remove(&party.id);
-+
-+    if r1_pkgs.len() != (party.params.n as usize - 1) {
-+        return Err(Error(format!(
-+            "expected {} peer round1 pkgs, got {}",
-+            party.params.n - 1,
-+            r1_pkgs.len()
-+        )));
-+    }
-+
-+    let r2_pkgs = st
-+        .round2_pkgs
-+        .remove(&party.id)
-+        .ok_or("missing round2 pkgs")?;
-+
-+    if r2_pkgs.len() != (party.params.n as usize - 1) {
-+        return Err(Error(format!(
-+            "expected {} round2 pkgs, got {}",
-+            party.params.n - 1,
-+            r2_pkgs.len()
-+        )));
-+    }
-+
-+    let (key_pkg, pubkey_pkg) =
-+        dkg::part3(&sec2, &r1_pkgs, &r2_pkgs).map_err(|e| Error(format!("dkg part3: {e:?}")))?;
-+
-+    party.key_pkg = Some(key_pkg.clone());
-+    party.pubkey_pkg = Some(pubkey_pkg.clone());
-+    st.key_pkg = Some(key_pkg.clone());
-+    st.pubkey_pkg = Some(pubkey_pkg.clone());
-+    st.party = Some(party.clone());
-+
-+    let vk_bytes = pubkey_pkg
-+        .verifying_key()
-+        .serialize()
-+        .map_err(|e| Error(format!("{e:?}")))?;
-+    let vk_arr: [u8; 32] =
-+        <[u8; 32]>::try_from(vk_bytes.as_slice()).map_err(|_| Error("vk len".into()))?;
-+    let dalek = DalekPubkey::from_bytes(&vk_arr).map_err(|_| Error("dalek pk".into()))?;
-+    let agg = Pubkey::new_from_array(dalek.to_bytes());
-+    let agg_str: String = agg.to_string();
-+
-+    println!(
-+        "Finalized key for node {}, pubkey={}",
-+        u16_from_id(&party.id),
-+        agg_str
-+    );
-+
-+    let node_id = u16_from_id(&party.id) as i16;
-+    let placeholder_email = "dkg_generated@system.local";
-+    
-+    match key_pkg.serialize() {
-+        Ok(key_bytes) => {
-+            match pubkey_pkg.serialize() {
-+                Ok(pubkey_bytes) => {
-+                    println!(
-+                        "Debug: Serializing key_pkg={} bytes, pubkey_pkg={} bytes for node {}",
-+                        key_bytes.len(), pubkey_bytes.len(), node_id
-+                    );
-+                    
-+                    if let Err(e) = st.mpc_store
-+                        .save_dkg_state(&agg_str, placeholder_email, node_id, &key_pkg, &pubkey_pkg)
-+                        .await 
-+                    {
-+                        println!(
-+                            "Warning: Failed to auto-save DKG state for node {}: {}",
-+                            node_id, e
-+                        );
-+                    } else {
-+                        println!(
-+                            "Auto-saved DKG state for node {} with pubkey {} (key={} bytes, pubkey={} bytes)",
-+                            node_id, agg_str, key_bytes.len(), pubkey_bytes.len()
-+                        );
-+                    }
-+                }
-+                Err(e) => {
-+                    println!("Warning: Failed to serialize pubkey package for debug: {e:?}");
-+                }
-+            }
-+        }
-+        Err(e) => {
-+            println!("Warning: Failed to serialize key package for debug: {e:?}");
-+        }
-+    }
-+
-+    Ok(web::Json(Round2Resp {
-+        id: u16_from_id(&party.id),
-+        pubkey: agg_str,
-+    }))
-+}
-+
-+#[post("/save-dkg")]
-+async fn save_dkg_state(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<SaveDkgRequest>,
-+) -> Result<impl Responder, Error> {
-+    let st = state.lock().unwrap();
-+    
-+    let party = st.party.as_ref().ok_or("party not initialized")?;
-+    let key_pkg = st.key_pkg.as_ref().ok_or("DKG not completed - no key package")?;
-+    let pubkey_pkg = st.pubkey_pkg.as_ref().ok_or("DKG not completed - no pubkey package")?;
-+
-+    let vk_bytes = pubkey_pkg
-+        .verifying_key()
-+        .serialize()
-+        .map_err(|e| Error(format!("serialize vk: {e:?}")))?;
-+    let vk_arr: [u8; 32] = vk_bytes.try_into()
-+        .map_err(|_| Error("invalid vk length".into()))?;
-+    let dalek = DalekPubkey::from_bytes(&vk_arr)
-+        .map_err(|_| Error("invalid dalek pubkey".into()))?;
-+    let pubkey_str = Pubkey::new_from_array(dalek.to_bytes()).to_string();
-+
-+    let node_id = u16_from_id(&party.id) as i16;
-+
-+    st.mpc_store
-+        .save_dkg_state(&pubkey_str, &req.user_email, node_id, key_pkg, pubkey_pkg)
-+        .await?;
-+
-+    println!(
-+        "Saved DKG state for user {} on node {} with pubkey {}",
-+        req.user_email, node_id, pubkey_str
-+    );
-+
-+    Ok(web::Json(serde_json::json!({
-+        "status": "success",
-+        "message": "DKG state saved to database",
-+        "pubkey": pubkey_str,
-+        "node_id": node_id
-+    })))
-+}
-+
-+#[derive(Serialize, Deserialize)]
-+struct LoadDkgRequest {
-+    pubkey: String,
-+}
-+
-+#[post("/load-dkg")]
-+async fn load_dkg_state(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<LoadDkgRequest>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    
-+    let pid: u16 = env::var("PARTY_ID").unwrap_or_else(|_| "1".to_string()).parse().unwrap();
-+    let node_id = pid as i16;
-+
-+    match st.mpc_store.load_dkg_packages(&req.pubkey, node_id).await? {
-+        Some((key_pkg, pubkey_pkg_opt)) => {
-+            if st.party.is_none() {
-+                let party = dkg_party_init(pid, st.params)?;
-+                st.party = Some(party);
-+            }
-+            
-+            if let Some(ref mut party) = st.party {
-+                party.key_pkg = Some(key_pkg.clone());
-+                if let Some(pubkey_pkg) = &pubkey_pkg_opt {
-+                    party.pubkey_pkg = Some(pubkey_pkg.clone());
-+                }
-+            }
-+            st.key_pkg = Some(key_pkg);
-+            if let Some(pubkey_pkg) = pubkey_pkg_opt {
-+                st.pubkey_pkg = Some(pubkey_pkg);
-+            }
-+
-+            println!(
-+                "Loaded DKG state for pubkey {} on node {}",
-+                req.pubkey, node_id
-+            );
-+
-+            Ok(web::Json(serde_json::json!({
-+                "status": "success",
-+                "message": "DKG state loaded from database",
-+                "pubkey": req.pubkey,
-+                "node_id": node_id
-+            })))
-+        }
-+        None => {
-+            Ok(web::Json(serde_json::json!({
-+                "status": "not_found",
-+                "message": "No DKG state found for this pubkey and node",
-+                "pubkey": req.pubkey,
-+                "node_id": node_id
-+            })))
-+        }
-+    }
-+}
-+
-+#[derive(serde::Serialize)]
-+struct PubkeyResponse {
-+    pubkey: String,
-+}
-+
-+#[get("/pubkey")]
-+async fn pubkey(state: web::Data<Mutex<AppState>>) -> Result<impl Responder, Error> {
-+    let st = state.lock().unwrap();
-+    let pk_pkg = st.pubkey_pkg.clone().ok_or("not finished")?;
-+    let vk_bytes = pk_pkg
-+        .verifying_key()
-+        .serialize()
-+        .map_err(|e| Error(format!("{e:?}")))?;
-+    let vk_arr: [u8; 32] =
-+        <[u8; 32]>::try_from(vk_bytes.as_slice()).map_err(|_| Error("vk len".into()))?;
-+    let dalek = DalekPubkey::from_bytes(&vk_arr).map_err(|_| Error("dalek pk".into()))?;
-+    let agg = Pubkey::new_from_array(dalek.to_bytes());
-+    let agg_str = agg.to_string();
-+
-+    Ok(web::Json(PubkeyResponse { pubkey: agg_str }))
-+}
-+
-+#[post("/sign-round1")]
-+async fn sign_round1_route(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<SignRound1Req>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let msg = decode_message(&req.message, req.message_format.as_deref())?;
-+    let key = msg_key(&msg);
-+
-+    let party = st.party.clone().ok_or("party not ready")?;
-+    let kp = party.key_pkg.clone().ok_or("no key_pkg")?;
-+    let mut rng = rand::rngs::OsRng;
-+
-+    let (nonces, commitments) = round1::commit(kp.signing_share(), &mut rng);
-+
-+    st.nonces_by_key.insert(key.clone(), nonces.clone());
-+    st.commits_by_key.insert(key.clone(), commitments.clone());
-+
-+    let pid = u16_from_id(&party.id);
-+    Ok(web::Json(CommitmentsMsg {
-+        id: pid,
-+        commitments,
-+    }))
-+}
-+
-+#[post("/sign-build")]
-+async fn sign_build_route(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<SignBuildReq>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let msg = decode_message(&req.message, req.message_format.as_deref())?;
-+    let key = msg_key(&msg);
-+
-+    let mut map: BTreeMap<Identifier<Ed25519Sha512>, round1::SigningCommitments> = BTreeMap::new();
-+    let mut signer_ids: Vec<u16> = Vec::new();
-+
-+    for c in &req.commitments {
-+        let id: Identifier<Ed25519Sha512> =
-+            c.id.try_into()
-+                .map_err(|_| Error(format!("bad id {}", c.id)))?;
-+        map.insert(id, c.commitments.clone());
-+        signer_ids.push(c.id);
-+    }
-+
-+    let sp = SigningPackage::new(map, &msg);
-+    st.sp_by_key.insert(key.clone(), sp);
-+
-+    Ok(web::Json(SignBuildResp {
-+        message_key: key,
-+        signer_ids,
-+    }))
-+}
-+
-+#[post("/sign-round2")]
-+async fn sign_round2_route(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<SignRound2Req>,
-+) -> Result<impl Responder, Error> {
-+    let mut st = state.lock().unwrap();
-+    let msg = decode_message(&req.message, req.message_format.as_deref())?;
-+    let key = msg_key(&msg);
-+
-+    let party = st.party.clone().ok_or("party not ready")?;
-+    let kp = party.key_pkg.clone().ok_or("no key_pkg")?;
-+    let local_nonces = st.nonces_by_key.get(&key).ok_or("no local nonces")?.clone();
-+
-+    let mut map: BTreeMap<Identifier<Ed25519Sha512>, round1::SigningCommitments> = BTreeMap::new();
-+    for c in &req.commitments {
-+        let id: Identifier<Ed25519Sha512> =
-+            c.id.try_into()
-+                .map_err(|_| Error(format!("bad id {}", c.id)))?;
-+        map.insert(id, c.commitments.clone());
-+    }
-+
-+    let sp = SigningPackage::new(map, &msg);
-+    let share = round2::sign(&sp, &local_nonces, &kp).map_err(|e| Error(format!("sign: {e:?}")))?;
-+    let pid = u16_from_id(&party.id);
-+
-+    st.shares_by_key
-+        .entry(key.clone())
-+        .or_default()
-+        .insert(party.id, share.clone());
-+
-+    Ok(web::Json(ShareMsg { id: pid, share }))
-+}
-+
-+#[post("/aggregate-signatures")]
-+async fn aggregate_signatures_route(
-+    state: web::Data<Mutex<AppState>>,
-+    req: web::Json<AggregateReq>,
-+) -> Result<impl Responder, Error> {
-+    let st = state.lock().unwrap();
-+    let msg = decode_message(&req.message, req.message_format.as_deref())?;
-+    let _key = msg_key(&msg);
-+
-+    let pk_pkg = st.pubkey_pkg.clone().ok_or("no pubkey_pkg")?;
-+
-+    let mut commits_map: BTreeMap<Identifier<Ed25519Sha512>, round1::SigningCommitments> =
-+        BTreeMap::new();
-+    for c in &req.commitments {
-+        let id: Identifier<Ed25519Sha512> =
-+            c.id.try_into()
-+                .map_err(|_| Error(format!("bad id {}", c.id)))?;
-+        commits_map.insert(id, c.commitments.clone());
-+    }
-+    let sp = SigningPackage::new(commits_map, &msg);
-+
-+    let mut shares_map: BTreeMap<Identifier<Ed25519Sha512>, round2::SignatureShare> =
-+        BTreeMap::new();
-+    for s in &req.shares {
-+        let id: Identifier<Ed25519Sha512> =
-+            s.id.try_into()
-+                .map_err(|_| Error(format!("bad id {}", s.id)))?;
-+        shares_map.insert(id, s.share.clone());
-+    }
-+
-+    let sig: Signature = frost::aggregate(&sp, &shares_map, &pk_pkg)
-+        .map_err(|e| Error(format!("aggregate: {e:?}")))?;
-+
-+    let sig_bytes = sig
-+        .serialize()
-+        .map_err(|e| Error(format!("serialize: {e:?}")))?;
-+    let arr: [u8; 64] =
-+        <[u8; 64]>::try_from(sig_bytes.as_slice()).map_err(|_| Error("sig len".into()))?;
-+
-+    use solana_sdk::signature::Signature as SolSig;
-+    let sol = SolSig::try_from(arr.as_slice()).map_err(|_| Error("sol sig".into()))?;
-+
-+    Ok(web::Json(AggregateResp {
-+        signature_hex: hex::encode(arr),
-+        signature_base64: {
-+            use base64::Engine;
-+            base64::engine::general_purpose::STANDARD.encode(arr)
-+        },
-+        solana_signature: sol.to_string(),
-+    }))
-+}
-+
-+#[actix_web::main]
-+async fn main() -> std::io::Result<()> {
-+    let port: u16 = env::var("PORT")
-+        .unwrap_or_else(|_| "8081".to_string())
-+        .parse()
-+        .expect("Invalid PORT");
-+    let pid: u16 = env::var("PARTY_ID")
-+        .unwrap_or_else(|_| "1".to_string())
-+        .parse()
-+        .expect("Invalid PARTY_ID");
-+    let params = ThresholdParams { t: 2, n: 3 };
-+
-+    let mpc_db_url = env::var("MPC_DATABASE_URL")
-+        .unwrap_or_else(|_| format!("postgres://mpc:mpcpass@localhost:543{}/mpc{}", 2 + pid, pid));
-+    
-+    println!("Connecting to MPC database: {}", mpc_db_url);
-+    let mpc_pool = PgPool::connect(&mpc_db_url)
-+        .await
-+        .expect("Failed to connect to MPC database");
-+    
-+    sqlx::query(
-+        r#"
-+        CREATE TABLE IF NOT EXISTS "MPCKeys" (
-+            pubkey TEXT NOT NULL,
-+            user_email TEXT NOT NULL,
-+            node_id SMALLINT NOT NULL,
-+            key_pkg BYTEA NOT NULL,
-+            pubkey_pkg BYTEA NOT NULL,
-+            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+            PRIMARY KEY (pubkey, node_id)
-+        );
-+        "#,
-+    )
-+    .execute(&mpc_pool)
-+    .await
-+    .expect("Failed to create MPCKeys table");
-+    
-+    let mpc_store = MPCStore { pool: mpc_pool };
-+
-+    let initial_state = AppState {
-+        params,
-+        party: None,
-+        round1: None,
-+        pubkey_pkg: None,
-+        key_pkg: None,
-+        nonces_by_key: HashMap::new(),
-+        commits_by_key: HashMap::new(),
-+        sp_by_key: HashMap::new(),
-+        shares_by_key: HashMap::new(),
-+        round2_pkgs: HashMap::new(),
-+        all_r1: BTreeMap::new(),
-+        mpc_store,
-+    };
-+
-+    println!("MPC server {pid} starting on port {port}");
-+    let state = web::Data::new(Mutex::new(initial_state));
-+
-+    HttpServer::new(move || {
-+        App::new()
-+            .app_data(state.clone())
-+            .service(dkg_round1_route)
-+            .service(dkg_round2_init)
-+            .service(dkg_round2_recv)
-+            .service(dkg_finalize)
-+            .service(save_dkg_state)
-+            .service(load_dkg_state)
-+            .service(pubkey)
-+            .service(sign_round1_route)
-+            .service(sign_build_route)
-+            .service(sign_round2_route)
-+            .service(aggregate_signatures_route)
-+    })
-+    .bind(("127.0.0.1", port))?
-+    .run()
-+    .await
-+}
-diff --git a/mpc/src/serialization.rs b/mpc/src/serialization.rs
-deleted file mode 100644
-index 336a908..0000000
---- a/mpc/src/serialization.rs
-+++ /dev/null
-@@ -1,57 +0,0 @@
--use std::fmt::{Display, Formatter};
--
--use bs58::decode::Error as Bs58Error;
--use solana_client::client_error::ClientError;
--
--use crate::serialization::Error as DeserializationError;
--
--#[derive(Debug)]
--pub enum Error {
--    WrongNetwork(String),
--    BadBase58(Bs58Error),
--    WrongKeyPair(ed25519_dalek::SignatureError),
--    AirdropFailed(ClientError),
--    RecentHashFailed(ClientError),
--    ConfirmingTransactionFailed(ClientError),
--    BalaceFailed(ClientError),
--    SendTransactionFailed(ClientError),
--    DeserializationFailed { error: DeserializationError, field_name: &'static str },
--    MismatchMessages,
--    InvalidSignature,
--    KeyPairIsNotInKeys,
--}
--
--impl Display for Error {
--    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
--        match self {
--            Self::WrongNetwork(net) => write!(f, "Unrecognized network: {}, please select Mainnet/Testnet/Devnet", net),
--            Self::BadBase58(e) => write!(f, "Based58 Error: {}", e),
--            Self::WrongKeyPair(e) => write!(f, "Failed deserializing keypair: {}", e),
--            Self::AirdropFailed(e) => write!(f, "Failed asking for an airdrop: {}", e),
--            Self::RecentHashFailed(e) => write!(f, "Failed recieving the latest hash: {}", e),
--            Self::ConfirmingTransactionFailed(e) => write!(f, "Failed confirming transaction: {}", e),
--            Self::BalaceFailed(e) => write!(f, "Failed checking balance: {}", e),
--            Self::SendTransactionFailed(e) => write!(f, "Failed sending transaction: {}", e),
--            Self::DeserializationFailed { error, field_name } => {
--                write!(f, "Failed deserializing {}: {}", field_name, error)
--            }
--            Self::MismatchMessages => write!(f, "There is a mismatch between first_messages and second_messages"),
--            Self::InvalidSignature => write!(f, "The resulting signature doesn't match the transaction"),
--            Self::KeyPairIsNotInKeys => write!(f, "The provided keypair is not in the list of pubkeys"),
--        }
--    }
--}
--
--impl From<Bs58Error> for Error {
--    fn from(e: Bs58Error) -> Self {
--        Self::BadBase58(e)
--    }
--}
--
--impl From<ed25519_dalek::SignatureError> for Error {
--    fn from(e: ed25519_dalek::SignatureError) -> Self {
--        Self::WrongKeyPair(e)
--    }
--}
--
--impl std::error::Error for Error {}
-\ No newline at end of file
-diff --git a/mpc/src/tss.rs b/mpc/src/tss.rs
-index 0503fb3..071dd7f 100644
---- a/mpc/src/tss.rs
-+++ b/mpc/src/tss.rs
-@@ -1,222 +1,411 @@
- #![allow(non_snake_case)]
- 
--use curv::elliptic::curves::{Ed25519, Point, Scalar};
--use multi_party_eddsa::protocols::musig2::{self, PrivatePartialNonces, PublicPartialNonces};
--use multi_party_eddsa::protocols::ExpandedKeyPair;
--use solana_sdk::signature::{Keypair, Signature, Signer, SignerError};
--use solana_sdk::{hash::Hash, pubkey::Pubkey, transaction::Transaction};
--
--use crate::serialization::{AggMessage1, Error as DeserializationError, PartialSignature, SecretAggStepOne};
--use crate::{create_unsigned_transaction, Error};
--
--/// Create the aggregate public key, pass key=None if you don't care about the coefficient
--pub fn key_agg(keys: Vec<Pubkey>, key: Option<Pubkey>) -> Result<musig2::PublicKeyAgg, Error> {
--    let convert_keys = |k: Pubkey| {
--        Point::from_bytes(&k.to_bytes()).map_err(|e| Error::DeserializationFailed {
--            error: DeserializationError::InvalidPoint(e),
--            field_name: "keys",
--        })
--    };
--    let keys: Vec<_> = keys.into_iter().map(convert_keys).collect::<Result<_, _>>()?;
--    let key = key.map(convert_keys).unwrap_or_else(|| Ok(keys[0].clone()))?;
--    musig2::PublicKeyAgg::key_aggregation_n(keys, &key).ok_or(Error::KeyPairIsNotInKeys)
--}
--
--/// Generate Message1 which contains nonce, public nonce, and commitment to nonces
--pub fn step_one(keypair: Keypair) -> (AggMessage1, SecretAggStepOne) {
--    let extended_kepair = ExpandedKeyPair::create_from_private_key(keypair.secret().to_bytes());
--    // we don't really need to pass a message here.
--    let (private_nonces, public_nonces) = musig2::generate_partial_nonces(&extended_kepair, None);
--
--    (
--        AggMessage1 { sender: keypair.pubkey(), public_nonces: public_nonces.clone() },
--        SecretAggStepOne { private_nonces, public_nonces },
--    )
--}
--
--#[allow(clippy::too_many_arguments)]
--pub fn step_two(
--    keypair: Keypair,
--    amount: f64,
--    to: Pubkey,
--    memo: Option<String>,
--    recent_block_hash: Hash,
--    keys: Vec<Pubkey>,
--    first_messages: Vec<AggMessage1>,
--    secret_state: SecretAggStepOne,
--) -> Result<PartialSignature, Error> {
--    let other_nonces: Vec<_> = first_messages.into_iter().map(|msg1| msg1.public_nonces.R).collect();
--
--    // Generate the aggregate key together with the coefficient of the current keypair
--    let aggkey = key_agg(keys, Some(keypair.pubkey()))?;
--    let aggpubkey = Pubkey::new(&*aggkey.agg_public_key.to_bytes(true));
--    let extended_kepair = ExpandedKeyPair::create_from_private_key(keypair.secret().to_bytes());
--
--    // Create the unsigned transaction
--    let mut tx = create_unsigned_transaction(amount, &to, memo, &aggpubkey);
--
--    let signer = PartialSigner {
--        signer_private_nonce: secret_state.private_nonces,
--        signer_public_nonce: secret_state.public_nonces,
--        other_nonces,
--        extended_kepair,
--        aggregated_pubkey: aggkey,
--    };
--    // Sign the transaction using a custom `PartialSigner`, this is required to comply with Solana's API.
--    tx.sign(&[&signer], recent_block_hash);
--    let sig = tx.signatures[0];
--    Ok(PartialSignature(sig))
--}
--
--pub fn sign_and_broadcast(
--    amount: f64,
--    to: Pubkey,
--    memo: Option<String>,
--    recent_block_hash: Hash,
--    keys: Vec<Pubkey>,
--    signatures: Vec<PartialSignature>,
--) -> Result<Transaction, Error> {
--    let aggkey = key_agg(keys, None)?;
--    let aggpubkey = Pubkey::new(&*aggkey.agg_public_key.to_bytes(true));
-+use std::collections::BTreeMap;
-+
-+use ed25519_dalek::{Signature as DalekSig, Verifier, VerifyingKey as DalekPubkey};
-+use frost::{
-+    keys::{dkg, KeyPackage, PublicKeyPackage},
-+    round1::{self, SigningCommitments, SigningNonces},
-+    round2, Identifier, Signature, SigningPackage,
-+};
-+use frost_ed25519 as frost;
-+use rand::rngs::OsRng;
-+use solana_sdk::{
-+    hash::Hash, pubkey::Pubkey, signature::Signature as SolSignature, signer::Signer, system_instruction, transaction::Transaction
-+};
-+use std::convert::TryFrom;
-+
-+use crate::error::Error;
- 
--    // Make sure all the `R`s are the same
--    if !signatures[1..].iter().map(|s| &s.0.as_ref()[..32]).all(|s| s == &signatures[0].0.as_ref()[..32]) {
--        return Err(Error::MismatchMessages);
-+#[derive(Clone, Copy, Debug)]
-+pub struct ThresholdParams {
-+    pub t: u16,
-+    pub n: u16,
-+}
-+#[derive(Clone)]
-+pub struct FrostParty {
-+    pub id: Identifier,
-+    pub index: u16,
-+    pub params: ThresholdParams,
-+    pub key_pkg: Option<KeyPackage>,
-+    pub pubkey_pkg: Option<PublicKeyPackage>,
-+    round1_secret: Option<dkg::round1::SecretPackage>,
-+    round2_secret: Option<dkg::round2::SecretPackage>,
-+}
-+
-+impl FrostParty {
-+    pub fn take_round1_secret(&mut self) -> Option<dkg::round1::SecretPackage> {
-+        self.round1_secret.take()
-     }
--    let deserialize_R = |s| {
--        Point::from_bytes(s).map_err(|e| Error::DeserializationFailed {
--            error: DeserializationError::InvalidPoint(e),
--            field_name: "signatures",
--        })
--    };
--    let deserialize_s = |s| {
--        Scalar::from_bytes(s).map_err(|e| Error::DeserializationFailed {
--            error: DeserializationError::InvalidScalar(e),
--            field_name: "signatures",
--        })
--    };
--
--    let first_sig = musig2::PartialSignature {
--        R: deserialize_R(&signatures[0].0.as_ref()[..32])?,
--        my_partial_s: deserialize_s(&signatures[0].0.as_ref()[32..])?,
--    };
--
--    let partial_sigs: Vec<_> =
--        signatures[1..].iter().map(|s| deserialize_s(&s.0.as_ref()[32..])).collect::<Result<_, _>>()?;
--
--    // Add the signatures up
--    let full_sig = musig2::aggregate_partial_signatures(&first_sig, &partial_sigs);
--
--    let mut sig_bytes = [0u8; 64];
--    sig_bytes[..32].copy_from_slice(&*full_sig.R.to_bytes(true));
--    sig_bytes[32..].copy_from_slice(&full_sig.s.to_bytes());
--    let sig = Signature::new(&sig_bytes);
--
--    // Create the same transaction again
--    let mut tx = create_unsigned_transaction(amount, &to, memo, &aggpubkey);
--    // Insert the recent_block_hash and the signature to the right places
--    tx.message.recent_blockhash = recent_block_hash;
--    assert_eq!(tx.signatures.len(), 1);
--    tx.signatures[0] = sig;
- 
--    // Make sure the resulting transaction is actually valid.
--    if tx.verify().is_err() {
--        return Err(Error::InvalidSignature);
-+    pub fn set_round1_secret(&mut self, sec: dkg::round1::SecretPackage) {
-+        self.round1_secret = Some(sec);
-     }
--    Ok(tx)
-+
-+    pub fn take_round2_secret(&mut self) -> Option<dkg::round2::SecretPackage> {
-+        self.round2_secret.take()
-+    }
-+
-+    pub fn set_round2_secret(&mut self, sec: dkg::round2::SecretPackage) {
-+        self.round2_secret = Some(sec);
-+    }
-+}
-+
-+/// DKG round 1 broadcast (same package sent to all peers).
-+#[derive(Clone)]
-+pub struct DkgRound1 {
-+    pub id: Identifier,
-+    pub pkg: dkg::round1::Package,
- }
- 
--struct PartialSigner {
--    signer_private_nonce: PrivatePartialNonces,
--    signer_public_nonce: PublicPartialNonces,
--    other_nonces: Vec<[Point<Ed25519>; 2]>,
--    extended_kepair: ExpandedKeyPair,
--    aggregated_pubkey: musig2::PublicKeyAgg,
-+/// Result per-party after finishing DKG.
-+#[derive(Clone)]
-+pub struct DkgRound2 {
-+    pub id: Identifier,
-+    pub key_pkg: KeyPackage,
-+    pub pubkey_pkg: PublicKeyPackage,
- }
- 
--impl Signer for PartialSigner {
--    fn try_pubkey(&self) -> Result<Pubkey, SignerError> {
--        Ok(Pubkey::new(&*self.aggregated_pubkey.agg_public_key.to_bytes(true)))
-+/// Round 1 of signing: each signer produces fresh nonces and commitments.
-+#[derive(Clone)]
-+pub struct SignRound1 {
-+    pub id: Identifier,
-+    pub nonces: SigningNonces,
-+    pub commitments: SigningCommitments,
-+}
-+
-+pub fn dkg_party_init(party_index: u16, params: ThresholdParams) -> Result<FrostParty, Error> {
-+    let id: Identifier = party_index
-+        .try_into()
-+        .map_err(|_| Error("party index must be nonzero <= 65535".into()))?;
-+    Ok(FrostParty {
-+        id,
-+        index: party_index,
-+        params,
-+        key_pkg: None,
-+        pubkey_pkg: None,
-+        round1_secret: None,
-+        round2_secret: None,
-+    })
-+}
-+
-+/// DKG part 1 for one party (produces a broadcast package and stores secret state).
-+pub fn dkg_round1(p: &mut FrostParty) -> Result<DkgRound1, Error> {
-+    let mut rng = OsRng;
-+    let (sec, pkg) = dkg::part1(p.id, p.params.n, p.params.t, &mut rng)
-+        .map_err(|e| Error(format!("dkg part1: {e:?}")))?;
-+    p.round1_secret = Some(sec);
-+    Ok(DkgRound1 { id: p.id, pkg })
-+}
-+
-+/// Complete DKG for all parties using the gathered part1 broadcasts.
-+/// Returns per-party key packages + a shared PublicKeyPackage (same for all).
-+pub fn dkg_round2_all(
-+    parties: &mut [FrostParty],
-+    all_r1: &[DkgRound1],
-+) -> Result<Vec<DkgRound2>, Error> {
-+    // Build "received round1 packages" mapping: receiver -> (sender -> pkg)
-+    let mut received_r1: BTreeMap<Identifier, BTreeMap<Identifier, dkg::round1::Package>> =
-+        BTreeMap::new();
-+    for r in all_r1 {
-+        for recv in parties.iter() {
-+            if recv.id == r.id {
-+                continue;
-+            }
-+            received_r1
-+                .entry(recv.id)
-+                .or_default()
-+                .insert(r.id, r.pkg.clone());
-+        }
-     }
- 
--    fn try_sign_message(&self, message: &[u8]) -> Result<Signature, SignerError> {
--        let sig = musig2::partial_sign(
--            &self.other_nonces,
--            self.signer_private_nonce.clone(),
--            self.signer_public_nonce.clone(),
--            &self.aggregated_pubkey,
--            &self.extended_kepair,
--            message,
--        );
--        let mut sig_bytes = [0u8; 64];
--        sig_bytes[..32].copy_from_slice(&*sig.R.to_bytes(true));
--        sig_bytes[32..].copy_from_slice(&sig.my_partial_s.to_bytes());
--        Ok(Signature::new(&sig_bytes))
-+    // Part 2: each participant processes received round1 packages and produces per-recipient packages.
-+    let mut r2_secrets: BTreeMap<Identifier, dkg::round2::SecretPackage> = BTreeMap::new();
-+    let mut to_deliver_r2: BTreeMap<Identifier, BTreeMap<Identifier, dkg::round2::Package>> =
-+        BTreeMap::new();
-+
-+    for p in parties.iter_mut() {
-+        let sec1 = p
-+            .round1_secret
-+            .take()
-+            .ok_or_else(|| Error("missing round1 secret".into()))?;
-+        let r1_pkgs = received_r1
-+            .get(&p.id)
-+            .ok_or_else(|| Error("missing received r1 pkgs".into()))?;
-+        let (sec2, per_recipient) =
-+            dkg::part2(sec1, r1_pkgs).map_err(|e| Error(format!("dkg part2: {e:?}")))?;
-+        r2_secrets.insert(p.id, sec2);
-+        for (recv, pkg) in per_recipient {
-+            to_deliver_r2.entry(recv).or_default().insert(p.id, pkg);
-+        }
-     }
- 
--    fn is_interactive(&self) -> bool {
--        false
-+    // Part 3: finalize key packages and the group public key package.
-+    let mut out: Vec<DkgRound2> = Vec::with_capacity(parties.len());
-+    for p in parties.iter_mut() {
-+        let sec2 = r2_secrets
-+            .remove(&p.id)
-+            .ok_or_else(|| Error("missing round2 secret".into()))?;
-+        let r1_pkgs = received_r1
-+            .get(&p.id)
-+            .ok_or_else(|| Error("missing received r1 pkgs".into()))?;
-+        let r2_pkgs = to_deliver_r2
-+            .get(&p.id)
-+            .ok_or_else(|| Error("missing received r2 pkgs".into()))?;
-+        let (key_pkg, pubkey_pkg) =
-+            dkg::part3(&sec2, r1_pkgs, r2_pkgs).map_err(|e| Error(format!("dkg part3: {e:?}")))?;
-+
-+        p.key_pkg = Some(key_pkg.clone());
-+        p.pubkey_pkg = Some(pubkey_pkg.clone());
-+        out.push(DkgRound2 {
-+            id: p.id,
-+            key_pkg,
-+            pubkey_pkg,
-+        });
-     }
-+    Ok(out)
- }
- 
--#[cfg(test)]
--mod tests {
--    use crate::native_token::lamports_to_sol;
--    use crate::serialization::Serialize;
--    use crate::tss::{key_agg, sign_and_broadcast, step_one, step_two};
--    use solana_sdk::pubkey::Pubkey;
--    use solana_sdk::signature::{Keypair, Signer};
--    use solana_streamer::socket::SocketAddrSpace;
--    use solana_test_validator::TestValidator;
-+/// Aggregate/group public key (Solana Pubkey).
-+pub fn aggregated_pubkey(any_party: &FrostParty) -> Result<Pubkey, Error> {
-+    let pk_pkg = any_party
-+        .pubkey_pkg
-+        .as_ref()
-+        .ok_or_else(|| Error("pubkey package not set; run DKG".into()))?;
-+    // Serialize the verifying key (32 bytes for Ed25519)
-+    let vk_bytes = pk_pkg
-+        .verifying_key()
-+        .serialize()
-+        .map_err(|e| Error(format!("serialize verifying key: {e:?}")))?;
-+    let vk_arr: [u8; 32] = vk_bytes
-+        .try_into()
-+        .map_err(|_| Error("unexpected verifying key length".into()))?;
- 
--    fn clone_keypair(k: &Keypair) -> Keypair {
--        Keypair::from_bytes(&k.to_bytes()).unwrap()
--    }
--    fn clone_serialize<T: Serialize>(t: &T) -> T {
--        let mut v = Vec::new();
--        t.serialize(&mut v);
--        T::deserialize(&v).unwrap()
-+    // Normalize via dalek to be extra safe
-+    let dalek =
-+        DalekPubkey::from_bytes(&vk_arr).map_err(|_| Error("invalid group pubkey".into()))?;
-+    Ok(Pubkey::new_from_array(dalek.to_bytes()))
-+}
-+
-+/// Signing round 1 for a party: produce fresh nonces + commitments.
-+/// (Commitments are sent to the coordinator; nonces are kept locally.)
-+pub fn sign_round1(p: &FrostParty) -> Result<SignRound1, Error> {
-+    let key_pkg = p
-+        .key_pkg
-+        .as_ref()
-+        .ok_or_else(|| Error("missing key package".into()))?;
-+    let mut rng = OsRng;
-+    let (nonces, commitments) = round1::commit(key_pkg.signing_share(), &mut rng);
-+    Ok(SignRound1 {
-+        id: p.id,
-+        nonces,
-+        commitments,
-+    })
-+}
-+
-+/// Build the signing package (coordinator side) for a specific message and signer set.
-+pub fn build_signing_package(
-+    message: &[u8],
-+    sign_r1_all: &[SignRound1],
-+    signer_ids: &[u16],
-+) -> Result<(SigningPackage, BTreeMap<Identifier, SigningNonces>), Error> {
-+    // Keep only selected signers (must be <= t and unique).
-+    let mut commitments_map = BTreeMap::new();
-+    let mut nonces_map = BTreeMap::new();
-+    for sid in signer_ids {
-+        let id: Identifier = (*sid)
-+            .try_into()
-+            .map_err(|_| Error("bad signer id".into()))?;
-+        let item = sign_r1_all
-+            .iter()
-+            .find(|r| r.id == id)
-+            .ok_or_else(|| Error("missing round1 for selected signer".into()))?;
-+        commitments_map.insert(id, item.commitments.clone());
-+        nonces_map.insert(id, item.nonces.clone());
-     }
--    #[test]
--    fn test_roundtrip() {
--        let n = 5;
--        let mut rng = rand07::thread_rng();
--        let keys: Vec<_> = (0..n).map(|_| Keypair::generate(&mut rng)).collect();
--        let pubkeys: Vec<_> = keys.iter().map(|k| k.pubkey()).collect();
--        // Key Generation
--        let aggpubkey = key_agg(pubkeys.clone(), None).unwrap().agg_public_key;
--        let aggpubkey_solana = Pubkey::new(&*aggpubkey.to_bytes(true));
--        let full_amount = 500_000_000;
--        // Get some money in it
--        let testnet = TestValidator::with_no_fees(aggpubkey_solana, None, SocketAddrSpace::Unspecified);
--        let rpc_client = testnet.get_rpc_client();
--
--        // step 1
--        let to = Keypair::generate(&mut rng);
--        let (first_msgs, first_secrets): (Vec<_>, Vec<_>) = keys.iter().map(clone_keypair).map(step_one).unzip();
--
--        let recent_block_hash = rpc_client.get_latest_blockhash().unwrap();
--        // step 2
--        let amount = lamports_to_sol(full_amount / 2);
--        let memo = Some("test_roundtrip".to_string());
--
--        let partial_sigs: Vec<_> = keys
-+    let sp = SigningPackage::new(commitments_map, message);
-+    Ok((sp, nonces_map))
-+}
-+
-+/// Each selected participant generates its signature share.
-+pub fn sign_round2_shares(
-+    parties: &[FrostParty],
-+    sp: &SigningPackage,
-+    nonces_map: &BTreeMap<Identifier, SigningNonces>,
-+) -> Result<BTreeMap<Identifier, round2::SignatureShare>, Error> {
-+    let mut sig_shares = BTreeMap::new();
-+    for (id, nonces) in nonces_map {
-+        let kp = parties
-             .iter()
--            .map(clone_keypair)
--            .zip(first_secrets.into_iter())
--            .enumerate()
--            .map(|(i, (key, secret))| {
--                let mut first_msgs: Vec<_> = first_msgs.iter().map(clone_serialize).collect();
--                first_msgs.remove(i);
--                step_two(key, amount, to.pubkey(), memo.clone(), recent_block_hash, pubkeys.clone(), first_msgs, secret)
--                    .unwrap()
--            })
--            .collect();
--
--        let full_tx = sign_and_broadcast(amount, to.pubkey(), memo, recent_block_hash, pubkeys, partial_sigs).unwrap();
--        let sig = rpc_client.send_transaction(&full_tx).unwrap();
--
--        // Wait for confirmation
--        rpc_client.confirm_transaction_with_spinner(&sig, &recent_block_hash, rpc_client.commitment()).unwrap();
-+            .find(|p| p.id == *id)
-+            .and_then(|p| p.key_pkg.as_ref())
-+            .ok_or_else(|| Error("missing key package for signer".into()))?;
-+        let ss = round2::sign(sp, nonces, kp).map_err(|e| Error(format!("round2 sign: {e:?}")))?;
-+        sig_shares.insert(*id, ss);
-     }
--}
-\ No newline at end of file
-+    Ok(sig_shares)
-+}
-+
-+/// Aggregate signature shares to a standard Ed25519 signature (64 bytes), verify, and return Solana `Signature`.
-+pub fn finalize_signature_solana(
-+    parties: &[FrostParty],
-+    sp: &SigningPackage,
-+    sig_shares: &BTreeMap<Identifier, round2::SignatureShare>,
-+) -> Result<SolSignature, Error> {
-+    // Get common pubkey package (same for all parties).
-+    let pk_pkg = parties
-+        .iter()
-+        .find_map(|p| p.pubkey_pkg.as_ref())
-+        .ok_or_else(|| Error("missing pubkey package".into()))?;
-+
-+    // Aggregate (also validates the signature shares internally).
-+    let group_sig: Signature =
-+        frost::aggregate(sp, sig_shares, pk_pkg).map_err(|e| Error(format!("aggregate: {e:?}")))?;
-+
-+    // Serialize signature to bytes
-+    let sig_bytes = group_sig
-+        .serialize()
-+        .map_err(|e| Error(format!("serialize signature: {e:?}")))?;
-+    let sig_arr: [u8; 64] = sig_bytes
-+        .try_into()
-+        .map_err(|_| Error("unexpected signature length".into()))?;
-+
-+    // Optional: verify with dalek before returning (defensive)
-+    let vk_bytes = pk_pkg
-+        .verifying_key()
-+        .serialize()
-+        .map_err(|e| Error(format!("serialize verifying key: {e:?}")))?;
-+    let vk_arr: [u8; 32] = vk_bytes
-+        .try_into()
-+        .map_err(|_| Error("unexpected verifying key length".into()))?;
-+    let dalek_pk =
-+        DalekPubkey::from_bytes(&vk_arr).map_err(|_| Error("invalid pubkey bytes".into()))?;
-+    let dalek_sig = DalekSig::from_bytes(&sig_arr);
-+    if let Err(e) = dalek_pk.verify(sp.message(), &dalek_sig) {
-+        return Err(Error(format!("dalek verify failed: {e:?}")));
-+    }
-+
-+    Ok(SolSignature::try_from(sig_arr).map_err(|_| Error("sol sig conversion failed".into()))?)
-+}
-+
-+/// Convenience: create a simple SOL transfer (optionally with Memo) unsigned.
-+pub fn create_unsigned_transaction(
-+    amount: u64,
-+    to: &Pubkey,
-+    memo: Option<String>,
-+    from: &Pubkey,
-+    recent_block_hash: Hash,
-+) -> Transaction {
-+    let ix = system_instruction::transfer(from, to, amount);
-+    let memo_ix_opt = memo.map(|m| {
-+        use solana_sdk::instruction::Instruction;
-+        let memo_pid = Pubkey::from_str_const("MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr");
-+        Instruction {
-+            program_id: memo_pid,
-+            accounts: vec![],
-+            data: m.into_bytes(),
-+        }
-+    });
-+    let mut ixs = vec![ix];
-+    if let Some(memo_ix) = memo_ix_opt {
-+        ixs.push(memo_ix);
-+    }
-+    let mut tx = Transaction::new_with_payer(&ixs, Some(from));
-+    tx.message.recent_blockhash = recent_block_hash;
-+    tx
-+}
-+
-+/// Attach a FROST signature to a tx and re-verify locally.
-+pub fn build_signed_tx(
-+    amount: u64,
-+    to: Pubkey,
-+    memo: Option<String>,
-+    recent_block_hash: Hash,
-+    agg_pubkey: Pubkey,
-+    sig: SolSignature,
-+) -> Result<Transaction, Error> {
-+    let mut tx = create_unsigned_transaction(amount, &to, memo, &agg_pubkey, recent_block_hash);
-+    if tx.signatures.len() != 1 {
-+        return Err(Error("unexpected signature slots".into()));
-+    }
-+
-+    // Defensive verification with dalek on message bytes.
-+    let msg_bytes = tx.message_data();
-+
-+    let dalek_pk = DalekPubkey::from_bytes(&agg_pubkey.to_bytes())
-+        .map_err(|_| Error("invalid pubkey".into()))?;
-+
-+    let sig_bytes: [u8; 64] = sig
-+        .as_ref()
-+        .try_into()
-+        .map_err(|_| Error("sig not 64 bytes".into()))?;
-+
-+    let dalek_sig =
-+        DalekSig::try_from(&sig_bytes).map_err(|_| Error("invalid signature".into()))?;
-+
-+    dalek_pk
-+        .verify(&msg_bytes, &dalek_sig)
-+        .map_err(|_| Error("dalek verification failed".into()))?;
-+
-+    tx.signatures[0] = sig;
-+    tx.verify().map_err(|_| Error("invalid signature".into()))?;
-+    Ok(tx)
-+}
-+
-+#[test]
-+fn frost_two_of_three() -> Result<(), crate::error::Error> {
-+    let params = ThresholdParams { t: 2, n: 3 };
-+
-+    use solana_sdk::signer::keypair::Keypair;
-+
-+    // Parties 1..=3
-+    let mut parties = vec![
-+        dkg_party_init(1, params)?,
-+        dkg_party_init(2, params)?,
-+        dkg_party_init(3, params)?,
-+    ];
-+
-+    // DKG part1 (broadcast)
-+    let r1_1 = dkg_round1(&mut parties[0])?;
-+    let r1_2 = dkg_round1(&mut parties[1])?;
-+    let r1_3 = dkg_round1(&mut parties[2])?;
-+    let _r2 = dkg_round2_all(&mut parties, &[r1_1, r1_2, r1_3])?;
-+
-+    // Aggregated (group) pubkey
-+    let agg_pk = aggregated_pubkey(&parties[0])?;
-+
-+    // Build unsigned tx FIRST
-+    let to = Keypair::new().pubkey();
-+    let bh = Hash::new_unique();
-+    let tx = create_unsigned_transaction(1_000, &to, None, &agg_pk, bh);
-+
-+    // The EXACT bytes that Solana verifies:
-+    let msg_bytes = tx.message_data();
-+
-+    // Round 1: nonces/commitments for chosen signers (say {1,3})
-+    let s1 = sign_round1(&parties[0])?;
-+    let s3 = sign_round1(&parties[2])?;
-+
-+    // Build signing package for {1,3} over tx.message_data()
-+    let (sp, nonces_map) = build_signing_package(&msg_bytes, &[s1.clone(), s3.clone()], &[1, 3])?;
-+
-+    // Round 2: signature shares
-+    let sig_shares = sign_round2_shares(&parties, &sp, &nonces_map)?;
-+
-+    // Aggregate to group signature (64 bytes)
-+    let sol_sig = finalize_signature_solana(&parties, &sp, &sig_shares)?;
-+
-+    // Sanity: dalek verify over the SAME msg_bytes
-+    let dalek_pk = DalekPubkey::from_bytes(&agg_pk.to_bytes())
-+        .map_err(|e| Error(format!("invalid pubkey: {e}")))?;
-+    let sig_arr: [u8; 64] = sol_sig.as_ref().try_into().unwrap();
-+    let dalek_sig = ed25519_dalek::Signature::from_bytes(&sig_arr);
-+    assert!(dalek_pk.verify(&msg_bytes, &dalek_sig).is_ok());
-+
-+    // Attach to tx and verify with Solana's checker
-+    let mut tx2 = tx.clone();
-+    tx2.signatures[0] = sol_sig;
-+    assert!(tx2.verify().is_ok());
-+
-+    Ok(())
-+}
-diff --git a/store/.DS_Store b/store/.DS_Store
-new file mode 100644
-index 0000000..63d17ba
-Binary files /dev/null and b/store/.DS_Store differ
-diff --git a/store/.sqlx/query-398c9fd2d16a0a55427927d02e1ea4056d3cfdbf7880d741fa2cc7a8989e93ba.json b/store/.sqlx/query-398c9fd2d16a0a55427927d02e1ea4056d3cfdbf7880d741fa2cc7a8989e93ba.json
-new file mode 100644
-index 0000000..c586b2d
---- /dev/null
-+++ b/store/.sqlx/query-398c9fd2d16a0a55427927d02e1ea4056d3cfdbf7880d741fa2cc7a8989e93ba.json
-@@ -0,0 +1,19 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "\n            INSERT INTO \"User\" (id, email, password, publicKey, createdAt, updatedAt)\n            VALUES ($1, $2, $3, $4, $5, $6)\n            ",
-+  "describe": {
-+    "columns": [],
-+    "parameters": {
-+      "Left": [
-+        "Uuid",
-+        "Text",
-+        "Text",
-+        "Text",
-+        "Timestamptz",
-+        "Timestamptz"
-+      ]
-+    },
-+    "nullable": []
-+  },
-+  "hash": "398c9fd2d16a0a55427927d02e1ea4056d3cfdbf7880d741fa2cc7a8989e93ba"
-+}
-diff --git a/store/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json b/store/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json
-new file mode 100644
-index 0000000..70d7ab3
---- /dev/null
-+++ b/store/.sqlx/query-9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8.json
-@@ -0,0 +1,22 @@
-+{
-+  "db_name": "PostgreSQL",
-+  "query": "SELECT id FROM \"User\" WHERE email = $1",
-+  "describe": {
-+    "columns": [
-+      {
-+        "ordinal": 0,
-+        "name": "id",
-+        "type_info": "Uuid"
-+      }
-+    ],
-+    "parameters": {
-+      "Left": [
-+        "Text"
-+      ]
-+    },
-+    "nullable": [
-+      false
-+    ]
-+  },
-+  "hash": "9e514a970adcc18e829e449b0b2802b0c866cd8825b4e49759c576c0364589d8"
-+}
-diff --git a/store/Cargo.toml b/store/Cargo.toml
-index 2ad0aee..861002c 100644
---- a/store/Cargo.toml
-+++ b/store/Cargo.toml
-@@ -1,11 +1,12 @@
- [package]
- name = "store"
- version = "0.1.0"
--edition = "2024"
-+edition = "2021"
- 
- [dependencies]
- uuid = { version = "1.0", features = ["v4"] }
- chrono = { version = "0.4", features = ["serde"] }
--sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "postgres", "uuid", "chrono"] }
--bcrypt = "0.15"
--tokio = { version = "1.0", features = ["full"] }
-+sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "postgres", "macros", "uuid", "chrono"] }
-+bcrypt = "0.16"
-+tokio = { version = "1", features = ["full"] }
-+dotenv = "0.15.0"
-diff --git a/store/migrations/.DS_Store b/store/migrations/.DS_Store
-new file mode 100644
-index 0000000..5008ddf
-Binary files /dev/null and b/store/migrations/.DS_Store differ
-diff --git a/store/migrations/20250904171707_create_users.sql b/store/migrations/20250904171707_create_users.sql
-new file mode 100644
-index 0000000..627a265
---- /dev/null
-+++ b/store/migrations/20250904171707_create_users.sql
-@@ -0,0 +1,11 @@
-+-- Add migration script here
-+CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
-+
-+CREATE TABLE IF NOT EXISTS users (
-+    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
-+    email TEXT UNIQUE NOT NULL,
-+    password_hash TEXT NOT NULL,
-+    public_key TEXT,
-+    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
-+    updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
-+);
-diff --git a/store/migrations/20250908050123_final_db.sql b/store/migrations/20250908050123_final_db.sql
-new file mode 100644
-index 0000000..a909454
---- /dev/null
-+++ b/store/migrations/20250908050123_final_db.sql
-@@ -0,0 +1,41 @@
-+-- Add migration script here
-+CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
-+
-+CREATE TABLE "User" (
-+    id          UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
-+    email       TEXT NOT NULL UNIQUE,
-+    password    TEXT NOT NULL,
-+    createdAt   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    updatedAt   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    publicKey   TEXT NOT NULL UNIQUE
-+);
-+
-+CREATE TABLE "Asset" (
-+    id          UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
-+    mintAddress TEXT NOT NULL UNIQUE,
-+    decimals    INT NOT NULL,
-+    name        TEXT NOT NULL,
-+    symbol      TEXT NOT NULL,
-+    logoUrl     TEXT,
-+    createdAt   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    updatedAt   TIMESTAMPTZ NOT NULL DEFAULT NOW()
-+);
-+
-+CREATE TABLE "Balance" (
-+    id          UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
-+    amount      BIGINT NOT NULL DEFAULT 0,
-+    createdAt   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    updatedAt   TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+
-+    -- Foreign Keys
-+    userId      UUID NOT NULL,
-+    assetId     UUID NOT NULL,
-+
-+    CONSTRAINT fk_user FOREIGN KEY(userId) REFERENCES "User"(id) ON DELETE CASCADE,
-+    CONSTRAINT fk_asset FOREIGN KEY(assetId) REFERENCES "Asset"(id) ON DELETE CASCADE,
-+    CONSTRAINT uq_user_asset UNIQUE(userId, assetId) -- one balance per asset per user
-+);
-+
-+CREATE INDEX idx_balance_user ON "Balance"(userId);
-+CREATE INDEX idx_balance_asset ON "Balance"(assetId);
-+CREATE INDEX idx_asset_mintAddress ON "Asset"(mintAddress);
-diff --git a/store/migrations_mpc1/20250910181952_create_mpc_keys_table.sql b/store/migrations_mpc1/20250910181952_create_mpc_keys_table.sql
-new file mode 100644
-index 0000000..7b38919
---- /dev/null
-+++ b/store/migrations_mpc1/20250910181952_create_mpc_keys_table.sql
-@@ -0,0 +1,8 @@
-+CREATE TABLE IF NOT EXISTS "MPCKeys" (
-+    pubkey TEXT NOT NULL,
-+    user_email TEXT NOT NULL,
-+    node_id SMALLINT NOT NULL,
-+    key_pkg BYTEA NOT NULL,
-+    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    PRIMARY KEY (pubkey, node_id)
-+);
-diff --git a/store/migrations_mpc2/20250910181958_create_mpc_keys_table.sql b/store/migrations_mpc2/20250910181958_create_mpc_keys_table.sql
-new file mode 100644
-index 0000000..19e16c7
---- /dev/null
-+++ b/store/migrations_mpc2/20250910181958_create_mpc_keys_table.sql
-@@ -0,0 +1,9 @@
-+-- Add migration script here
-+CREATE TABLE IF NOT EXISTS "MPCKeys" (
-+    pubkey TEXT NOT NULL,
-+    user_email TEXT NOT NULL,
-+    node_id SMALLINT NOT NULL,
-+    key_pkg BYTEA NOT NULL,
-+    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    PRIMARY KEY (pubkey, node_id)
-+);
-diff --git a/store/migrations_mpc3/20250910182000_create_mpc_keys_table.sql b/store/migrations_mpc3/20250910182000_create_mpc_keys_table.sql
-new file mode 100644
-index 0000000..19e16c7
---- /dev/null
-+++ b/store/migrations_mpc3/20250910182000_create_mpc_keys_table.sql
-@@ -0,0 +1,9 @@
-+-- Add migration script here
-+CREATE TABLE IF NOT EXISTS "MPCKeys" (
-+    pubkey TEXT NOT NULL,
-+    user_email TEXT NOT NULL,
-+    node_id SMALLINT NOT NULL,
-+    key_pkg BYTEA NOT NULL,
-+    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
-+    PRIMARY KEY (pubkey, node_id)
-+);
-diff --git a/store/src/lib.rs b/store/src/lib.rs
-index e291994..ff68b72 100644
---- a/store/src/lib.rs
-+++ b/store/src/lib.rs
-@@ -1,7 +1,8 @@
-+pub mod mpc_store;
- pub mod user;
--
- use sqlx::PgPool;
--
-+pub mod mpc_key;
-+#[derive(Clone)]
- pub struct Store {
-     pub pool: PgPool,
- }
-diff --git a/store/src/main.rs b/store/src/main.rs
-new file mode 100644
-index 0000000..2771fb2
---- /dev/null
-+++ b/store/src/main.rs
-@@ -0,0 +1,53 @@
-+use sqlx::PgPool;
-+use store::{Store, mpc_key::SaveKeyRequest, mpc_store::MPCStore, user::CreateUserRequest};
-+
-+#[tokio::main]
-+async fn main() -> Result<(), Box<dyn std::error::Error>> {
-+    dotenv::dotenv().ok();
-+
-+    let user_pool = PgPool::connect(&std::env::var("USER_DATABASE_URL")?).await?;
-+    let user_store = Store::new(user_pool);
-+
-+    let mpc1_pool = PgPool::connect(&std::env::var("MPC1_DATABASE_URL")?).await?;
-+    let mpc2_pool = PgPool::connect(&std::env::var("MPC2_DATABASE_URL")?).await?;
-+    let mpc3_pool = PgPool::connect(&std::env::var("MPC3_DATABASE_URL")?).await?;
-+
-+    let mpc1_store = MPCStore::new(mpc1_pool);
-+    let mpc2_store = MPCStore::new(mpc2_pool);
-+    let mpc3_store = MPCStore::new(mpc3_pool);
-+
-+    let request = CreateUserRequest {
-+        email: "kartiktomar1212@gmail.com".into(),
-+        password: "password".into(),
-+    };
-+    let user = user_store.create_user(request).await?;
-+    println!("Created user: {:?}", user);
-+
-+    let save_req1 = SaveKeyRequest {
-+        pubkey: "agg_pubkey_123".into(),
-+        user_email: user.email.clone(),
-+        node_id: 1,
-+        key_pkg: vec![1, 2, 3, 4],
-+    };
-+    mpc1_store.save_mpc_key(save_req1).await?;
-+
-+    let save_req2 = SaveKeyRequest {
-+        pubkey: "agg_pubkey_123".into(),
-+        user_email: user.email.clone(),
-+        node_id: 2,
-+        key_pkg: vec![5, 6, 7, 8],
-+    };
-+    mpc2_store.save_mpc_key(save_req2).await?;
-+
-+    let save_req3 = SaveKeyRequest {
-+        pubkey: "agg_pubkey_123".into(),
-+        user_email: user.email,
-+        node_id: 3,
-+        key_pkg: vec![9, 10, 11, 12],
-+    };
-+    mpc3_store.save_mpc_key(save_req3).await?;
-+
-+    println!("Saved MPC keys into all 3 MPC DBs");
-+
-+    Ok(())
-+}
-diff --git a/store/src/mpc_key.rs b/store/src/mpc_key.rs
-new file mode 100644
-index 0000000..cecbe23
---- /dev/null
-+++ b/store/src/mpc_key.rs
-@@ -0,0 +1,85 @@
-+use crate::mpc_store::MPCStore;
-+use chrono::{DateTime, Utc};
-+use sqlx::Row;
-+
-+#[derive(Debug, Clone)]
-+pub struct MPCKey {
-+    pub pubkey: String,
-+    pub user_email: String,
-+    pub node_id: i16,
-+    pub key_pkg: Vec<u8>,
-+    pub created_at: DateTime<Utc>,
-+}
-+
-+#[derive(Debug)]
-+pub struct SaveKeyRequest {
-+    pub pubkey: String,
-+    pub user_email: String,
-+    pub node_id: i16,
-+    pub key_pkg: Vec<u8>,
-+}
-+
-+#[derive(Debug)]
-+pub enum MPCKeyError {
-+    DatabaseError(String),
-+    NotFound,
-+}
-+
-+impl std::fmt::Display for MPCKeyError {
-+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-+        match self {
-+            MPCKeyError::DatabaseError(msg) => write!(f, "Database error: {}", msg),
-+            MPCKeyError::NotFound => write!(f, "MPC key not found"),
-+        }
-+    }
-+}
-+impl std::error::Error for MPCKeyError {}
-+
-+impl MPCStore {
-+    pub async fn save_mpc_key(&self, req: SaveKeyRequest) -> Result<(), MPCKeyError> {
-+        sqlx::query(
-+            r#"
-+            INSERT INTO "MPCKeys" (pubkey, user_email, node_id, key_pkg, created_at)
-+            VALUES ($1, $2, $3, $4, NOW())
-+            ON CONFLICT (pubkey, node_id)
-+            DO UPDATE SET key_pkg = EXCLUDED.key_pkg, created_at = NOW()
-+            "#,
-+        )
-+        .bind(&req.pubkey)
-+        .bind(&req.user_email)
-+        .bind(req.node_id)
-+        .bind(&req.key_pkg)
-+        .execute(&self.pool)
-+        .await
-+        .map_err(|e| MPCKeyError::DatabaseError(e.to_string()))?;
-+
-+        Ok(())
-+    }
-+
-+    pub async fn get_mpc_key(&self, pubkey: &str, node_id: i16) -> Result<MPCKey, MPCKeyError> {
-+        let row = sqlx::query(
-+            r#"
-+            SELECT pubkey, user_email, node_id, key_pkg, created_at
-+            FROM "MPCKeys"
-+            WHERE pubkey = $1 AND node_id = $2
-+            "#,
-+        )
-+        .bind(pubkey)
-+        .bind(node_id)
-+        .fetch_optional(&self.pool)
-+        .await
-+        .map_err(|e| MPCKeyError::DatabaseError(e.to_string()))?;
-+
-+        if let Some(r) = row {
-+            Ok(MPCKey {
-+                pubkey: r.get::<String, _>("pubkey"),
-+                user_email: r.get::<String, _>("user_email"),
-+                node_id: r.get::<i16, _>("node_id"),
-+                key_pkg: r.get::<Vec<u8>, _>("key_pkg"),
-+                created_at: r.get::<DateTime<Utc>, _>("created_at"),
-+            })
-+        } else {
-+            Err(MPCKeyError::NotFound)
-+        }
-+    }
-+}
-diff --git a/store/src/mpc_store.rs b/store/src/mpc_store.rs
-new file mode 100644
-index 0000000..46fd6ba
---- /dev/null
-+++ b/store/src/mpc_store.rs
-@@ -0,0 +1,12 @@
-+use sqlx::PgPool;
-+
-+#[derive(Clone)]
-+pub struct MPCStore {
-+    pub pool: PgPool,
-+}
-+
-+impl MPCStore {
-+    pub fn new(pool: PgPool) -> Self {
-+        Self { pool }
-+    }
-+}
-diff --git a/store/src/user.rs b/store/src/user.rs
-index 30b1f27..32d9d41 100644
---- a/store/src/user.rs
-+++ b/store/src/user.rs
-@@ -1,12 +1,14 @@
- use crate::Store;
--use uuid::Uuid;
- use chrono::{DateTime, Utc};
-+use uuid::Uuid;
- 
- #[derive(Debug, Clone)]
- pub struct User {
--    pub id: String,
-+    pub id: Uuid,
-     pub email: String,
--    pub created_at: String,
-+    pub public_key: Option<String>,
-+    pub created_at: DateTime<Utc>,
-+    pub updated_at: DateTime<Utc>,
- }
- 
- #[derive(Debug)]
-@@ -36,56 +38,55 @@ impl std::error::Error for UserError {}
- 
- impl Store {
-     pub async fn create_user(&self, request: CreateUserRequest) -> Result<User, UserError> {
--        // Validate email format
-         if !request.email.contains('@') {
-             return Err(UserError::InvalidInput("Invalid email format".to_string()));
-         }
- 
--        // Validate password length
-         if request.password.len() < 6 {
--            return Err(UserError::InvalidInput("Password must be at least 6 characters".to_string()));
-+            return Err(UserError::InvalidInput(
-+                "Password must be at least 6 characters".to_string(),
-+            ));
-         }
- 
--        // Check if user already exists
--        let existing_user = sqlx::query!(
--            "SELECT id FROM users WHERE email = $1",
--            request.email
--        )
--        .fetch_optional(&self.pool)
--        .await
--        .map_err(|e| UserError::DatabaseError(e.to_string()))?;
-+        let existing_user =
-+            sqlx::query!(r#"SELECT id FROM "User" WHERE email = $1"#, request.email)
-+                .fetch_optional(&self.pool)
-+                .await
-+                .map_err(|e| UserError::DatabaseError(e.to_string()))?;
- 
-         if existing_user.is_some() {
-             return Err(UserError::UserExists);
-         }
- 
--        // Hash the password
-         let password_hash = bcrypt::hash(&request.password, bcrypt::DEFAULT_COST)
-             .map_err(|e| UserError::DatabaseError(format!("Password hashing failed: {}", e)))?;
- 
--        // Generate user ID and timestamp
--        let user_id = Uuid::new_v4().to_string();
-+        let user_id = Uuid::new_v4();
-         let created_at = Utc::now();
-+        let updated_at = Utc::now();
- 
--        // Insert user into database
-         sqlx::query!(
--            "INSERT INTO users (id, email, password_hash, created_at) VALUES ($1, $2, $3, $4)",
-+            r#"
-+            INSERT INTO "User" (id, email, password, publicKey, createdAt, updatedAt)
-+            VALUES ($1, $2, $3, $4, $5, $6)
-+            "#,
-             user_id,
-             request.email,
-             password_hash,
--            created_at
-+            Option::<String>::None,
-+            created_at,
-+            updated_at
-         )
-         .execute(&self.pool)
-         .await
-         .map_err(|e| UserError::DatabaseError(e.to_string()))?;
- 
--        // Return the created user
--        let user = User {
-+        Ok(User {
-             id: user_id,
-             email: request.email,
--            created_at: created_at.to_rfc3339(),
--        };
--
--        Ok(user)
-+            public_key: None,
-+            created_at,
-+            updated_at,
-+        })
-     }
- }
diff --git a/backend/src/routes/solana.rs b/backend/src/routes/solana.rs
index d7fd357..e255625 100644
--- a/backend/src/routes/solana.rs
+++ b/backend/src/routes/solana.rs
@@ -17,7 +17,7 @@ lazy_static! {
     static ref QUOTES: Mutex<HashMap<String, serde_json::Value>> = Mutex::new(HashMap::new());
 }
 const JUP_QUOTE_URL: &str = "https://lite-api.jup.ag/swap/v1/quote";
-const JUP_SWAP_URL: &str = "https://quote-api.jup.ag/v6/swap";
+const JUP_SWAP_URL: &str = "https://lite-api.jup.ag/swap/v1/swap";
 
 #[derive(Deserialize)]
 pub struct SignUpRequest {
@@ -48,7 +48,6 @@ pub struct AuthResponse {
     pub token: String,
 }
 
-
 #[derive(Serialize)]
 pub struct SignupResponse {
     message: String,
@@ -60,8 +59,6 @@ struct Claims {
     exp: usize,
 }
 
-
-
 #[derive(Deserialize)]
 pub struct QuoteRequest {
     #[serde(rename = "inputMint")]
@@ -84,7 +81,6 @@ pub struct SwapRequest {
     pub id: String,
 }
 
-
 #[derive(Deserialize)]
 pub struct SendRequest {
     pub to: String,
@@ -223,11 +219,12 @@ pub async fn sign_up(
             .map_err(|e| {
                 actix_web::error::ErrorInternalServerError(format!("finalize error: {e}"))
             })?;
-        
+
         if !finalize_resp.status().is_success() {
             let error_text = finalize_resp.text().await.unwrap_or_default();
             return Err(actix_web::error::ErrorInternalServerError(format!(
-                "DKG finalize failed on {}: {}", url, error_text
+                "DKG finalize failed on {}: {}",
+                url, error_text
             )));
         }
     }
@@ -267,7 +264,7 @@ pub async fn sign_up(
     .await
     .map_err(|e| actix_web::error::ErrorInternalServerError(format!("DB insert error: {e}")))?;
 
-        // Note: DKG state is now automatically saved during dkg_finalize with placeholder email
+    // Note: DKG state is now automatically saved during dkg_finalize with placeholder email
     // TODO: In production, add an email-only update endpoint to replace placeholder with real email
     println!("DKG state auto-saved with placeholder email - consider adding email update endpoint for production");
 
@@ -298,9 +295,7 @@ pub async fn sign_in(
         if verify(&req.password, &user.password).unwrap_or(false) {
             let jwt = generate_jwt(&user.email);
 
-            return Ok(HttpResponse::Ok().json(AuthResponse {
-                token: jwt,
-            }));
+            return Ok(HttpResponse::Ok().json(AuthResponse { token: jwt }));
         }
     }
 
@@ -432,18 +427,40 @@ pub async fn swap(
     })?;
 
     // Decode Jupiter tx into VersionedTransaction
-    let tx_bytes = base64::Engine::decode(&base64::engine::general_purpose::STANDARD, tx_b64).map_err(|e| {
-        actix_web::error::ErrorInternalServerError(format!("base64 decode error: {e}"))
-    })?;
+    let tx_bytes = base64::Engine::decode(&base64::engine::general_purpose::STANDARD, tx_b64)
+        .map_err(|e| {
+            actix_web::error::ErrorInternalServerError(format!("base64 decode error: {e}"))
+        })?;
     let mut tx: VersionedTransaction = bincode::deserialize(&tx_bytes).map_err(|e| {
         actix_web::error::ErrorInternalServerError(format!("tx deserialize error: {e}"))
     })?;
 
+    // Get fresh blockhash to avoid "Blockhash not found" error
+    let rpc_url =
+        env::var("SOLANA_RPC_URL").unwrap_or_else(|_| "https://api.mainnet-beta.solana.com".to_string());
+    let rpc = RpcClient::new(rpc_url);
+    
+    let latest_blockhash = rpc
+        .get_latest_blockhash()
+        .await
+        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to get latest blockhash: {e}")))?;
+    
+    // Update transaction with fresh blockhash
+    match &mut tx.message {
+        solana_sdk::message::VersionedMessage::Legacy(ref mut msg) => {
+            msg.recent_blockhash = latest_blockhash;
+        }
+        solana_sdk::message::VersionedMessage::V0(ref mut msg) => {
+            msg.recent_blockhash = latest_blockhash;
+        }
+    }
+
     // Ensure DKG state is loaded before signing
-    ensure_dkg_loaded(&user_pubkey).await
-        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}")))?;
+    ensure_dkg_loaded(&user_pubkey).await.map_err(|e| {
+        actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}"))
+    })?;
 
-    //  MPC sign message (use Jupiter's blockhash, don't override)
+    // MPC sign message with updated blockhash
     let msg_hex = hex::encode(tx.message.serialize());
     println!(
         "Message hex (first 64 chars): {}",
@@ -453,17 +470,60 @@ pub async fn swap(
     let sig = mpc_sign_message(msg_hex)
         .await
         .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC sign failed: {e}")))?;
-    tx.signatures = vec![sig]; // Replace Jupiter’s placeholder
-
-    // Send to Solana RPC
-    let rpc_url = env::var("SOLANA_RPC_URL")
-        .unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
-    let rpc = RpcClient::new(rpc_url);
-
-    let signature = rpc
-        .send_and_confirm_transaction(&tx)
-        .await
-        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Send tx error: {e}")))?;
+    tx.signatures = vec![sig]; // Replace Jupiter's placeholder
+
+    // Send to Solana RPC with retry mechanism
+    let mut retries = 3;
+    let mut signature = None;
+    let mut last_error = String::new();
+    
+    while retries > 0 {
+        match rpc
+            .send_and_confirm_transaction(&tx)
+            .await
+        {
+            Ok(sig) => {
+                signature = Some(sig);
+                break;
+            }
+            Err(e) => {
+                last_error = e.to_string();
+                println!("Transaction failed (retries left: {}): {}", retries - 1, e);
+                retries -= 1;
+                if retries > 0 {
+                    // Wait a bit before retrying
+                    tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;
+                    
+                    // Get fresh blockhash for retry
+                    let new_blockhash = rpc
+                        .get_latest_blockhash()
+                        .await
+                        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to get fresh blockhash for retry: {e}")))?;
+                    
+                    // Update transaction with new blockhash
+                    match &mut tx.message {
+                        solana_sdk::message::VersionedMessage::Legacy(ref mut msg) => {
+                            msg.recent_blockhash = new_blockhash;
+                        }
+                        solana_sdk::message::VersionedMessage::V0(ref mut msg) => {
+                            msg.recent_blockhash = new_blockhash;
+                        }
+                    }
+                    
+                    // Re-sign with new blockhash
+                    let msg_hex = hex::encode(tx.message.serialize());
+                    let sig = mpc_sign_message(msg_hex)
+                        .await
+                        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("MPC re-sign failed: {e}")))?;
+                    tx.signatures = vec![sig];
+                }
+            }
+        }
+    }
+    
+    let signature = signature.ok_or_else(|| {
+        actix_web::error::ErrorInternalServerError(format!("Transaction failed after all retries. Last error: {}", last_error))
+    })?;
 
     println!("Swap sent successfully. Signature={}", signature);
 
@@ -510,7 +570,7 @@ pub async fn send(
         .map_err(|_| actix_web::error::ErrorInternalServerError("Bad stored publicKey"))?;
 
     let rpc_url =
-        env::var("SOLANA_RPC_URL").unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
+        env::var("SOLANA_RPC_URL").unwrap_or_else(|_| "https://api.mainnet-beta.solana.com".to_string());
     let rpc = RpcClient::new(rpc_url);
 
     let blockhash = rpc
@@ -526,15 +586,20 @@ pub async fn send(
         return Ok(HttpResponse::BadRequest().body("Token transfers not yet implemented"));
     } else {
         // SOL transfer
-        vec![system_instruction::transfer(&from_pubkey, &to_pubkey, req.amount)]
+        vec![system_instruction::transfer(
+            &from_pubkey,
+            &to_pubkey,
+            req.amount,
+        )]
     };
 
     let mut tx = Transaction::new_with_payer(&instructions, Some(&from_pubkey));
     tx.message.recent_blockhash = blockhash;
 
     // Ensure DKG state is loaded before signing
-    ensure_dkg_loaded(&row.publickey).await
-        .map_err(|e| actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}")))?;
+    ensure_dkg_loaded(&row.publickey).await.map_err(|e| {
+        actix_web::error::ErrorInternalServerError(format!("Failed to load DKG state: {e}"))
+    })?;
 
     let msg_hex = hex::encode(tx.message_data());
     let sig = mpc_sign_message(msg_hex)
@@ -572,11 +637,16 @@ async fn ensure_dkg_loaded(user_pubkey: &str) -> Result<(), String> {
             .await
             .map_err(|e| format!("Failed to load DKG state from {}: {}", url, e))?;
 
-        let load_result: serde_json::Value = load_resp.json().await
+        let load_result: serde_json::Value = load_resp
+            .json()
+            .await
             .map_err(|e| format!("Failed to parse load response from {}: {}", url, e))?;
 
         if load_result["status"] == "not_found" {
-            return Err(format!("DKG state not found for pubkey {} on node {}", user_pubkey, url));
+            return Err(format!(
+                "DKG state not found for pubkey {} on node {}",
+                user_pubkey, url
+            ));
         }
 
         println!("Loaded DKG state from {}: {:?}", url, load_result);
@@ -695,8 +765,8 @@ pub async fn get_sol_balance(
     pool: web::Data<PgPool>,
     auth: AuthToken,
 ) -> Result<HttpResponse, ActixError> {
-    let email = validate_jwt(&auth.0)
-        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
+    let email =
+        validate_jwt(&auth.0).ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
 
     let row = sqlx::query!("SELECT publickey FROM \"User\" WHERE email = $1", email)
         .fetch_one(pool.get_ref())
@@ -708,16 +778,13 @@ pub async fn get_sol_balance(
     let user_pubkey = Pubkey::from_str(&row.publickey)
         .map_err(|_| actix_web::error::ErrorInternalServerError("Invalid stored publicKey"))?;
 
-    let rpc_url = env::var("SOLANA_RPC_URL")
-        .unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
+    let rpc_url =
+        env::var("SOLANA_RPC_URL").unwrap_or_else(|_| "https://api.devnet.solana.com".to_string());
     let rpc = RpcClient::new(rpc_url);
 
-    let balance = rpc
-        .get_balance(&user_pubkey)
-        .await
-        .map_err(|e| {
-            actix_web::error::ErrorInternalServerError(format!("Failed to get balance: {e}"))
-        })?;
+    let balance = rpc.get_balance(&user_pubkey).await.map_err(|e| {
+        actix_web::error::ErrorInternalServerError(format!("Failed to get balance: {e}"))
+    })?;
 
     Ok(HttpResponse::Ok().json(SolBalanceResponse { balance }))
 }
@@ -727,8 +794,8 @@ pub async fn get_token_balances(
     pool: web::Data<PgPool>,
     auth: AuthToken,
 ) -> Result<HttpResponse, ActixError> {
-    let email = validate_jwt(&auth.0)
-        .ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
+    let email =
+        validate_jwt(&auth.0).ok_or_else(|| actix_web::error::ErrorUnauthorized("Unauthorized"))?;
 
     // Get user's public key from database
     let user_row = sqlx::query!("SELECT publickey FROM \"User\" WHERE email = $1", email)
diff --git a/indexer/src/main.rs b/indexer/src/main.rs
index 549890d..6b8fddd 100644
--- a/indexer/src/main.rs
+++ b/indexer/src/main.rs
@@ -1,11 +1,11 @@
 use dotenvy::dotenv;
+use reqwest::Client;
+use solana_client::rpc_client::RpcClient;
 use solana_sdk::pubkey::Pubkey;
 use sqlx::PgPool;
 use std::collections::HashSet;
 use std::env;
 use std::str::FromStr;
-use reqwest::Client;
-use solana_client::rpc_client::RpcClient;
 
 // Helius API structures - using flexible JSON parsing
 type HeliusTransaction = serde_json::Value;
@@ -16,10 +16,10 @@ async fn load_tracked_pubkeys(pool: &PgPool) -> anyhow::Result<HashSet<Pubkey>>
     let rows = sqlx::query!(r#"SELECT publickey FROM "User""#)
         .fetch_all(pool)
         .await?;
-    
+
     let mut tracked_pubkeys = HashSet::new();
     let mut invalid_pubkeys = Vec::new();
-    
+
     for row in rows {
         match Pubkey::from_str(&row.publickey) {
             Ok(pubkey) => {
@@ -30,28 +30,28 @@ async fn load_tracked_pubkeys(pool: &PgPool) -> anyhow::Result<HashSet<Pubkey>>
             }
         }
     }
-    
+
     if !invalid_pubkeys.is_empty() {
-        eprintln!("Warning: Found {} invalid public keys in database:", invalid_pubkeys.len());
+        eprintln!(
+            "Warning: Found {} invalid public keys in database:",
+            invalid_pubkeys.len()
+        );
         for (pubkey_str, error) in invalid_pubkeys {
             eprintln!("  - {}: {}", pubkey_str, error);
         }
     }
-    
+
     Ok(tracked_pubkeys)
 }
 
 async fn ensure_sol_asset(pool: &PgPool) -> anyhow::Result<()> {
     const SOL_MINT: &str = "So11111111111111111111111111111111111111112";
-    
+
     // Check if SOL asset exists
-    let exists = sqlx::query!(
-        r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#,
-        SOL_MINT
-    )
-    .fetch_optional(pool)
-    .await?;
-    
+    let exists = sqlx::query!(r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#, SOL_MINT)
+        .fetch_optional(pool)
+        .await?;
+
     if exists.is_none() {
         println!("SOL asset not found, creating it...");
         sqlx::query!(
@@ -71,7 +71,7 @@ async fn ensure_sol_asset(pool: &PgPool) -> anyhow::Result<()> {
     } else {
         println!("SOL asset already exists");
     }
-    
+
     Ok(())
 }
 
@@ -85,25 +85,32 @@ async fn fetch_helius_transactions(
         "https://api.helius.xyz/v0/addresses/{}/transactions?api-key={}&cluster={}",
         address, api_key, network
     );
-    
-    println!("🔍 Fetching transactions for {} on {}: {}", address, network, url);
-    
-    let response = client
-        .get(&url)
-        .send()
-        .await?;
-    
+
+    println!(
+        "Fetching transactions for {} on {}: {}",
+        address, network, url
+    );
+
+    let response = client.get(&url).send().await?;
+
     let status = response.status();
     if !status.is_success() {
         let error_text = response.text().await.unwrap_or_default();
-        return Err(anyhow::anyhow!("Helius API error: {} - {}", status, error_text));
+        return Err(anyhow::anyhow!(
+            "Helius API error: {} - {}",
+            status,
+            error_text
+        ));
     }
-    
+
     let response_text = response.text().await?;
-    println!("Raw Helius API response: {}", &response_text[..500.min(response_text.len())]);
-    
+    println!(
+        "Raw Helius API response: {}",
+        &response_text[..500.min(response_text.len())]
+    );
+
     let transactions: Vec<HeliusTransaction> = serde_json::from_str(&response_text)?;
-    println!("📊 Parsed {} transactions", transactions.len());
+    println!("Parsed {} transactions", transactions.len());
     Ok(transactions)
 }
 
@@ -117,23 +124,27 @@ async fn fetch_transaction_by_signature(
         "https://api.helius.xyz/v0/transactions?api-key={}&cluster={}&signature={}",
         api_key, network, signature
     );
-    
-    println!("🔍 Looking up transaction by signature: {}", url);
-    
-    let response = client
-        .get(&url)
-        .send()
-        .await?;
-    
+
+    println!("Looking up transaction by signature: {}", url);
+
+    let response = client.get(&url).send().await?;
+
     let status = response.status();
     if !status.is_success() {
         let error_text = response.text().await.unwrap_or_default();
-        return Err(anyhow::anyhow!("Helius API error: {} - {}", status, error_text));
+        return Err(anyhow::anyhow!(
+            "Helius API error: {} - {}",
+            status,
+            error_text
+        ));
     }
-    
+
     let response_text = response.text().await?;
-    println!("Raw transaction lookup response: {}", &response_text[..500.min(response_text.len())]);
-    
+    println!(
+        "Raw transaction lookup response: {}",
+        &response_text[..500.min(response_text.len())]
+    );
+
     let transactions: Vec<HeliusTransaction> = serde_json::from_str(&response_text)?;
     Ok(transactions.into_iter().next())
 }
@@ -142,33 +153,57 @@ async fn fetch_rpc_transactions(
     rpc_client: &RpcClient,
     address: &Pubkey,
 ) -> anyhow::Result<Vec<serde_json::Value>> {
-    println!("🔍 Fetching transactions from Solana RPC for {}", address);
-    
+    println!("Fetching transactions from Solana RPC for {}", address);
+
     // Get the current SOL balance from the blockchain
     let current_balance = rpc_client
         .get_balance(address)
         .map_err(|e| anyhow::anyhow!("RPC error getting balance: {}", e))?;
-    
-    println!("💰 Current SOL balance for {}: {} lamports", address, current_balance);
-    
+
+    println!(
+        "💰 Current SOL balance for {}: {} lamports",
+        address, current_balance
+    );
+
     // Create a single transaction entry that represents the current balance sync
     let mut tx_json = serde_json::Map::new();
-    tx_json.insert("signature".to_string(), serde_json::Value::String("balance_sync".to_string()));
-    tx_json.insert("slot".to_string(), serde_json::Value::Number(serde_json::Number::from(0)));
-    tx_json.insert("blockTime".to_string(), serde_json::Value::Number(serde_json::Number::from(0)));
-    
+    tx_json.insert(
+        "signature".to_string(),
+        serde_json::Value::String("balance_sync".to_string()),
+    );
+    tx_json.insert(
+        "slot".to_string(),
+        serde_json::Value::Number(serde_json::Number::from(0)),
+    );
+    tx_json.insert(
+        "blockTime".to_string(),
+        serde_json::Value::Number(serde_json::Number::from(0)),
+    );
+
     // Create a native transfer entry with the current balance
     let mut native_transfers = Vec::new();
     let mut transfer = serde_json::Map::new();
-    transfer.insert("fromUserAccount".to_string(), serde_json::Value::String(address.to_string()));
-    transfer.insert("toUserAccount".to_string(), serde_json::Value::String(address.to_string()));
-    transfer.insert("amount".to_string(), serde_json::Value::Number(serde_json::Number::from(current_balance)));
+    transfer.insert(
+        "fromUserAccount".to_string(),
+        serde_json::Value::String(address.to_string()),
+    );
+    transfer.insert(
+        "toUserAccount".to_string(),
+        serde_json::Value::String(address.to_string()),
+    );
+    transfer.insert(
+        "amount".to_string(),
+        serde_json::Value::Number(serde_json::Number::from(current_balance)),
+    );
     native_transfers.push(serde_json::Value::Object(transfer));
-    tx_json.insert("nativeTransfers".to_string(), serde_json::Value::Array(native_transfers));
-    
+    tx_json.insert(
+        "nativeTransfers".to_string(),
+        serde_json::Value::Array(native_transfers),
+    );
+
     let mut transactions = Vec::new();
     transactions.push(serde_json::Value::Object(tx_json));
-    
+
     Ok(transactions)
 }
 
@@ -178,7 +213,10 @@ async fn process_helius_transaction(
     tracked_pubkeys: &HashSet<Pubkey>,
 ) -> anyhow::Result<()> {
     // Process native transfers (SOL)
-    if let Some(native_transfers) = transaction.get("nativeTransfers").and_then(|v| v.as_array()) {
+    if let Some(native_transfers) = transaction
+        .get("nativeTransfers")
+        .and_then(|v| v.as_array())
+    {
         for transfer in native_transfers {
             if let (Some(from_pubkey), Some(to_pubkey), Some(amount)) = (
                 transfer.get("fromUserAccount").and_then(|v| v.as_str()),
@@ -186,22 +224,29 @@ async fn process_helius_transaction(
                 transfer.get("amount").and_then(|v| v.as_u64()),
             ) {
                 // Check if either account is tracked
-                let from_tracked = Pubkey::from_str(from_pubkey).ok()
+                let from_tracked = Pubkey::from_str(from_pubkey)
+                    .ok()
                     .map(|pk| tracked_pubkeys.contains(&pk))
                     .unwrap_or(false);
-                let to_tracked = Pubkey::from_str(to_pubkey).ok()
+                let to_tracked = Pubkey::from_str(to_pubkey)
+                    .ok()
                     .map(|pk| tracked_pubkeys.contains(&pk))
                     .unwrap_or(false);
-                
+
                 if from_tracked || to_tracked {
                     if amount > 0 {
-                        println!("Processing SOL transfer: {} -> {} ({} lamports)", 
-                            from_pubkey, to_pubkey, amount);
-                        
+                        println!(
+                            "Processing SOL transfer: {} -> {} ({} lamports)",
+                            from_pubkey, to_pubkey, amount
+                        );
+
                         // Check if this is a balance sync (same from/to address)
                         if from_pubkey == to_pubkey {
                             // This is a balance sync, set the balance directly
-                            println!("🔄 Syncing SOL balance for {}: {} lamports", from_pubkey, amount);
+                            println!(
+                                "Syncing SOL balance for {}: {} lamports",
+                                from_pubkey, amount
+                            );
                             sync_sol_balance(pool, from_pubkey, amount as i64).await?;
                         } else {
                             // This is a real transfer, update balances
@@ -213,14 +258,16 @@ async fn process_helius_transaction(
                             }
                         }
                     } else {
-                        println!("Skipping zero-amount transfer: {} -> {} ({} lamports)", 
-                            from_pubkey, to_pubkey, amount);
+                        println!(
+                            "Skipping zero-amount transfer: {} -> {} ({} lamports)",
+                            from_pubkey, to_pubkey, amount
+                        );
                     }
                 }
             }
         }
     }
-    
+
     // Process token transfers
     if let Some(token_transfers) = transaction.get("tokenTransfers").and_then(|v| v.as_array()) {
         for transfer in token_transfers {
@@ -231,31 +278,45 @@ async fn process_helius_transaction(
                 transfer.get("mint").and_then(|v| v.as_str()),
             ) {
                 // Check if either account is tracked
-                let from_tracked = Pubkey::from_str(from_pubkey).ok()
+                let from_tracked = Pubkey::from_str(from_pubkey)
+                    .ok()
                     .map(|pk| tracked_pubkeys.contains(&pk))
                     .unwrap_or(false);
-                let to_tracked = Pubkey::from_str(to_pubkey).ok()
+                let to_tracked = Pubkey::from_str(to_pubkey)
+                    .ok()
                     .map(|pk| tracked_pubkeys.contains(&pk))
                     .unwrap_or(false);
-                
+
                 if from_tracked || to_tracked {
-                    println!("Processing token transfer: {} -> {} ({} {})", 
-                        from_pubkey, to_pubkey, token_amount, mint);
-                    
+                    println!(
+                        "Processing token transfer: {} -> {} ({} {})",
+                        from_pubkey, to_pubkey, token_amount, mint
+                    );
+
                     // Update token balance for the tracked account
                     if from_tracked {
-                        update_token_balance(pool, from_pubkey, mint, 
-                            -(token_amount.parse::<i64>().unwrap_or(0))).await?;
+                        update_token_balance(
+                            pool,
+                            from_pubkey,
+                            mint,
+                            -(token_amount.parse::<i64>().unwrap_or(0)),
+                        )
+                        .await?;
                     }
                     if to_tracked {
-                        update_token_balance(pool, to_pubkey, mint, 
-                            token_amount.parse::<i64>().unwrap_or(0)).await?;
+                        update_token_balance(
+                            pool,
+                            to_pubkey,
+                            mint,
+                            token_amount.parse::<i64>().unwrap_or(0),
+                        )
+                        .await?;
                     }
                 }
             }
         }
     }
-    
+
     Ok(())
 }
 
@@ -263,7 +324,7 @@ async fn update_sol_balance(pool: &PgPool, pubkey: &str, amount_change: i64) ->
     // Get current balance
     let current_balance = sqlx::query!(
         r#"
-        SELECT b.amount 
+        SELECT b.amount
         FROM "Balance" b
         JOIN "User" u ON b.userId = u.id
         JOIN "Asset" a ON b.assetId = a.id
@@ -275,9 +336,9 @@ async fn update_sol_balance(pool: &PgPool, pubkey: &str, amount_change: i64) ->
     .await?
     .map(|row| row.amount)
     .unwrap_or(0);
-    
+
     let new_balance = (current_balance + amount_change).max(0);
-    
+
     // Use upsert to create or update balance
     sqlx::query!(
         r#"
@@ -297,8 +358,11 @@ async fn update_sol_balance(pool: &PgPool, pubkey: &str, amount_change: i64) ->
     )
     .execute(pool)
     .await?;
-    
-    println!("Updated SOL balance for {}: {} lamports", pubkey, new_balance);
+
+    println!(
+        "Updated SOL balance for {}: {} lamports",
+        pubkey, new_balance
+    );
     Ok(())
 }
 
@@ -322,20 +386,22 @@ async fn sync_sol_balance(pool: &PgPool, pubkey: &str, balance: i64) -> anyhow::
     )
     .execute(pool)
     .await?;
-    
-    println!("🔄 Synced SOL balance for {}: {} lamports", pubkey, balance);
+
+    println!("Synced SOL balance for {}: {} lamports", pubkey, balance);
     Ok(())
 }
 
-async fn update_token_balance(pool: &PgPool, pubkey: &str, mint: &str, amount_change: i64) -> anyhow::Result<()> {
+async fn update_token_balance(
+    pool: &PgPool,
+    pubkey: &str,
+    mint: &str,
+    amount_change: i64,
+) -> anyhow::Result<()> {
     // Ensure asset exists
-    let asset_exists = sqlx::query!(
-        r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#,
-        mint
-    )
-    .fetch_optional(pool)
-    .await?;
-    
+    let asset_exists = sqlx::query!(r#"SELECT id FROM "Asset" WHERE mintAddress = $1"#, mint)
+        .fetch_optional(pool)
+        .await?;
+
     if asset_exists.is_none() {
         // Create asset if it doesn't exist
         sqlx::query!(
@@ -351,11 +417,11 @@ async fn update_token_balance(pool: &PgPool, pubkey: &str, mint: &str, amount_ch
         .execute(pool)
         .await?;
     }
-    
+
     // Get current balance
     let current_balance = sqlx::query!(
         r#"
-        SELECT b.amount 
+        SELECT b.amount
         FROM "Balance" b
         JOIN "User" u ON b.userId = u.id
         JOIN "Asset" a ON b.assetId = a.id
@@ -368,9 +434,9 @@ async fn update_token_balance(pool: &PgPool, pubkey: &str, mint: &str, amount_ch
     .await?
     .map(|row| row.amount)
     .unwrap_or(0);
-    
+
     let new_balance = (current_balance + amount_change).max(0);
-    
+
     sqlx::query!(
         r#"
         UPDATE "Balance"
@@ -384,8 +450,11 @@ async fn update_token_balance(pool: &PgPool, pubkey: &str, mint: &str, amount_ch
     )
     .execute(pool)
     .await?;
-    
-    println!("Updated token balance for {}: {} {}", pubkey, new_balance, mint);
+
+    println!(
+        "Updated token balance for {}: {} {}",
+        pubkey, new_balance, mint
+    );
     Ok(())
 }
 
@@ -397,22 +466,28 @@ async fn main() -> anyhow::Result<()> {
     let pool = PgPool::connect(&db_url).await?;
 
     let mut tracked_pubkeys = load_tracked_pubkeys(&pool).await?;
-    println!("Loaded {} valid tracked pubkeys from DB", tracked_pubkeys.len());
+    println!(
+        "Loaded {} valid tracked pubkeys from DB",
+        tracked_pubkeys.len()
+    );
 
     if tracked_pubkeys.is_empty() {
         println!("No users found in database. Indexer will wait for users to be added.");
-        println!("You can add users through the backend API, and the indexer will automatically start tracking them.");
+        println!(
+            "You can add users through the backend API, and the indexer will automatically start tracking them."
+        );
     }
 
     // Ensure SOL asset exists in database
     ensure_sol_asset(&pool).await?;
 
-    let helius_api_key = env::var("HELIUS_API_KEY").unwrap_or_else(|_| "63ae33a5-5979-4f58-bca8-9140418a4a8b".to_string());
+    let helius_api_key = env::var("HELIUS_API_KEY")
+        .unwrap_or_else(|_| "63ae33a5-5979-4f58-bca8-9140418a4a8b".to_string());
     let network = env::var("SOLANA_NETWORK").unwrap_or_else(|_| "devnet".to_string());
-    
+
     // Create HTTP client for Helius API
     let http_client = Client::new();
-    
+
     // Create RPC client as fallback
     let rpc_url = match network.as_str() {
         "devnet" => "https://api.devnet.solana.com",
@@ -421,25 +496,32 @@ async fn main() -> anyhow::Result<()> {
     };
     let rpc_client = RpcClient::new(rpc_url.to_string());
 
-    println!("🚀 Starting Helius-based indexer...");
+    println!("Starting Helius-based indexer...");
     println!("Using Helius API key: {}...", &helius_api_key[..8]);
-    println!("🌐 Network: {}", network);
+    println!("Network: {}", network);
 
     // Test lookup of the specific transaction signature
-    let test_signature = "3aPkVkR8fj4qsycKVTaD2BtekbAM1fgfeCrMqdtW3RS2yXDXJxyyS39TsLrs41RA2MdbQbcDsyNrRaVNB68ED3Lb";
-    println!("🧪 Testing transaction lookup for signature: {}", test_signature);
-    match fetch_transaction_by_signature(&http_client, &helius_api_key, test_signature, &network).await {
+    let test_signature =
+        "3aPkVkR8fj4qsycKVTaD2BtekbAM1fgfeCrMqdtW3RS2yXDXJxyyS39TsLrs41RA2MdbQbcDsyNrRaVNB68ED3Lb";
+    println!(
+        "Testing transaction lookup for signature: {}",
+        test_signature
+    );
+    match fetch_transaction_by_signature(&http_client, &helius_api_key, test_signature, &network)
+        .await
+    {
         Ok(Some(transaction)) => {
-            println!("✅ Found transaction by signature!");
-            if let Err(e) = process_helius_transaction(&pool, &transaction, &tracked_pubkeys).await {
-                eprintln!("❌ Error processing test transaction: {}", e);
+            println!("Found transaction by signature!");
+            if let Err(e) = process_helius_transaction(&pool, &transaction, &tracked_pubkeys).await
+            {
+                eprintln!("Error processing test transaction: {}", e);
             }
         }
         Ok(None) => {
-            println!("❌ Transaction not found by signature");
+            println!("Transaction not found by signature");
         }
         Err(e) => {
-            eprintln!("❌ Error looking up transaction by signature: {}", e);
+            eprintln!("Error looking up transaction by signature: {}", e);
         }
     }
 
@@ -448,75 +530,109 @@ async fn main() -> anyhow::Result<()> {
     let mut last_helius_poll = std::time::Instant::now();
     let refresh_interval = std::time::Duration::from_secs(60); // Refresh every minute
     let helius_poll_interval = std::time::Duration::from_secs(30); // Poll Helius every 30 seconds
-    
+
     loop {
         // Refresh tracked pubkeys periodically
         if last_refresh.elapsed() >= refresh_interval {
-            println!("🔄 Refreshing tracked public keys from database...");
+            println!("Refreshing tracked public keys from database...");
             let new_tracked_pubkeys = load_tracked_pubkeys(&pool).await?;
             if new_tracked_pubkeys.len() != tracked_pubkeys.len() {
-                println!("✅ Found {} new public keys to track (total: {})", 
-                    new_tracked_pubkeys.len() - tracked_pubkeys.len(), 
-                    new_tracked_pubkeys.len());
+                println!(
+                    "Found {} new public keys to track (total: {})",
+                    new_tracked_pubkeys.len() - tracked_pubkeys.len(),
+                    new_tracked_pubkeys.len()
+                );
                 tracked_pubkeys = new_tracked_pubkeys;
             }
             last_refresh = std::time::Instant::now();
         }
-        
+
         // Poll Helius API for recent transactions
         if last_helius_poll.elapsed() >= helius_poll_interval {
-            println!("📡 Polling for recent transactions...");
+            println!("Polling for recent transactions...");
             for pubkey in &tracked_pubkeys {
                 let pubkey_str = pubkey.to_string();
-                
+
                 // Try Helius API first
                 let mut transactions = Vec::new();
-                match fetch_helius_transactions(&http_client, &helius_api_key, &pubkey_str, &network).await {
+                match fetch_helius_transactions(
+                    &http_client,
+                    &helius_api_key,
+                    &pubkey_str,
+                    &network,
+                )
+                .await
+                {
                     Ok(helius_txs) => {
                         if !helius_txs.is_empty() {
-                            println!("📊 Found {} transactions via Helius for {}", helius_txs.len(), pubkey_str);
+                            println!(
+                                "Found {} transactions via Helius for {}",
+                                helius_txs.len(),
+                                pubkey_str
+                            );
                             transactions = helius_txs;
                         } else {
-                            println!("⚠️  Helius returned empty results for {}, trying RPC fallback...", pubkey_str);
+                            println!(
+                                "Helius returned empty results for {}, trying RPC fallback...",
+                                pubkey_str
+                            );
                             // Fallback to RPC
                             match fetch_rpc_transactions(&rpc_client, pubkey).await {
                                 Ok(rpc_txs) => {
-                                    println!("📊 Found {} transactions via RPC for {}", rpc_txs.len(), pubkey_str);
+                                    println!(
+                                        "Found {} transactions via RPC for {}",
+                                        rpc_txs.len(),
+                                        pubkey_str
+                                    );
                                     transactions = rpc_txs;
                                 }
                                 Err(e) => {
-                                    eprintln!("❌ Error fetching RPC transactions for {}: {}", pubkey_str, e);
+                                    eprintln!(
+                                        "Error fetching RPC transactions for {}: {}",
+                                        pubkey_str, e
+                                    );
                                 }
                             }
                         }
                     }
                     Err(e) => {
-                        eprintln!("❌ Error fetching Helius transactions for {}: {}, trying RPC fallback...", pubkey_str, e);
+                        eprintln!(
+                            "Error fetching Helius transactions for {}: {}, trying RPC fallback...",
+                            pubkey_str, e
+                        );
                         // Fallback to RPC
                         match fetch_rpc_transactions(&rpc_client, pubkey).await {
                             Ok(rpc_txs) => {
-                                println!("📊 Found {} transactions via RPC for {}", rpc_txs.len(), pubkey_str);
+                                println!(
+                                    "Found {} transactions via RPC for {}",
+                                    rpc_txs.len(),
+                                    pubkey_str
+                                );
                                 transactions = rpc_txs;
                             }
                             Err(rpc_e) => {
-                                eprintln!("❌ Error fetching RPC transactions for {}: {}", pubkey_str, rpc_e);
+                                eprintln!(
+                                    "Error fetching RPC transactions for {}: {}",
+                                    pubkey_str, rpc_e
+                                );
                             }
                         }
                     }
                 }
-                
+
                 // Process all found transactions
                 for transaction in &transactions {
-                    if let Err(e) = process_helius_transaction(&pool, transaction, &tracked_pubkeys).await {
-                        eprintln!("❌ Error processing transaction: {}", e);
+                    if let Err(e) =
+                        process_helius_transaction(&pool, transaction, &tracked_pubkeys).await
+                    {
+                        eprintln!("Error processing transaction: {}", e);
                     }
                 }
             }
             last_helius_poll = std::time::Instant::now();
         }
-        
+
         // Sleep for a short interval before next poll
         tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
     }
 }
-
